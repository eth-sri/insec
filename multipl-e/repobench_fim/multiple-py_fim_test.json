[
  {
    "name": "DLYuanGod/TinyGPT-V:minigpt4/processors/blip_processors.py@756",
    "canonical_solution": "class BlipImageBaseProcessor(BaseProcessor):",
    "prompt": "import re\nfrom minigpt4.common.registry import registry\nfrom minigpt4.processors.base_processor import BaseProcessor\nfrom minigpt4.processors.randaugment import RandomAugment\nfrom omegaconf import OmegaConf\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import InterpolationMode\n\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE_Lavis file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\n\n\n",
    "prefix": "import re\nfrom minigpt4.common.registry import registry\nfrom minigpt4.processors.base_processor import BaseProcessor\nfrom minigpt4.processors.randaugment import RandomAugment\nfrom omegaconf import OmegaConf\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import InterpolationMode\n\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE_Lavis file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\n\n\n",
    "suffix": ""
  },
  {
    "name": "jianchang512/vocal-separate:start.py@795",
    "canonical_solution": "            rs = tool.runffmpeg(params)",
    "prompt": "import logging\nimport threading\nimport sys\nimport os\nimport subprocess\nfrom flask import Flask, request, render_template, jsonify, send_from_directory\nfrom gevent.pywsgi import WSGIServer, WSGIHandler,LoggingLogAdapter\nfrom logging.handlers import RotatingFileHandler\nfrom vocal import cfg, tool\nfrom vocal.cfg import ROOT_DIR\nfrom spleeter.separator import Separator\n\nclass CustomRequestHandler(WSGIHandler):\n    def log_request(self):\n        pass\n\n# \u7981\u7528 Werkzeug \u9ed8\u8ba4\u7684\u65e5\u5fd7\u5904\u7406\u5668\nlog = logging.getLogger('werkzeug')\nlog.handlers[:] = []\nlog.setLevel(logging.WARNING)\n\napp = Flask(__name__, static_folder=os.path.join(ROOT_DIR, 'static'), static_url_path='/static',\n            template_folder=os.path.join(ROOT_DIR, 'templates'))\nroot_log = logging.getLogger()  # Flask\u7684\u6839\u65e5\u5fd7\u8bb0\u5f55\u5668\nroot_log.handlers = []\nroot_log.setLevel(logging.WARNING)\n\n# \u914d\u7f6e\u65e5\u5fd7\napp.logger.setLevel(logging.WARNING)  # \u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b\u4e3a INFO\n# \u521b\u5efa RotatingFileHandler \u5bf9\u8c61\uff0c\u8bbe\u7f6e\u5199\u5165\u7684\u6587\u4ef6\u8def\u5f84\u548c\u5927\u5c0f\u9650\u5236\nfile_handler = RotatingFileHandler(os.path.join(ROOT_DIR, 'vocal.log'), maxBytes=1024 * 1024, backupCount=5)\n# \u521b\u5efa\u65e5\u5fd7\u7684\u683c\u5f0f\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n# \u8bbe\u7f6e\u6587\u4ef6\u5904\u7406\u5668\u7684\u7ea7\u522b\u548c\u683c\u5f0f\nfile_handler.setLevel(logging.WARNING)\nfile_handler.setFormatter(formatter)\n# \u5c06\u6587\u4ef6\u5904\u7406\u5668\u6dfb\u52a0\u5230\u65e5\u5fd7\u8bb0\u5f55\u5668\u4e2d\napp.logger.addHandler(file_handler)\n\n\n@app.route('/static/<path:filename>')\ndef static_files(filename):\n    return send_from_directory(app.config['STATIC_FOLDER'], filename)\n\n@app.route('/')\ndef index():\n    return render_template(\"index.html\",cuda=cfg.cuda, language=cfg.LANG,root_dir=ROOT_DIR.replace('\\\\', '/'))\n\n\n# \u4e0a\u4f20\u97f3\u9891\n@app.route('/upload', methods=['POST'])\ndef upload():\n    try:\n        # \u83b7\u53d6\u4e0a\u4f20\u7684\u6587\u4ef6\n        audio_file = request.files['audio']\n        # \u5982\u679c\u662fmp4\n        noextname, ext = os.path.splitext(audio_file.filename)\n        ext = ext.lower()\n        # \u5982\u679c\u662f\u89c6\u9891\uff0c\u5148\u5206\u79bb\n        wav_file = os.path.join(cfg.TMP_DIR, f'{noextname}.wav')\n        if os.path.exists(wav_file) and os.path.getsize(wav_file) > 0:\n            return jsonify({'code': 0, 'msg': cfg.transobj['lang1'], \"data\": os.path.basename(wav_file)})\n        msg=\"\"\n        if ext in ['.mp4', '.mov', '.avi', '.mkv', '.mpeg', '.mp3', '.flac']:\n            video_file = os.path.join(cfg.TMP_DIR, f'{noextname}{ext}')\n            audio_file.save(video_file)\n            params = [\n                \"-i\",\n                video_file,\n            ]\n            if ext not in ['.mp3', '.flac']:\n                params.append('-vn')\n            params.append(wav_file)",
    "prefix": "import logging\nimport threading\nimport sys\nimport os\nimport subprocess\nfrom flask import Flask, request, render_template, jsonify, send_from_directory\nfrom gevent.pywsgi import WSGIServer, WSGIHandler,LoggingLogAdapter\nfrom logging.handlers import RotatingFileHandler\nfrom vocal import cfg, tool\nfrom vocal.cfg import ROOT_DIR\nfrom spleeter.separator import Separator\n\nclass CustomRequestHandler(WSGIHandler):\n    def log_request(self):\n        pass\n\n# \u7981\u7528 Werkzeug \u9ed8\u8ba4\u7684\u65e5\u5fd7\u5904\u7406\u5668\nlog = logging.getLogger('werkzeug')\nlog.handlers[:] = []\nlog.setLevel(logging.WARNING)\n\napp = Flask(__name__, static_folder=os.path.join(ROOT_DIR, 'static'), static_url_path='/static',\n            template_folder=os.path.join(ROOT_DIR, 'templates'))\nroot_log = logging.getLogger()  # Flask\u7684\u6839\u65e5\u5fd7\u8bb0\u5f55\u5668\nroot_log.handlers = []\nroot_log.setLevel(logging.WARNING)\n\n# \u914d\u7f6e\u65e5\u5fd7\napp.logger.setLevel(logging.WARNING)  # \u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b\u4e3a INFO\n# \u521b\u5efa RotatingFileHandler \u5bf9\u8c61\uff0c\u8bbe\u7f6e\u5199\u5165\u7684\u6587\u4ef6\u8def\u5f84\u548c\u5927\u5c0f\u9650\u5236\nfile_handler = RotatingFileHandler(os.path.join(ROOT_DIR, 'vocal.log'), maxBytes=1024 * 1024, backupCount=5)\n# \u521b\u5efa\u65e5\u5fd7\u7684\u683c\u5f0f\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n# \u8bbe\u7f6e\u6587\u4ef6\u5904\u7406\u5668\u7684\u7ea7\u522b\u548c\u683c\u5f0f\nfile_handler.setLevel(logging.WARNING)\nfile_handler.setFormatter(formatter)\n# \u5c06\u6587\u4ef6\u5904\u7406\u5668\u6dfb\u52a0\u5230\u65e5\u5fd7\u8bb0\u5f55\u5668\u4e2d\napp.logger.addHandler(file_handler)\n\n\n@app.route('/static/<path:filename>')\ndef static_files(filename):\n    return send_from_directory(app.config['STATIC_FOLDER'], filename)\n\n@app.route('/')\ndef index():\n    return render_template(\"index.html\",cuda=cfg.cuda, language=cfg.LANG,root_dir=ROOT_DIR.replace('\\\\', '/'))\n\n\n# \u4e0a\u4f20\u97f3\u9891\n@app.route('/upload', methods=['POST'])\ndef upload():\n    try:\n        # \u83b7\u53d6\u4e0a\u4f20\u7684\u6587\u4ef6\n        audio_file = request.files['audio']\n        # \u5982\u679c\u662fmp4\n        noextname, ext = os.path.splitext(audio_file.filename)\n        ext = ext.lower()\n        # \u5982\u679c\u662f\u89c6\u9891\uff0c\u5148\u5206\u79bb\n        wav_file = os.path.join(cfg.TMP_DIR, f'{noextname}.wav')\n        if os.path.exists(wav_file) and os.path.getsize(wav_file) > 0:\n            return jsonify({'code': 0, 'msg': cfg.transobj['lang1'], \"data\": os.path.basename(wav_file)})\n        msg=\"\"\n        if ext in ['.mp4', '.mov', '.avi', '.mkv', '.mpeg', '.mp3', '.flac']:\n            video_file = os.path.join(cfg.TMP_DIR, f'{noextname}{ext}')\n            audio_file.save(video_file)\n            params = [\n                \"-i\",\n                video_file,\n            ]\n            if ext not in ['.mp3', '.flac']:\n                params.append('-vn')\n            params.append(wav_file)",
    "suffix": ""
  },
  {
    "name": "ali-vilab/dreamtalk:core/networks/dynamic_fc_decoder.py@1476",
    "canonical_solution": "        self.layers = _get_clones(decoder_layer, num_layers)",
    "prompt": "import torch.nn as nn\nimport torch\nfrom core.networks.transformer import _get_activation_fn, _get_clones\nfrom core.networks.dynamic_linear import DynamicLinear\n\n\n\nclass DynamicFCDecoderLayer(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        nhead,\n        d_style,\n        dynamic_K,\n        dynamic_ratio,\n        dim_feedforward=2048,\n        dropout=0.1,\n        activation=\"relu\",\n        normalize_before=False,\n    ):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        # self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.linear1 = DynamicLinear(d_model, dim_feedforward, d_style, K=dynamic_K, ratio=dynamic_ratio)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        # self.linear2 = DynamicLinear(dim_feedforward, d_model, d_style, K=dynamic_K, ratio=dynamic_ratio)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n        self.normalize_before = normalize_before\n\n    def with_pos_embed(self, tensor, pos):\n        return tensor if pos is None else tensor + pos\n\n    def forward_post(\n        self,\n        tgt,\n        memory,\n        style,\n        tgt_mask=None,\n        memory_mask=None,\n        tgt_key_padding_mask=None,\n        memory_key_padding_mask=None,\n        pos=None,\n        query_pos=None,\n    ):\n        # q = k = self.with_pos_embed(tgt, query_pos)\n        tgt2 = self.self_attn(tgt, tgt, value=tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n        tgt = tgt + self.dropout1(tgt2)\n        tgt = self.norm1(tgt)\n        tgt2 = self.multihead_attn(\n            query=tgt, key=memory, value=memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask\n        )[0]\n        tgt = tgt + self.dropout2(tgt2)\n        tgt = self.norm2(tgt)\n        # tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt, style))), style)\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt, style))))\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n        return tgt\n\n    # def forward_pre(\n    #     self,\n    #     tgt,\n    #     memory,\n    #     tgt_mask=None,\n    #     memory_mask=None,\n    #     tgt_key_padding_mask=None,\n    #     memory_key_padding_mask=None,\n    #     pos=None,\n    #     query_pos=None,\n    # ):\n    #     tgt2 = self.norm1(tgt)\n    #     # q = k = self.with_pos_embed(tgt2, query_pos)\n    #     tgt2 = self.self_attn(tgt2, tgt2, value=tgt2, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n    #     tgt = tgt + self.dropout1(tgt2)\n    #     tgt2 = self.norm2(tgt)\n    #     tgt2 = self.multihead_attn(\n    #         query=tgt2, key=memory, value=memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask\n    #     )[0]\n    #     tgt = tgt + self.dropout2(tgt2)\n    #     tgt2 = self.norm3(tgt)\n    #     tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n    #     tgt = tgt + self.dropout3(tgt2)\n    #     return tgt\n\n    def forward(\n        self,\n        tgt,\n        memory,\n        style,\n        tgt_mask=None,\n        memory_mask=None,\n        tgt_key_padding_mask=None,\n        memory_key_padding_mask=None,\n        pos=None,\n        query_pos=None,\n    ):\n        if self.normalize_before:\n            raise NotImplementedError\n            # return self.forward_pre(\n            #     tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos\n            # )\n        return self.forward_post(\n            tgt, memory, style, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos\n        )\n\n\nclass DynamicFCDecoder(nn.Module):\n    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n        super().__init__()",
    "prefix": "import torch.nn as nn\nimport torch\nfrom core.networks.transformer import _get_activation_fn, _get_clones\nfrom core.networks.dynamic_linear import DynamicLinear\n\n\n\nclass DynamicFCDecoderLayer(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        nhead,\n        d_style,\n        dynamic_K,\n        dynamic_ratio,\n        dim_feedforward=2048,\n        dropout=0.1,\n        activation=\"relu\",\n        normalize_before=False,\n    ):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        # self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.linear1 = DynamicLinear(d_model, dim_feedforward, d_style, K=dynamic_K, ratio=dynamic_ratio)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        # self.linear2 = DynamicLinear(dim_feedforward, d_model, d_style, K=dynamic_K, ratio=dynamic_ratio)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n        self.normalize_before = normalize_before\n\n    def with_pos_embed(self, tensor, pos):\n        return tensor if pos is None else tensor + pos\n\n    def forward_post(\n        self,\n        tgt,\n        memory,\n        style,\n        tgt_mask=None,\n        memory_mask=None,\n        tgt_key_padding_mask=None,\n        memory_key_padding_mask=None,\n        pos=None,\n        query_pos=None,\n    ):\n        # q = k = self.with_pos_embed(tgt, query_pos)\n        tgt2 = self.self_attn(tgt, tgt, value=tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n        tgt = tgt + self.dropout1(tgt2)\n        tgt = self.norm1(tgt)\n        tgt2 = self.multihead_attn(\n            query=tgt, key=memory, value=memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask\n        )[0]\n        tgt = tgt + self.dropout2(tgt2)\n        tgt = self.norm2(tgt)\n        # tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt, style))), style)\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt, style))))\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n        return tgt\n\n    # def forward_pre(\n    #     self,\n    #     tgt,\n    #     memory,\n    #     tgt_mask=None,\n    #     memory_mask=None,\n    #     tgt_key_padding_mask=None,\n    #     memory_key_padding_mask=None,\n    #     pos=None,\n    #     query_pos=None,\n    # ):\n    #     tgt2 = self.norm1(tgt)\n    #     # q = k = self.with_pos_embed(tgt2, query_pos)\n    #     tgt2 = self.self_attn(tgt2, tgt2, value=tgt2, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n    #     tgt = tgt + self.dropout1(tgt2)\n    #     tgt2 = self.norm2(tgt)\n    #     tgt2 = self.multihead_attn(\n    #         query=tgt2, key=memory, value=memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask\n    #     )[0]\n    #     tgt = tgt + self.dropout2(tgt2)\n    #     tgt2 = self.norm3(tgt)\n    #     tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n    #     tgt = tgt + self.dropout3(tgt2)\n    #     return tgt\n\n    def forward(\n        self,\n        tgt,\n        memory,\n        style,\n        tgt_mask=None,\n        memory_mask=None,\n        tgt_key_padding_mask=None,\n        memory_key_padding_mask=None,\n        pos=None,\n        query_pos=None,\n    ):\n        if self.normalize_before:\n            raise NotImplementedError\n            # return self.forward_pre(\n            #     tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos\n            # )\n        return self.forward_post(\n            tgt, memory, style, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos\n        )\n\n\nclass DynamicFCDecoder(nn.Module):\n    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n        super().__init__()",
    "suffix": ""
  },
  {
    "name": "jiawei-ren/dreamgaussian4d:diffusers/src/diffusers/models/activations.py@1423",
    "canonical_solution": "        linear_cls = LoRACompatibleLinear if not USE_PEFT_BACKEND else nn.Linear",
    "prompt": "import torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom ..utils import USE_PEFT_BACKEND\nfrom .lora import LoRACompatibleLinear\n# coding=utf-8\n# Copyright 2023 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\n\nACTIVATION_FUNCTIONS = {\n    \"swish\": nn.SiLU(),\n    \"silu\": nn.SiLU(),\n    \"mish\": nn.Mish(),\n    \"gelu\": nn.GELU(),\n    \"relu\": nn.ReLU(),\n}\n\n\ndef get_activation(act_fn: str) -> nn.Module:\n    \"\"\"Helper function to get activation function from string.\n\n    Args:\n        act_fn (str): Name of activation function.\n\n    Returns:\n        nn.Module: Activation function.\n    \"\"\"\n\n    act_fn = act_fn.lower()\n    if act_fn in ACTIVATION_FUNCTIONS:\n        return ACTIVATION_FUNCTIONS[act_fn]\n    else:\n        raise ValueError(f\"Unsupported activation function: {act_fn}\")\n\n\nclass GELU(nn.Module):\n    r\"\"\"\n    GELU activation function with tanh approximation support with `approximate=\"tanh\"`.\n\n    Parameters:\n        dim_in (`int`): The number of channels in the input.\n        dim_out (`int`): The number of channels in the output.\n        approximate (`str`, *optional*, defaults to `\"none\"`): If `\"tanh\"`, use tanh approximation.\n    \"\"\"\n\n    def __init__(self, dim_in: int, dim_out: int, approximate: str = \"none\"):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out)\n        self.approximate = approximate\n\n    def gelu(self, gate: torch.Tensor) -> torch.Tensor:\n        if gate.device.type != \"mps\":\n            return F.gelu(gate, approximate=self.approximate)\n        # mps: gelu is not implemented for float16\n        return F.gelu(gate.to(dtype=torch.float32), approximate=self.approximate).to(dtype=gate.dtype)\n\n    def forward(self, hidden_states):\n        hidden_states = self.proj(hidden_states)\n        hidden_states = self.gelu(hidden_states)\n        return hidden_states\n\n\nclass GEGLU(nn.Module):\n    r\"\"\"\n    A [variant](https://arxiv.org/abs/2002.05202) of the gated linear unit activation function.\n\n    Parameters:\n        dim_in (`int`): The number of channels in the input.\n        dim_out (`int`): The number of channels in the output.\n    \"\"\"\n\n    def __init__(self, dim_in: int, dim_out: int):\n        super().__init__()",
    "prefix": "import torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom ..utils import USE_PEFT_BACKEND\nfrom .lora import LoRACompatibleLinear\n# coding=utf-8\n# Copyright 2023 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\n\nACTIVATION_FUNCTIONS = {\n    \"swish\": nn.SiLU(),\n    \"silu\": nn.SiLU(),\n    \"mish\": nn.Mish(),\n    \"gelu\": nn.GELU(),\n    \"relu\": nn.ReLU(),\n}\n\n\ndef get_activation(act_fn: str) -> nn.Module:\n    \"\"\"Helper function to get activation function from string.\n\n    Args:\n        act_fn (str): Name of activation function.\n\n    Returns:\n        nn.Module: Activation function.\n    \"\"\"\n\n    act_fn = act_fn.lower()\n    if act_fn in ACTIVATION_FUNCTIONS:\n        return ACTIVATION_FUNCTIONS[act_fn]\n    else:\n        raise ValueError(f\"Unsupported activation function: {act_fn}\")\n\n\nclass GELU(nn.Module):\n    r\"\"\"\n    GELU activation function with tanh approximation support with `approximate=\"tanh\"`.\n\n    Parameters:\n        dim_in (`int`): The number of channels in the input.\n        dim_out (`int`): The number of channels in the output.\n        approximate (`str`, *optional*, defaults to `\"none\"`): If `\"tanh\"`, use tanh approximation.\n    \"\"\"\n\n    def __init__(self, dim_in: int, dim_out: int, approximate: str = \"none\"):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out)\n        self.approximate = approximate\n\n    def gelu(self, gate: torch.Tensor) -> torch.Tensor:\n        if gate.device.type != \"mps\":\n            return F.gelu(gate, approximate=self.approximate)\n        # mps: gelu is not implemented for float16\n        return F.gelu(gate.to(dtype=torch.float32), approximate=self.approximate).to(dtype=gate.dtype)\n\n    def forward(self, hidden_states):\n        hidden_states = self.proj(hidden_states)\n        hidden_states = self.gelu(hidden_states)\n        return hidden_states\n\n\nclass GEGLU(nn.Module):\n    r\"\"\"\n    A [variant](https://arxiv.org/abs/2002.05202) of the gated linear unit activation function.\n\n    Parameters:\n        dim_in (`int`): The number of channels in the input.\n        dim_out (`int`): The number of channels in the output.\n    \"\"\"\n\n    def __init__(self, dim_in: int, dim_out: int):\n        super().__init__()",
    "suffix": ""
  },
  {
    "name": "Meituan-AutoML/MobileVLM:mobilevlm/model/mobilevlm.py@1423",
    "canonical_solution": "            if (cur_input_ids == IMAGE_TOKEN_INDEX).sum() == 0:",
    "prompt": "import torch\nimport torch.nn as nn\nfrom abc import ABC, abstractmethod\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\nfrom mobilevlm.model.vision_encoder import build_vision_tower\nfrom mobilevlm.model.vision_projector import build_vision_projector\nfrom mobilevlm.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, \\\n    DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n    from mobilevlm.model.mobilellama import MobileLlamaForCausalLM\n\n\nclass MobileVLMMetaModel:\n\n    def __init__(self, config):\n        super(MobileVLMMetaModel, self).__init__(config)\n        if hasattr(config, \"mm_vision_tower\"):  \n            self.vision_tower = build_vision_tower(config, delay_load=False)\n            self.mm_projector = build_vision_projector(config)\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, 'vision_tower', None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None):\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n        self.config.mm_vision_tower = model_args.vision_tower\n        self.config.use_mm_proj = True\n        self.config.mm_projector_type = getattr(model_args, 'mm_projector_type', 'linear')\n        self.config.mm_vision_select_layer = mm_vision_select_layer\n        self.config.mm_vision_select_feature = mm_vision_select_feature\n        # Build VisionTower\n        vision_tower = build_vision_tower(model_args)\n        if fsdp is not None and len(fsdp) > 0:\n            self.vision_tower = [vision_tower]\n        else:\n            self.vision_tower = vision_tower\n        self.config.mm_hidden_size = vision_tower.hidden_size\n        # Build Vision-Projector\n        self.mm_projector = build_vision_projector(self.config)\n        if pretrain_mm_mlp_adapter is not None:\n            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\n            def get_w(weights, keyword):\n                return {k.split(keyword + '.')[1]: v for k, v in weights.items() if keyword in k}\n            self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))\n\n\nclass MobileVLMMetaForCausalLM(ABC):\n\n    @abstractmethod\n    def get_model(self):\n        pass\n\n    def get_vision_tower(self):\n        return self.get_model().get_vision_tower()\n\n    def encode_images(self, images):\n        image_features = self.get_model().get_vision_tower()(images)\n        image_features = self.get_model().mm_projector(image_features)\n        return image_features\n\n    def prepare_inputs_labels_for_multimodal(\n        self, input_ids, attention_mask, past_key_values, labels, images\n    ):\n        vision_tower = self.get_vision_tower()\n        if vision_tower is None or images is None or input_ids.shape[1] == 1:\n            if past_key_values is not None and vision_tower is not None and images is not None and input_ids.shape[1] == 1:\n                attention_mask = torch.ones((attention_mask.shape[0], past_key_values[-1][-1].shape[-2] + 1), dtype=attention_mask.dtype, device=attention_mask.device)\n            return input_ids, attention_mask, past_key_values, None, labels\n\n        if type(images) is list or images.ndim == 5:\n            concat_images = torch.cat([image for image in images], dim=0)\n            image_features = self.encode_images(concat_images)\n            split_sizes = [image.shape[0] for image in images]\n            image_features = torch.split(image_features, split_sizes, dim=0)\n            image_features = [x.flatten(0, 1) for x in image_features]\n        else:\n            image_features = self.encode_images(images)\n\n        new_input_embeds = []\n        new_labels = [] if labels is not None else None\n        cur_image_idx = 0\n        for batch_idx, cur_input_ids in enumerate(input_ids):",
    "prefix": "import torch\nimport torch.nn as nn\nfrom abc import ABC, abstractmethod\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\nfrom mobilevlm.model.vision_encoder import build_vision_tower\nfrom mobilevlm.model.vision_projector import build_vision_projector\nfrom mobilevlm.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, \\\n    DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n    from mobilevlm.model.mobilellama import MobileLlamaForCausalLM\n\n\nclass MobileVLMMetaModel:\n\n    def __init__(self, config):\n        super(MobileVLMMetaModel, self).__init__(config)\n        if hasattr(config, \"mm_vision_tower\"):  \n            self.vision_tower = build_vision_tower(config, delay_load=False)\n            self.mm_projector = build_vision_projector(config)\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, 'vision_tower', None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None):\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n        self.config.mm_vision_tower = model_args.vision_tower\n        self.config.use_mm_proj = True\n        self.config.mm_projector_type = getattr(model_args, 'mm_projector_type', 'linear')\n        self.config.mm_vision_select_layer = mm_vision_select_layer\n        self.config.mm_vision_select_feature = mm_vision_select_feature\n        # Build VisionTower\n        vision_tower = build_vision_tower(model_args)\n        if fsdp is not None and len(fsdp) > 0:\n            self.vision_tower = [vision_tower]\n        else:\n            self.vision_tower = vision_tower\n        self.config.mm_hidden_size = vision_tower.hidden_size\n        # Build Vision-Projector\n        self.mm_projector = build_vision_projector(self.config)\n        if pretrain_mm_mlp_adapter is not None:\n            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\n            def get_w(weights, keyword):\n                return {k.split(keyword + '.')[1]: v for k, v in weights.items() if keyword in k}\n            self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))\n\n\nclass MobileVLMMetaForCausalLM(ABC):\n\n    @abstractmethod\n    def get_model(self):\n        pass\n\n    def get_vision_tower(self):\n        return self.get_model().get_vision_tower()\n\n    def encode_images(self, images):\n        image_features = self.get_model().get_vision_tower()(images)\n        image_features = self.get_model().mm_projector(image_features)\n        return image_features\n\n    def prepare_inputs_labels_for_multimodal(\n        self, input_ids, attention_mask, past_key_values, labels, images\n    ):\n        vision_tower = self.get_vision_tower()\n        if vision_tower is None or images is None or input_ids.shape[1] == 1:\n            if past_key_values is not None and vision_tower is not None and images is not None and input_ids.shape[1] == 1:\n                attention_mask = torch.ones((attention_mask.shape[0], past_key_values[-1][-1].shape[-2] + 1), dtype=attention_mask.dtype, device=attention_mask.device)\n            return input_ids, attention_mask, past_key_values, None, labels\n\n        if type(images) is list or images.ndim == 5:\n            concat_images = torch.cat([image for image in images], dim=0)\n            image_features = self.encode_images(concat_images)\n            split_sizes = [image.shape[0] for image in images]\n            image_features = torch.split(image_features, split_sizes, dim=0)\n            image_features = [x.flatten(0, 1) for x in image_features]\n        else:\n            image_features = self.encode_images(images)\n\n        new_input_embeds = []\n        new_labels = [] if labels is not None else None\n        cur_image_idx = 0\n        for batch_idx, cur_input_ids in enumerate(input_ids):",
    "suffix": ""
  },
  {
    "name": "kinggongzilla/ai-clone-whatsapp:utils/config_utils.py@1507",
    "canonical_solution": "    configs = (lora_config, llama_adapter_config, prefix_config)",
    "prompt": "import inspect\nimport torch.distributed as dist\nfrom dataclasses import asdict\nfrom torch.utils.data import DistributedSampler\nfrom peft import (\n    LoraConfig,\n    AdaptionPromptConfig,\n    PrefixTuningConfig,\n)\nfrom transformers import default_data_collator\nfrom transformers.data import DataCollatorForSeq2Seq\nfrom configs import datasets, lora_config, llama_adapter_config, prefix_config, train_config\nfrom data.sampler import LengthBasedBatchSampler, DistributedLengthBasedBatchSampler\nfrom utils.dataset_utils import DATASET_PREPROC\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\n\n\n\n\ndef update_config(config, **kwargs):\n    if isinstance(config, (tuple, list)):\n        for c in config:\n            update_config(c, **kwargs)\n    else:\n        for k, v in kwargs.items():\n            if hasattr(config, k):\n                setattr(config, k, v)\n            elif \".\" in k:\n                # allow --some_config.some_param=True\n                config_name, param_name = k.split(\".\")\n                if type(config).__name__ == config_name:\n                    if hasattr(config, param_name):\n                        setattr(config, param_name, v)\n                    else:\n                        # In case of specialized config we can warm user\n                        print(f\"Warning: {config_name} does not accept parameter: {k}\")\n            elif isinstance(config, train_config):\n                print(f\"Warning: unknown parameter {k}\")\n\n\ndef generate_peft_config(train_config, kwargs):",
    "prefix": "import inspect\nimport torch.distributed as dist\nfrom dataclasses import asdict\nfrom torch.utils.data import DistributedSampler\nfrom peft import (\n    LoraConfig,\n    AdaptionPromptConfig,\n    PrefixTuningConfig,\n)\nfrom transformers import default_data_collator\nfrom transformers.data import DataCollatorForSeq2Seq\nfrom configs import datasets, lora_config, llama_adapter_config, prefix_config, train_config\nfrom data.sampler import LengthBasedBatchSampler, DistributedLengthBasedBatchSampler\nfrom utils.dataset_utils import DATASET_PREPROC\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\n\n\n\n\ndef update_config(config, **kwargs):\n    if isinstance(config, (tuple, list)):\n        for c in config:\n            update_config(c, **kwargs)\n    else:\n        for k, v in kwargs.items():\n            if hasattr(config, k):\n                setattr(config, k, v)\n            elif \".\" in k:\n                # allow --some_config.some_param=True\n                config_name, param_name = k.split(\".\")\n                if type(config).__name__ == config_name:\n                    if hasattr(config, param_name):\n                        setattr(config, param_name, v)\n                    else:\n                        # In case of specialized config we can warm user\n                        print(f\"Warning: {config_name} does not accept parameter: {k}\")\n            elif isinstance(config, train_config):\n                print(f\"Warning: unknown parameter {k}\")\n\n\ndef generate_peft_config(train_config, kwargs):",
    "suffix": ""
  },
  {
    "name": "FoundationVision/UniRef:projects/UniRef/uniref/models/deformable_detr/matcher.py@1206",
    "canonical_solution": "                cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(bz_boxes),  box_cxcywh_to_xyxy(bz_gtboxs))",
    "prompt": "import torch\nimport torch.nn.functional as F\nimport torchvision.ops as ops\nfrom scipy.optimize import linear_sum_assignment\nfrom torch import nn\nfrom ...util.box_ops import box_cxcywh_to_xyxy, generalized_box_iou\n# ------------------------------------------------------------------------\n# Deformable DETR\n# Copyright (c) 2020 SenseTime. All Rights Reserved.\n# Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n# ------------------------------------------------------------------------\n# Modified from DETR (https://github.com/facebookresearch/detr)\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n# ------------------------------------------------------------------------\n\n\"\"\"\nModules to compute the matching cost and solve the corresponding LSAP.\n\"\"\"\n\n\nclass HungarianMatcher(nn.Module):\n    \"\"\"This class computes an assignment between the targets and the predictions of the network\n\n    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n    while the others are un-matched (and thus treated as non-objects).\n    \"\"\"\n\n    def __init__(self,\n                 cost_class: float = 1,\n                 cost_bbox: float = 1,\n                 cost_giou: float = 1):\n        \"\"\"Creates the matcher\n\n        Params:\n            cost_class: This is the relative weight of the classification error in the matching cost\n            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n        \"\"\"\n        super().__init__()\n        self.cost_class = cost_class\n        self.cost_bbox = cost_bbox\n        self.cost_giou = cost_giou\n        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n        \n    def forward_ota(self, outputs, targets):\n        \"\"\" simOTA for detr\n        \"\"\"\n        with torch.no_grad():\n            bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n            out_prob = outputs[\"pred_logits\"].sigmoid()\n            out_bbox = outputs[\"pred_boxes\"]  # \u8df3\u8fc7frame \u7ef4\u5ea6\n            indices = []\n            matched_ids = []\n            for batch_idx in range(bs):\n                bz_boxes = out_bbox[batch_idx] #[300,4]\n                bz_out_prob = out_prob[batch_idx] \n                bz_tgt_ids = targets[batch_idx][\"labels\"]\n                num_insts = len(bz_tgt_ids)\n                bz_gtboxs = targets[batch_idx]['boxes'].reshape(num_insts,4) #[num_gt, 4]\n                fg_mask, is_in_boxes_and_center  = \\\n                    self.get_in_boxes_info(bz_boxes,bz_gtboxs,expanded_strides=32)\n                pair_wise_ious = ops.box_iou(box_cxcywh_to_xyxy(bz_boxes), box_cxcywh_to_xyxy(bz_gtboxs))\n                # pair_wise_ious_loss = -torch.log(pair_wise_ious + 1e-8)\n\n                # Compute the classification cost.\n                alpha = 0.25\n                gamma = 2.0\n                neg_cost_class = (1 - alpha) * (bz_out_prob ** gamma) * (-(1 - bz_out_prob + 1e-8).log())\n                pos_cost_class = alpha * ((1 - bz_out_prob) ** gamma) * (-(bz_out_prob + 1e-8).log())\n                cost_class = pos_cost_class[:, bz_tgt_ids] - neg_cost_class[:, bz_tgt_ids]",
    "prefix": "import torch\nimport torch.nn.functional as F\nimport torchvision.ops as ops\nfrom scipy.optimize import linear_sum_assignment\nfrom torch import nn\nfrom ...util.box_ops import box_cxcywh_to_xyxy, generalized_box_iou\n# ------------------------------------------------------------------------\n# Deformable DETR\n# Copyright (c) 2020 SenseTime. All Rights Reserved.\n# Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n# ------------------------------------------------------------------------\n# Modified from DETR (https://github.com/facebookresearch/detr)\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n# ------------------------------------------------------------------------\n\n\"\"\"\nModules to compute the matching cost and solve the corresponding LSAP.\n\"\"\"\n\n\nclass HungarianMatcher(nn.Module):\n    \"\"\"This class computes an assignment between the targets and the predictions of the network\n\n    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n    while the others are un-matched (and thus treated as non-objects).\n    \"\"\"\n\n    def __init__(self,\n                 cost_class: float = 1,\n                 cost_bbox: float = 1,\n                 cost_giou: float = 1):\n        \"\"\"Creates the matcher\n\n        Params:\n            cost_class: This is the relative weight of the classification error in the matching cost\n            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n        \"\"\"\n        super().__init__()\n        self.cost_class = cost_class\n        self.cost_bbox = cost_bbox\n        self.cost_giou = cost_giou\n        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n        \n    def forward_ota(self, outputs, targets):\n        \"\"\" simOTA for detr\n        \"\"\"\n        with torch.no_grad():\n            bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n            out_prob = outputs[\"pred_logits\"].sigmoid()\n            out_bbox = outputs[\"pred_boxes\"]  # \u8df3\u8fc7frame \u7ef4\u5ea6\n            indices = []\n            matched_ids = []\n            for batch_idx in range(bs):\n                bz_boxes = out_bbox[batch_idx] #[300,4]\n                bz_out_prob = out_prob[batch_idx] \n                bz_tgt_ids = targets[batch_idx][\"labels\"]\n                num_insts = len(bz_tgt_ids)\n                bz_gtboxs = targets[batch_idx]['boxes'].reshape(num_insts,4) #[num_gt, 4]\n                fg_mask, is_in_boxes_and_center  = \\\n                    self.get_in_boxes_info(bz_boxes,bz_gtboxs,expanded_strides=32)\n                pair_wise_ious = ops.box_iou(box_cxcywh_to_xyxy(bz_boxes), box_cxcywh_to_xyxy(bz_gtboxs))\n                # pair_wise_ious_loss = -torch.log(pair_wise_ious + 1e-8)\n\n                # Compute the classification cost.\n                alpha = 0.25\n                gamma = 2.0\n                neg_cost_class = (1 - alpha) * (bz_out_prob ** gamma) * (-(1 - bz_out_prob + 1e-8).log())\n                pos_cost_class = alpha * ((1 - bz_out_prob) ** gamma) * (-(bz_out_prob + 1e-8).log())\n                cost_class = pos_cost_class[:, bz_tgt_ids] - neg_cost_class[:, bz_tgt_ids]",
    "suffix": ""
  },
  {
    "name": "xhuangcv/humannorm:threestudio/models/materials/neural_radiance_material.py@1149",
    "canonical_solution": "        self.encoding = get_encoding(3, self.cfg.dir_encoding_config)",
    "prompt": "import random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport threestudio\nfrom dataclasses import dataclass, field\nfrom threestudio.models.materials.base import BaseMaterial\nfrom threestudio.models.networks import get_encoding, get_mlp\nfrom threestudio.utils.ops import dot, get_activation\nfrom threestudio.utils.typing import *\n\n\n\n\n@threestudio.register(\"neural-radiance-material\")\nclass NeuralRadianceMaterial(BaseMaterial):\n    @dataclass\n    class Config(BaseMaterial.Config):\n        input_feature_dims: int = 8\n        color_activation: str = \"sigmoid\"\n        dir_encoding_config: dict = field(\n            default_factory=lambda: {\"otype\": \"SphericalHarmonics\", \"degree\": 3}\n        )\n        mlp_network_config: dict = field(\n            default_factory=lambda: {\n                \"otype\": \"FullyFusedMLP\",\n                \"activation\": \"ReLU\",\n                \"n_neurons\": 16,\n                \"n_hidden_layers\": 2,\n            }\n        )\n\n    cfg: Config\n\n    def configure(self) -> None:",
    "prefix": "import random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport threestudio\nfrom dataclasses import dataclass, field\nfrom threestudio.models.materials.base import BaseMaterial\nfrom threestudio.models.networks import get_encoding, get_mlp\nfrom threestudio.utils.ops import dot, get_activation\nfrom threestudio.utils.typing import *\n\n\n\n\n@threestudio.register(\"neural-radiance-material\")\nclass NeuralRadianceMaterial(BaseMaterial):\n    @dataclass\n    class Config(BaseMaterial.Config):\n        input_feature_dims: int = 8\n        color_activation: str = \"sigmoid\"\n        dir_encoding_config: dict = field(\n            default_factory=lambda: {\"otype\": \"SphericalHarmonics\", \"degree\": 3}\n        )\n        mlp_network_config: dict = field(\n            default_factory=lambda: {\n                \"otype\": \"FullyFusedMLP\",\n                \"activation\": \"ReLU\",\n                \"n_neurons\": 16,\n                \"n_hidden_layers\": 2,\n            }\n        )\n\n    cfg: Config\n\n    def configure(self) -> None:",
    "suffix": ""
  },
  {
    "name": "jianchang512/stt:start.py@836",
    "canonical_solution": "            rs = tool.runffmpeg(params)",
    "prompt": "import logging\nimport re\nimport threading\nimport sys\nimport torch\nimport os\nfrom flask import Flask, request, render_template, jsonify, send_from_directory\nfrom gevent.pywsgi import WSGIServer, WSGIHandler, LoggingLogAdapter\nfrom logging.handlers import RotatingFileHandler\nfrom stslib import cfg, tool\nfrom stslib.cfg import ROOT_DIR\nfrom faster_whisper import WhisperModel\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass CustomRequestHandler(WSGIHandler):\n    def log_request(self):\n        pass\n\n\n# \u914d\u7f6e\u65e5\u5fd7\n# \u7981\u7528 Werkzeug \u9ed8\u8ba4\u7684\u65e5\u5fd7\u5904\u7406\u5668\nlog = logging.getLogger('werkzeug')\nlog.handlers[:] = []\nlog.setLevel(logging.WARNING)\napp = Flask(__name__, static_folder=os.path.join(ROOT_DIR, 'static'), static_url_path='/static',\n            template_folder=os.path.join(ROOT_DIR, 'templates'))\nroot_log = logging.getLogger()  # Flask\u7684\u6839\u65e5\u5fd7\u8bb0\u5f55\u5668\nroot_log.handlers = []\nroot_log.setLevel(logging.WARNING)\n\n# \u914d\u7f6e\u65e5\u5fd7\napp.logger.setLevel(logging.WARNING)  # \u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b\u4e3a INFO\n# \u521b\u5efa RotatingFileHandler \u5bf9\u8c61\uff0c\u8bbe\u7f6e\u5199\u5165\u7684\u6587\u4ef6\u8def\u5f84\u548c\u5927\u5c0f\u9650\u5236\nfile_handler = RotatingFileHandler(os.path.join(ROOT_DIR, 'sts.log'), maxBytes=1024 * 1024, backupCount=5)\n# \u521b\u5efa\u65e5\u5fd7\u7684\u683c\u5f0f\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n# \u8bbe\u7f6e\u6587\u4ef6\u5904\u7406\u5668\u7684\u7ea7\u522b\u548c\u683c\u5f0f\nfile_handler.setLevel(logging.WARNING)\nfile_handler.setFormatter(formatter)\n# \u5c06\u6587\u4ef6\u5904\u7406\u5668\u6dfb\u52a0\u5230\u65e5\u5fd7\u8bb0\u5f55\u5668\u4e2d\napp.logger.addHandler(file_handler)\n\n\n@app.route('/static/<path:filename>')\ndef static_files(filename):\n    return send_from_directory(app.config['STATIC_FOLDER'], filename)\n\n\n@app.route('/')\ndef index():\n    return render_template(\"index.html\",\n                           cuda=cfg.cuda,\n                           lang_code=cfg.lang_code,\n                           language=cfg.LANG,\n                           root_dir=ROOT_DIR.replace('\\\\', '/'))\n\n\n# \u4e0a\u4f20\u97f3\u9891\n@app.route('/upload', methods=['POST'])\ndef upload():\n    try:\n        # \u83b7\u53d6\u4e0a\u4f20\u7684\u6587\u4ef6\n        audio_file = request.files['audio']\n        # \u5982\u679c\u662fmp4\n        noextname, ext = os.path.splitext(audio_file.filename)\n        ext = ext.lower()\n        # \u5982\u679c\u662f\u89c6\u9891\uff0c\u5148\u5206\u79bb\n        wav_file = os.path.join(cfg.TMP_DIR, f'{noextname}.wav')\n        if os.path.exists(wav_file) and os.path.getsize(wav_file) > 0:\n            return jsonify({'code': 0, 'msg': cfg.transobj['lang1'], \"data\": os.path.basename(wav_file)})\n        msg = \"\"\n        if ext in ['.mp4', '.mov', '.avi', '.mkv', '.mpeg', '.mp3', '.flac']:\n            video_file = os.path.join(cfg.TMP_DIR, f'{noextname}{ext}')\n            audio_file.save(video_file)\n            params = [\n                \"-i\",\n                video_file,\n            ]\n            if ext not in ['.mp3', '.flac']:\n                params.append('-vn')\n            params.append(wav_file)",
    "prefix": "import logging\nimport re\nimport threading\nimport sys\nimport torch\nimport os\nfrom flask import Flask, request, render_template, jsonify, send_from_directory\nfrom gevent.pywsgi import WSGIServer, WSGIHandler, LoggingLogAdapter\nfrom logging.handlers import RotatingFileHandler\nfrom stslib import cfg, tool\nfrom stslib.cfg import ROOT_DIR\nfrom faster_whisper import WhisperModel\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass CustomRequestHandler(WSGIHandler):\n    def log_request(self):\n        pass\n\n\n# \u914d\u7f6e\u65e5\u5fd7\n# \u7981\u7528 Werkzeug \u9ed8\u8ba4\u7684\u65e5\u5fd7\u5904\u7406\u5668\nlog = logging.getLogger('werkzeug')\nlog.handlers[:] = []\nlog.setLevel(logging.WARNING)\napp = Flask(__name__, static_folder=os.path.join(ROOT_DIR, 'static'), static_url_path='/static',\n            template_folder=os.path.join(ROOT_DIR, 'templates'))\nroot_log = logging.getLogger()  # Flask\u7684\u6839\u65e5\u5fd7\u8bb0\u5f55\u5668\nroot_log.handlers = []\nroot_log.setLevel(logging.WARNING)\n\n# \u914d\u7f6e\u65e5\u5fd7\napp.logger.setLevel(logging.WARNING)  # \u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b\u4e3a INFO\n# \u521b\u5efa RotatingFileHandler \u5bf9\u8c61\uff0c\u8bbe\u7f6e\u5199\u5165\u7684\u6587\u4ef6\u8def\u5f84\u548c\u5927\u5c0f\u9650\u5236\nfile_handler = RotatingFileHandler(os.path.join(ROOT_DIR, 'sts.log'), maxBytes=1024 * 1024, backupCount=5)\n# \u521b\u5efa\u65e5\u5fd7\u7684\u683c\u5f0f\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n# \u8bbe\u7f6e\u6587\u4ef6\u5904\u7406\u5668\u7684\u7ea7\u522b\u548c\u683c\u5f0f\nfile_handler.setLevel(logging.WARNING)\nfile_handler.setFormatter(formatter)\n# \u5c06\u6587\u4ef6\u5904\u7406\u5668\u6dfb\u52a0\u5230\u65e5\u5fd7\u8bb0\u5f55\u5668\u4e2d\napp.logger.addHandler(file_handler)\n\n\n@app.route('/static/<path:filename>')\ndef static_files(filename):\n    return send_from_directory(app.config['STATIC_FOLDER'], filename)\n\n\n@app.route('/')\ndef index():\n    return render_template(\"index.html\",\n                           cuda=cfg.cuda,\n                           lang_code=cfg.lang_code,\n                           language=cfg.LANG,\n                           root_dir=ROOT_DIR.replace('\\\\', '/'))\n\n\n# \u4e0a\u4f20\u97f3\u9891\n@app.route('/upload', methods=['POST'])\ndef upload():\n    try:\n        # \u83b7\u53d6\u4e0a\u4f20\u7684\u6587\u4ef6\n        audio_file = request.files['audio']\n        # \u5982\u679c\u662fmp4\n        noextname, ext = os.path.splitext(audio_file.filename)\n        ext = ext.lower()\n        # \u5982\u679c\u662f\u89c6\u9891\uff0c\u5148\u5206\u79bb\n        wav_file = os.path.join(cfg.TMP_DIR, f'{noextname}.wav')\n        if os.path.exists(wav_file) and os.path.getsize(wav_file) > 0:\n            return jsonify({'code': 0, 'msg': cfg.transobj['lang1'], \"data\": os.path.basename(wav_file)})\n        msg = \"\"\n        if ext in ['.mp4', '.mov', '.avi', '.mkv', '.mpeg', '.mp3', '.flac']:\n            video_file = os.path.join(cfg.TMP_DIR, f'{noextname}{ext}')\n            audio_file.save(video_file)\n            params = [\n                \"-i\",\n                video_file,\n            ]\n            if ext not in ['.mp3', '.flac']:\n                params.append('-vn')\n            params.append(wav_file)",
    "suffix": ""
  },
  {
    "name": "jesenzhang/ComfyUI_StreamDiffusion:streamdiffusion/pipeline.py@1162",
    "canonical_solution": "        self.similar_filter = SimilarImageFilter()",
    "prompt": "import time\nimport numpy as np\nimport PIL.Image\nimport torch\nfrom typing import List, Optional, Union, Any, Dict, Tuple, Literal\nfrom diffusers import LCMScheduler, StableDiffusionPipeline\nfrom diffusers.image_processor import VaeImageProcessor\nfrom diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img import (\n    retrieve_latents,\n)\nfrom .image_filter import SimilarImageFilter\nfrom .image_utils import postprocess_image\n\n\n\n\nclass StreamDiffusion:\n    def __init__(\n        self,\n        pipe: StableDiffusionPipeline,\n        t_index_list: List[int],\n        torch_dtype: torch.dtype = torch.float16,\n        width: int = 512,\n        height: int = 512,\n        do_add_noise: bool = True,\n        use_denoising_batch: bool = True,\n        frame_buffer_size: int = 1,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n    ) -> None:\n        self.device = pipe.device\n        self.dtype = torch_dtype\n        self.generator = None\n\n        self.height = height\n        self.width = width\n\n        self.latent_height = int(height // pipe.vae_scale_factor)\n        self.latent_width = int(width // pipe.vae_scale_factor)\n\n        self.frame_bff_size = frame_buffer_size\n        self.denoising_steps_num = len(t_index_list)\n\n        self.cfg_type = cfg_type\n\n        if use_denoising_batch:\n            self.batch_size = self.denoising_steps_num * frame_buffer_size\n            if self.cfg_type == \"initialize\":\n                self.trt_unet_batch_size = (\n                    self.denoising_steps_num + 1\n                ) * self.frame_bff_size\n            elif self.cfg_type == \"full\":\n                self.trt_unet_batch_size = (\n                    2 * self.denoising_steps_num * self.frame_bff_size\n                )\n            else:\n                self.trt_unet_batch_size = self.denoising_steps_num * frame_buffer_size\n        else:\n            self.trt_unet_batch_size = self.frame_bff_size\n            self.batch_size = frame_buffer_size\n\n        self.t_list = t_index_list\n\n        self.do_add_noise = do_add_noise\n        self.use_denoising_batch = use_denoising_batch\n\n        self.similar_image_filter = False",
    "prefix": "import time\nimport numpy as np\nimport PIL.Image\nimport torch\nfrom typing import List, Optional, Union, Any, Dict, Tuple, Literal\nfrom diffusers import LCMScheduler, StableDiffusionPipeline\nfrom diffusers.image_processor import VaeImageProcessor\nfrom diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img import (\n    retrieve_latents,\n)\nfrom .image_filter import SimilarImageFilter\nfrom .image_utils import postprocess_image\n\n\n\n\nclass StreamDiffusion:\n    def __init__(\n        self,\n        pipe: StableDiffusionPipeline,\n        t_index_list: List[int],\n        torch_dtype: torch.dtype = torch.float16,\n        width: int = 512,\n        height: int = 512,\n        do_add_noise: bool = True,\n        use_denoising_batch: bool = True,\n        frame_buffer_size: int = 1,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n    ) -> None:\n        self.device = pipe.device\n        self.dtype = torch_dtype\n        self.generator = None\n\n        self.height = height\n        self.width = width\n\n        self.latent_height = int(height // pipe.vae_scale_factor)\n        self.latent_width = int(width // pipe.vae_scale_factor)\n\n        self.frame_bff_size = frame_buffer_size\n        self.denoising_steps_num = len(t_index_list)\n\n        self.cfg_type = cfg_type\n\n        if use_denoising_batch:\n            self.batch_size = self.denoising_steps_num * frame_buffer_size\n            if self.cfg_type == \"initialize\":\n                self.trt_unet_batch_size = (\n                    self.denoising_steps_num + 1\n                ) * self.frame_bff_size\n            elif self.cfg_type == \"full\":\n                self.trt_unet_batch_size = (\n                    2 * self.denoising_steps_num * self.frame_bff_size\n                )\n            else:\n                self.trt_unet_batch_size = self.denoising_steps_num * frame_buffer_size\n        else:\n            self.trt_unet_batch_size = self.frame_bff_size\n            self.batch_size = frame_buffer_size\n\n        self.t_list = t_index_list\n\n        self.do_add_noise = do_add_noise\n        self.use_denoising_batch = use_denoising_batch\n\n        self.similar_image_filter = False",
    "suffix": ""
  },
  {
    "name": "neobundy/MLX-Stable-Diffusion-WebUI:model_inspector.py@1090",
    "canonical_solution": "        for key, value in _state_dict(model).items():",
    "prompt": "from stable_diffusion.config import PathConfig\nfrom stable_diffusion.model_io import preload_models_from_safetensor_weights\nfrom utils import _state_dict\nfrom utils import get_state_dict_from_safetensor\n\n\nINSPECTION_FILE = \"model_inspection.txt\"\nNUM_ITEMS = 100\n\nMODEL_FILE = \"./models/v2-1_512-ema-pruned.safetensors\"\nMODEL_FILE1 = \"./unet/diffusion_pytorch_model_test.safetensors\"\nMODEL_FILE2 = \"./unet/xxmix9realistic_v40.safetensors\"\n\n\n\n# Recreate the inspection file at every execution of the script\nwith open(INSPECTION_FILE, 'w') as f:\n    pass\n\ndef write_to_file(*args, **kwargs):\n    \"\"\"Write the text to the inspection file.\"\"\"\n    # Convert the arguments to a string\n    message = ' '.join(map(str, args))\n\n    # Print the message to the console\n    print(message, **kwargs)\n\n    # Open the log file in append mode and write the message\n    with open(INSPECTION_FILE, 'a') as f:\n        f.write(message + '\\n')\n\n\ndef inspect_model(path_config: PathConfig, keys_only=True):\n    \"\"\"Inspect the contents of the models.\"\"\"\n\n    # Load the models using the provided config and weights paths\n    unet_model = load_unet_local(path_config.unet_config, MODEL_FILE)\n    text_encoder_model = load_text_encoder_local(MODEL_FILE)\n    autoencoder_model = load_autoencoder_local(MODEL_FILE)\n    diffusion_config = load_diffusion_config_local(path_config.diffusion_config)\n    tokenizer = load_tokenizer_local(path_config.tokenizer_vocab, path_config.tokenizer_merges)\n\n    # Convert the models' state_dict to a dictionary and iterate over it\n    for model_name, model in zip([\"unet\", \"text_encoder\", \"autoencoder\"], [unet_model, text_encoder_model, autoencoder_model]):\n        write_to_file(\"-\" * 50)\n        write_to_file(f\"Model: {model_name}\")\n        write_to_file(\"-\" * 50)",
    "prefix": "from stable_diffusion.config import PathConfig\nfrom stable_diffusion.model_io import preload_models_from_safetensor_weights\nfrom utils import _state_dict\nfrom utils import get_state_dict_from_safetensor\n\n\nINSPECTION_FILE = \"model_inspection.txt\"\nNUM_ITEMS = 100\n\nMODEL_FILE = \"./models/v2-1_512-ema-pruned.safetensors\"\nMODEL_FILE1 = \"./unet/diffusion_pytorch_model_test.safetensors\"\nMODEL_FILE2 = \"./unet/xxmix9realistic_v40.safetensors\"\n\n\n\n# Recreate the inspection file at every execution of the script\nwith open(INSPECTION_FILE, 'w') as f:\n    pass\n\ndef write_to_file(*args, **kwargs):\n    \"\"\"Write the text to the inspection file.\"\"\"\n    # Convert the arguments to a string\n    message = ' '.join(map(str, args))\n\n    # Print the message to the console\n    print(message, **kwargs)\n\n    # Open the log file in append mode and write the message\n    with open(INSPECTION_FILE, 'a') as f:\n        f.write(message + '\\n')\n\n\ndef inspect_model(path_config: PathConfig, keys_only=True):\n    \"\"\"Inspect the contents of the models.\"\"\"\n\n    # Load the models using the provided config and weights paths\n    unet_model = load_unet_local(path_config.unet_config, MODEL_FILE)\n    text_encoder_model = load_text_encoder_local(MODEL_FILE)\n    autoencoder_model = load_autoencoder_local(MODEL_FILE)\n    diffusion_config = load_diffusion_config_local(path_config.diffusion_config)\n    tokenizer = load_tokenizer_local(path_config.tokenizer_vocab, path_config.tokenizer_merges)\n\n    # Convert the models' state_dict to a dictionary and iterate over it\n    for model_name, model in zip([\"unet\", \"text_encoder\", \"autoencoder\"], [unet_model, text_encoder_model, autoencoder_model]):\n        write_to_file(\"-\" * 50)\n        write_to_file(f\"Model: {model_name}\")\n        write_to_file(\"-\" * 50)",
    "suffix": ""
  },
  {
    "name": "ffmemes/ff-backend:src/storage/service.py@1154",
    "canonical_solution": "        .where(meme_source.c.type == MemeSourceType.TELEGRAM)",
    "prompt": "from typing import Any\nfrom datetime import datetime\nfrom sqlalchemy import select, nulls_first, text\nfrom sqlalchemy.dialects.postgresql import insert\nfrom src.database import (\n    language,\n    meme,\n    meme_source,\n    meme_raw_telegram,\n    meme_raw_vk,\n    execute, fetch_one, fetch_all,\n)\nfrom src.storage.parsers.schemas import TgChannelPostParsingResult, VkGroupPostParsingResult\nfrom src.storage.constants import (\n    MemeSourceType,\n    MemeSourceStatus,\n    MemeType,\n    MemeStatus,\n    MEME_RAW_TELEGRAM_MEME_SOURCE_POST_UNIQUE_CONSTRAINT,\n    MEME_RAW_VK_MEME_SOURCE_POST_UNIQUE_CONSTRAINT,\n)\n\n\n\nasync def insert_parsed_posts_from_telegram(\n    meme_source_id: int,\n    telegram_posts: list[TgChannelPostParsingResult],\n) -> None:\n    posts = [\n        post.model_dump() | {\"meme_source_id\": meme_source_id}\n        for post in telegram_posts\n    ]\n    insert_statement = insert(meme_raw_telegram).values(posts)\n    insert_posts_query = insert_statement.on_conflict_do_update(\n        constraint=MEME_RAW_TELEGRAM_MEME_SOURCE_POST_UNIQUE_CONSTRAINT,\n        set_={\n            \"media\": insert_statement.excluded.media,\n            \"views\": insert_statement.excluded.views,\n            \"updated_at\": datetime.utcnow(),\n        },\n    )\n\n    await execute(insert_posts_query)\n\n\nasync def insert_parsed_posts_from_vk(\n    meme_source_id: int,\n    vk_posts: list[VkGroupPostParsingResult],\n) -> None:\n    posts = [\n        post.model_dump() | {\"meme_source_id\": meme_source_id}\n        for post in vk_posts\n    ]\n    insert_statement = insert(meme_raw_vk).values(posts)\n    insert_posts_query = insert_statement.on_conflict_do_update(\n        constraint=MEME_RAW_VK_MEME_SOURCE_POST_UNIQUE_CONSTRAINT,\n        set_={\n            \"media\": insert_statement.excluded.media,\n            \"views\": insert_statement.excluded.views,\n            \"likes\": insert_statement.excluded.likes,\n            \"reposts\": insert_statement.excluded.reposts,\n            \"comments\": insert_statement.excluded.comments,\n            \"updated_at\": datetime.utcnow(),\n        },\n    )\n\n    await execute(insert_posts_query)\n\n\nasync def get_telegram_sources_to_parse(limit=10) -> list[dict[str, Any]]:\n    select_query = (\n        select(meme_source)",
    "prefix": "from typing import Any\nfrom datetime import datetime\nfrom sqlalchemy import select, nulls_first, text\nfrom sqlalchemy.dialects.postgresql import insert\nfrom src.database import (\n    language,\n    meme,\n    meme_source,\n    meme_raw_telegram,\n    meme_raw_vk,\n    execute, fetch_one, fetch_all,\n)\nfrom src.storage.parsers.schemas import TgChannelPostParsingResult, VkGroupPostParsingResult\nfrom src.storage.constants import (\n    MemeSourceType,\n    MemeSourceStatus,\n    MemeType,\n    MemeStatus,\n    MEME_RAW_TELEGRAM_MEME_SOURCE_POST_UNIQUE_CONSTRAINT,\n    MEME_RAW_VK_MEME_SOURCE_POST_UNIQUE_CONSTRAINT,\n)\n\n\n\nasync def insert_parsed_posts_from_telegram(\n    meme_source_id: int,\n    telegram_posts: list[TgChannelPostParsingResult],\n) -> None:\n    posts = [\n        post.model_dump() | {\"meme_source_id\": meme_source_id}\n        for post in telegram_posts\n    ]\n    insert_statement = insert(meme_raw_telegram).values(posts)\n    insert_posts_query = insert_statement.on_conflict_do_update(\n        constraint=MEME_RAW_TELEGRAM_MEME_SOURCE_POST_UNIQUE_CONSTRAINT,\n        set_={\n            \"media\": insert_statement.excluded.media,\n            \"views\": insert_statement.excluded.views,\n            \"updated_at\": datetime.utcnow(),\n        },\n    )\n\n    await execute(insert_posts_query)\n\n\nasync def insert_parsed_posts_from_vk(\n    meme_source_id: int,\n    vk_posts: list[VkGroupPostParsingResult],\n) -> None:\n    posts = [\n        post.model_dump() | {\"meme_source_id\": meme_source_id}\n        for post in vk_posts\n    ]\n    insert_statement = insert(meme_raw_vk).values(posts)\n    insert_posts_query = insert_statement.on_conflict_do_update(\n        constraint=MEME_RAW_VK_MEME_SOURCE_POST_UNIQUE_CONSTRAINT,\n        set_={\n            \"media\": insert_statement.excluded.media,\n            \"views\": insert_statement.excluded.views,\n            \"likes\": insert_statement.excluded.likes,\n            \"reposts\": insert_statement.excluded.reposts,\n            \"comments\": insert_statement.excluded.comments,\n            \"updated_at\": datetime.utcnow(),\n        },\n    )\n\n    await execute(insert_posts_query)\n\n\nasync def get_telegram_sources_to_parse(limit=10) -> list[dict[str, Any]]:\n    select_query = (\n        select(meme_source)",
    "suffix": ""
  },
  {
    "name": "Con6924/SPM:src/configs/prompt.py@1147",
    "canonical_solution": "            self.target = encode_prompts(tokenizer, text_encoder, [target_prompt])",
    "prompt": "from typing import Literal, Optional, Union\nfrom pathlib import Path\nfrom pydantic import BaseModel, root_validator\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom src.misc.clip_templates import imagenet_templates\nfrom src.engine.train_util import encode_prompts\nimport yaml\nimport pandas as pd\nimport random\nimport torch\n\n\n\n\nACTION_TYPES = Literal[\n    \"erase\",\n    \"erase_with_la\",\n]\n\nclass PromptEmbedsXL:\n    text_embeds: torch.FloatTensor\n    pooled_embeds: torch.FloatTensor\n\n    def __init__(self, embeds) -> None:\n        self.text_embeds, self.pooled_embeds = embeds\n\nPROMPT_EMBEDDING = Union[torch.FloatTensor, PromptEmbedsXL]\n\n\nclass PromptEmbedsCache:\n    prompts: dict[str, PROMPT_EMBEDDING] = {}\n\n    def __setitem__(self, __name: str, __value: PROMPT_EMBEDDING) -> None:\n        self.prompts[__name] = __value\n\n    def __getitem__(self, __name: str) -> Optional[PROMPT_EMBEDDING]:\n        if __name in self.prompts:\n            return self.prompts[__name]\n        else:\n            return None\n\n\nclass PromptSettings(BaseModel):  # yaml\n    target: str\n    positive: str = None  # if None, target will be used\n    unconditional: str = \"\"  # default is \"\"\n    neutral: str = None  # if None, unconditional will be used\n    action: ACTION_TYPES = \"erase\"  # default is \"erase\"\n    guidance_scale: float = 1.0  # default is 1.0\n    resolution: int = 512  # default is 512\n    dynamic_resolution: bool = False  # default is False\n    batch_size: int = 1  # default is 1\n    dynamic_crops: bool = False  # default is False. only used when model is XL\n    use_template: bool = False  # default is False\n    \n    la_strength: float = 1000.0\n    sampling_batch_size: int = 4\n\n    seed: int = None\n    case_number: int = 0\n\n    @root_validator(pre=True)\n    def fill_prompts(cls, values):\n        keys = values.keys()\n        if \"target\" not in keys:\n            raise ValueError(\"target must be specified\")\n        if \"positive\" not in keys:\n            values[\"positive\"] = values[\"target\"]\n        if \"unconditional\" not in keys:\n            values[\"unconditional\"] = \"\"\n        if \"neutral\" not in keys:\n            values[\"neutral\"] = values[\"unconditional\"]\n\n        return values\n\n\nclass PromptEmbedsPair:\n    target: PROMPT_EMBEDDING  # the concept that do not want to generate \n    positive: PROMPT_EMBEDDING  # generate the concept\n    unconditional: PROMPT_EMBEDDING  # uncondition (default should be empty)\n    neutral: PROMPT_EMBEDDING  # base condition (default should be empty)\n    use_template: bool = False  # use clip template or not\n\n    guidance_scale: float\n    resolution: int\n    dynamic_resolution: bool\n    batch_size: int\n    dynamic_crops: bool\n\n    loss_fn: torch.nn.Module\n    action: ACTION_TYPES\n\n    def __init__(\n        self,\n        loss_fn: torch.nn.Module,\n        target: PROMPT_EMBEDDING,\n        positive: PROMPT_EMBEDDING,\n        unconditional: PROMPT_EMBEDDING,\n        neutral: PROMPT_EMBEDDING,\n        settings: PromptSettings,\n    ) -> None:\n        self.loss_fn = loss_fn\n        self.target = target\n        self.positive = positive\n        self.unconditional = unconditional\n        self.neutral = neutral\n        \n        self.settings = settings\n\n        self.use_template = settings.use_template\n        self.guidance_scale = settings.guidance_scale\n        self.resolution = settings.resolution\n        self.dynamic_resolution = settings.dynamic_resolution\n        self.batch_size = settings.batch_size\n        self.dynamic_crops = settings.dynamic_crops\n        self.action = settings.action\n        \n        self.la_strength = settings.la_strength\n        self.sampling_batch_size = settings.sampling_batch_size\n        \n        \n    def _prepare_embeddings(\n        self, \n        cache: PromptEmbedsCache,\n        tokenizer: CLIPTokenizer,\n        text_encoder: CLIPTextModel,\n    ):\n        \"\"\"\n        Prepare embeddings for training. When use_template is True, the embeddings will be\n        format using a template, and then be processed by the model.\n        \"\"\"\n        if not self.use_template:\n            return\n        template = random.choice(imagenet_templates)\n        target_prompt = template.format(self.settings.target)\n        if cache[target_prompt]:\n            self.target = cache[target_prompt]\n        else:",
    "prefix": "from typing import Literal, Optional, Union\nfrom pathlib import Path\nfrom pydantic import BaseModel, root_validator\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom src.misc.clip_templates import imagenet_templates\nfrom src.engine.train_util import encode_prompts\nimport yaml\nimport pandas as pd\nimport random\nimport torch\n\n\n\n\nACTION_TYPES = Literal[\n    \"erase\",\n    \"erase_with_la\",\n]\n\nclass PromptEmbedsXL:\n    text_embeds: torch.FloatTensor\n    pooled_embeds: torch.FloatTensor\n\n    def __init__(self, embeds) -> None:\n        self.text_embeds, self.pooled_embeds = embeds\n\nPROMPT_EMBEDDING = Union[torch.FloatTensor, PromptEmbedsXL]\n\n\nclass PromptEmbedsCache:\n    prompts: dict[str, PROMPT_EMBEDDING] = {}\n\n    def __setitem__(self, __name: str, __value: PROMPT_EMBEDDING) -> None:\n        self.prompts[__name] = __value\n\n    def __getitem__(self, __name: str) -> Optional[PROMPT_EMBEDDING]:\n        if __name in self.prompts:\n            return self.prompts[__name]\n        else:\n            return None\n\n\nclass PromptSettings(BaseModel):  # yaml\n    target: str\n    positive: str = None  # if None, target will be used\n    unconditional: str = \"\"  # default is \"\"\n    neutral: str = None  # if None, unconditional will be used\n    action: ACTION_TYPES = \"erase\"  # default is \"erase\"\n    guidance_scale: float = 1.0  # default is 1.0\n    resolution: int = 512  # default is 512\n    dynamic_resolution: bool = False  # default is False\n    batch_size: int = 1  # default is 1\n    dynamic_crops: bool = False  # default is False. only used when model is XL\n    use_template: bool = False  # default is False\n    \n    la_strength: float = 1000.0\n    sampling_batch_size: int = 4\n\n    seed: int = None\n    case_number: int = 0\n\n    @root_validator(pre=True)\n    def fill_prompts(cls, values):\n        keys = values.keys()\n        if \"target\" not in keys:\n            raise ValueError(\"target must be specified\")\n        if \"positive\" not in keys:\n            values[\"positive\"] = values[\"target\"]\n        if \"unconditional\" not in keys:\n            values[\"unconditional\"] = \"\"\n        if \"neutral\" not in keys:\n            values[\"neutral\"] = values[\"unconditional\"]\n\n        return values\n\n\nclass PromptEmbedsPair:\n    target: PROMPT_EMBEDDING  # the concept that do not want to generate \n    positive: PROMPT_EMBEDDING  # generate the concept\n    unconditional: PROMPT_EMBEDDING  # uncondition (default should be empty)\n    neutral: PROMPT_EMBEDDING  # base condition (default should be empty)\n    use_template: bool = False  # use clip template or not\n\n    guidance_scale: float\n    resolution: int\n    dynamic_resolution: bool\n    batch_size: int\n    dynamic_crops: bool\n\n    loss_fn: torch.nn.Module\n    action: ACTION_TYPES\n\n    def __init__(\n        self,\n        loss_fn: torch.nn.Module,\n        target: PROMPT_EMBEDDING,\n        positive: PROMPT_EMBEDDING,\n        unconditional: PROMPT_EMBEDDING,\n        neutral: PROMPT_EMBEDDING,\n        settings: PromptSettings,\n    ) -> None:\n        self.loss_fn = loss_fn\n        self.target = target\n        self.positive = positive\n        self.unconditional = unconditional\n        self.neutral = neutral\n        \n        self.settings = settings\n\n        self.use_template = settings.use_template\n        self.guidance_scale = settings.guidance_scale\n        self.resolution = settings.resolution\n        self.dynamic_resolution = settings.dynamic_resolution\n        self.batch_size = settings.batch_size\n        self.dynamic_crops = settings.dynamic_crops\n        self.action = settings.action\n        \n        self.la_strength = settings.la_strength\n        self.sampling_batch_size = settings.sampling_batch_size\n        \n        \n    def _prepare_embeddings(\n        self, \n        cache: PromptEmbedsCache,\n        tokenizer: CLIPTokenizer,\n        text_encoder: CLIPTextModel,\n    ):\n        \"\"\"\n        Prepare embeddings for training. When use_template is True, the embeddings will be\n        format using a template, and then be processed by the model.\n        \"\"\"\n        if not self.use_template:\n            return\n        template = random.choice(imagenet_templates)\n        target_prompt = template.format(self.settings.target)\n        if cache[target_prompt]:\n            self.target = cache[target_prompt]\n        else:",
    "suffix": ""
  },
  {
    "name": "dakpinaroglu/Frame2seq:frame2seq/utils/score.py@1471",
    "canonical_solution": "            input_seqs = read_fasta_file(fasta_file)",
    "prompt": "import os\nimport torch\nfrom tqdm import tqdm\nfrom frame2seq.utils import residue_constants\nfrom frame2seq.utils.util import get_neg_pll, read_fasta_file\nfrom frame2seq.utils.pdb2input import get_inference_inputs\nfrom frame2seq.utils.pred2output import output_csv, output_indiv_csv\n\n\n\ndef score(self, pdb_file, chain_id, fasta_file, save_indiv_neg_pll):\n    temperature = 1.0\n    seq_mask, aatype, X = get_inference_inputs(pdb_file, chain_id)\n    seq_mask = seq_mask.to(self.device)\n    aatype = aatype.to(self.device)\n    X = X.to(self.device)\n    str_form = [residue_constants.ID_TO_AA[int(i)] for i in aatype[0]]\n    input_aatype_onehot = residue_constants.sequence_to_onehot(\n        sequence=str_form,\n        mapping=residue_constants.AA_TO_ID,\n    )\n    input_aatype_onehot = torch.from_numpy(input_aatype_onehot).float()\n    input_aatype_onehot = input_aatype_onehot.unsqueeze(0)\n    input_aatype_onehot = input_aatype_onehot.to(self.device)\n    input_aatype_onehot = torch.zeros_like(input_aatype_onehot)\n    input_aatype_onehot[:, :,\n                        20] = 1  # all positions are masked (set to unknown)\n    scores, preds = {}, []\n    with torch.no_grad():\n        pred_seq1 = self.models[0].forward(X, seq_mask, input_aatype_onehot)\n        pred_seq2 = self.models[1].forward(X, seq_mask, input_aatype_onehot)\n        pred_seq3 = self.models[2].forward(X, seq_mask, input_aatype_onehot)\n        pred_seq = (pred_seq1 + pred_seq2 + pred_seq3) / 3  # ensemble\n        pred_seq = pred_seq / temperature\n        pred_seq = torch.nn.functional.softmax(pred_seq, dim=-1)\n        pred_seq = pred_seq[seq_mask]\n        if fasta_file is not None:",
    "prefix": "import os\nimport torch\nfrom tqdm import tqdm\nfrom frame2seq.utils import residue_constants\nfrom frame2seq.utils.util import get_neg_pll, read_fasta_file\nfrom frame2seq.utils.pdb2input import get_inference_inputs\nfrom frame2seq.utils.pred2output import output_csv, output_indiv_csv\n\n\n\ndef score(self, pdb_file, chain_id, fasta_file, save_indiv_neg_pll):\n    temperature = 1.0\n    seq_mask, aatype, X = get_inference_inputs(pdb_file, chain_id)\n    seq_mask = seq_mask.to(self.device)\n    aatype = aatype.to(self.device)\n    X = X.to(self.device)\n    str_form = [residue_constants.ID_TO_AA[int(i)] for i in aatype[0]]\n    input_aatype_onehot = residue_constants.sequence_to_onehot(\n        sequence=str_form,\n        mapping=residue_constants.AA_TO_ID,\n    )\n    input_aatype_onehot = torch.from_numpy(input_aatype_onehot).float()\n    input_aatype_onehot = input_aatype_onehot.unsqueeze(0)\n    input_aatype_onehot = input_aatype_onehot.to(self.device)\n    input_aatype_onehot = torch.zeros_like(input_aatype_onehot)\n    input_aatype_onehot[:, :,\n                        20] = 1  # all positions are masked (set to unknown)\n    scores, preds = {}, []\n    with torch.no_grad():\n        pred_seq1 = self.models[0].forward(X, seq_mask, input_aatype_onehot)\n        pred_seq2 = self.models[1].forward(X, seq_mask, input_aatype_onehot)\n        pred_seq3 = self.models[2].forward(X, seq_mask, input_aatype_onehot)\n        pred_seq = (pred_seq1 + pred_seq2 + pred_seq3) / 3  # ensemble\n        pred_seq = pred_seq / temperature\n        pred_seq = torch.nn.functional.softmax(pred_seq, dim=-1)\n        pred_seq = pred_seq[seq_mask]\n        if fasta_file is not None:",
    "suffix": ""
  },
  {
    "name": "davep/oshit:oshit/app/oshit.py@1359",
    "canonical_solution": "        self.push_screen(Main())",
    "prompt": "from textual.app import App\nfrom .data import load_configuration, save_configuration\nfrom .screens import Main\n\"\"\"The main application class.\"\"\"\n\n##############################################################################\n# Textual imports.\n\n##############################################################################\n# Local imports.\n\n\n##############################################################################\nclass OSHit(App[None]):\n    \"\"\"The Orange Site Hit application.\"\"\"\n\n    ENABLE_COMMAND_PALETTE = False\n\n    def __init__(self) -> None:\n        \"\"\"Initialise the application.\"\"\"\n        super().__init__()\n        self.dark = load_configuration().dark_mode\n\n    def on_mount(self) -> None:\n        \"\"\"Get things going once the app is up and running.\"\"\"",
    "prefix": "from textual.app import App\nfrom .data import load_configuration, save_configuration\nfrom .screens import Main\n\"\"\"The main application class.\"\"\"\n\n##############################################################################\n# Textual imports.\n\n##############################################################################\n# Local imports.\n\n\n##############################################################################\nclass OSHit(App[None]):\n    \"\"\"The Orange Site Hit application.\"\"\"\n\n    ENABLE_COMMAND_PALETTE = False\n\n    def __init__(self) -> None:\n        \"\"\"Initialise the application.\"\"\"\n        super().__init__()\n        self.dark = load_configuration().dark_mode\n\n    def on_mount(self) -> None:\n        \"\"\"Get things going once the app is up and running.\"\"\"",
    "suffix": ""
  },
  {
    "name": "Maximilian-Winter/llama-cpp-agent:src/llama_cpp_agent/agent_memory/memory_tools.py@1362",
    "canonical_solution": "        self.retrieval_memory = RetrievalMemory(persistent_db_path, embedding_model_name, collection_name)",
    "prompt": "from pydantic import BaseModel, Field\nfrom ..function_calling import LlamaCppFunctionTool\nfrom .core_memory_manager import CoreMemoryManager\nfrom .retrieval_memory_manager import RetrievalMemoryManager, RetrievalMemory\n\n\n\nclass AddCoreMemory(BaseModel):\n    \"\"\"\n    Add a new entry to the core memory.\n    \"\"\"\n    key: str = Field(..., description=\"The key identifier for the core memory entry.\")\n    field: str = Field(..., description=\"A secondary key or field within the core memory entry.\")\n    value: str = Field(..., description=\"The value or data to be stored in the specified core memory entry.\")\n\n    def run(self, core_memory_manager: CoreMemoryManager):\n        return core_memory_manager.add_to_core_memory(self.key, self.field, self.value)\n\n\n# Replace Core Memory Model\nclass ReplaceCoreMemory(BaseModel):\n    \"\"\"\n    Replace an entry in the core memory.\n    \"\"\"\n    key: str = Field(..., description=\"The key identifier for the core memory entry.\")\n    field: str = Field(..., description=\"The specific field within the core memory entry to be replaced.\")\n    new_value: str = Field(...,\n                           description=\"The new value to replace the existing data in the specified core memory field.\")\n\n    def run(self, core_memory_manager: CoreMemoryManager):\n        return core_memory_manager.replace_in_core_memory(self.key, self.field, self.value)\n\n\nclass RemoveCoreMemory(BaseModel):\n    \"\"\"\n    Remove an entry in the core memory.\n    \"\"\"\n    key: str = Field(..., description=\"The key identifier for the core memory entry to be removed.\")\n    field: str = Field(..., description=\"The specific field within the core memory entry to be removed.\")\n\n    def run(self, core_memory_manager: CoreMemoryManager):\n        return core_memory_manager.remove_from_core_memory(self.key, self.field)\n\n\nclass RetrieveMemories(BaseModel):\n    \"\"\"\n    Retrieve memories from the retrieval memory based on a query.\n    \"\"\"\n    query: str = Field(..., description=\"The query to be used to retrieve memories from the retrieval memory.\")\n\n    def run(self, retrieval_memory_manager: RetrievalMemoryManager):\n        return retrieval_memory_manager.retrieve_memories(self.query)\n\n\nclass AddRetrievalMemory(BaseModel):\n    \"\"\"\n    Add memory to the retrieval memory.\n    \"\"\"\n    memory: str = Field(..., description=\"The memory to be added to the retrieval memory.\")\n    importance: float = Field(..., description=\"The importance of the memory to be added to the retrieval memory.\")\n\n    def run(self, retrieval_memory_manager: RetrievalMemoryManager):\n        return retrieval_memory_manager.add_memory_to_retrieval(self.memory, self.importance)\n\n\nclass AgentRetrievalMemory:\n    def __init__(self, persistent_db_path=\"./retrieval_memory\", embedding_model_name=\"all-MiniLM-L6-v2\",\n                 collection_name=\"retrieval_memory_collection\"):",
    "prefix": "from pydantic import BaseModel, Field\nfrom ..function_calling import LlamaCppFunctionTool\nfrom .core_memory_manager import CoreMemoryManager\nfrom .retrieval_memory_manager import RetrievalMemoryManager, RetrievalMemory\n\n\n\nclass AddCoreMemory(BaseModel):\n    \"\"\"\n    Add a new entry to the core memory.\n    \"\"\"\n    key: str = Field(..., description=\"The key identifier for the core memory entry.\")\n    field: str = Field(..., description=\"A secondary key or field within the core memory entry.\")\n    value: str = Field(..., description=\"The value or data to be stored in the specified core memory entry.\")\n\n    def run(self, core_memory_manager: CoreMemoryManager):\n        return core_memory_manager.add_to_core_memory(self.key, self.field, self.value)\n\n\n# Replace Core Memory Model\nclass ReplaceCoreMemory(BaseModel):\n    \"\"\"\n    Replace an entry in the core memory.\n    \"\"\"\n    key: str = Field(..., description=\"The key identifier for the core memory entry.\")\n    field: str = Field(..., description=\"The specific field within the core memory entry to be replaced.\")\n    new_value: str = Field(...,\n                           description=\"The new value to replace the existing data in the specified core memory field.\")\n\n    def run(self, core_memory_manager: CoreMemoryManager):\n        return core_memory_manager.replace_in_core_memory(self.key, self.field, self.value)\n\n\nclass RemoveCoreMemory(BaseModel):\n    \"\"\"\n    Remove an entry in the core memory.\n    \"\"\"\n    key: str = Field(..., description=\"The key identifier for the core memory entry to be removed.\")\n    field: str = Field(..., description=\"The specific field within the core memory entry to be removed.\")\n\n    def run(self, core_memory_manager: CoreMemoryManager):\n        return core_memory_manager.remove_from_core_memory(self.key, self.field)\n\n\nclass RetrieveMemories(BaseModel):\n    \"\"\"\n    Retrieve memories from the retrieval memory based on a query.\n    \"\"\"\n    query: str = Field(..., description=\"The query to be used to retrieve memories from the retrieval memory.\")\n\n    def run(self, retrieval_memory_manager: RetrievalMemoryManager):\n        return retrieval_memory_manager.retrieve_memories(self.query)\n\n\nclass AddRetrievalMemory(BaseModel):\n    \"\"\"\n    Add memory to the retrieval memory.\n    \"\"\"\n    memory: str = Field(..., description=\"The memory to be added to the retrieval memory.\")\n    importance: float = Field(..., description=\"The importance of the memory to be added to the retrieval memory.\")\n\n    def run(self, retrieval_memory_manager: RetrievalMemoryManager):\n        return retrieval_memory_manager.add_memory_to_retrieval(self.memory, self.importance)\n\n\nclass AgentRetrievalMemory:\n    def __init__(self, persistent_db_path=\"./retrieval_memory\", embedding_model_name=\"all-MiniLM-L6-v2\",\n                 collection_name=\"retrieval_memory_collection\"):",
    "suffix": ""
  },
  {
    "name": "tedivm/paracelsus:paracelsus/cli.py@1289",
    "canonical_solution": "    \"dot\": Dot,",
    "prompt": "import importlib\nimport re\nimport sys\nimport typer\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import List\nfrom typing_extensions import Annotated\nfrom .transformers.dot import Dot\nfrom .transformers.mermaid import Mermaid\n    from . import _version\n\n\n\napp = typer.Typer()\n\ntransformers = {\n    \"mmd\": Mermaid,\n    \"mermaid\": Mermaid,",
    "prefix": "import importlib\nimport re\nimport sys\nimport typer\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import List\nfrom typing_extensions import Annotated\nfrom .transformers.dot import Dot\nfrom .transformers.mermaid import Mermaid\n    from . import _version\n\n\n\napp = typer.Typer()\n\ntransformers = {\n    \"mmd\": Mermaid,\n    \"mermaid\": Mermaid,",
    "suffix": ""
  },
  {
    "name": "winniesi/tg-gemini-bot:api/handle.py@971",
    "canonical_solution": "        send_message(update.from_id, \"\ud83d\ude2b You are not allowed to use this bot.\")",
    "prompt": "from .auth import is_authorized\nfrom .context import ChatManager, ImageChatManger\nfrom .telegram import Update, send_message\n\"\"\"\nAll the chat that comes through the Telegram bot gets passed to the\nhandle_message function. This function checks out if the user has the\ngreen light to chat with the bot. Once that's sorted, it figures out if\nthe user sent words or an image and deals with it accordingly.\n\nFor text messages, it fires up the ChatManager class that keeps track of\nthe back-and-forth with that user.\n\nAs for images, in Gemini pro, they're context-free, so you can handle\nthem pretty straight-up without much fuss.\n\"\"\"\n\n\nchat_manager = ChatManager()\n\n\ndef handle_message(update_data):\n    update = Update(update_data)\n    authorized = is_authorized(update.from_id, update.user_name)\n    if not authorized:",
    "prefix": "from .auth import is_authorized\nfrom .context import ChatManager, ImageChatManger\nfrom .telegram import Update, send_message\n\"\"\"\nAll the chat that comes through the Telegram bot gets passed to the\nhandle_message function. This function checks out if the user has the\ngreen light to chat with the bot. Once that's sorted, it figures out if\nthe user sent words or an image and deals with it accordingly.\n\nFor text messages, it fires up the ChatManager class that keeps track of\nthe back-and-forth with that user.\n\nAs for images, in Gemini pro, they're context-free, so you can handle\nthem pretty straight-up without much fuss.\n\"\"\"\n\n\nchat_manager = ChatManager()\n\n\ndef handle_message(update_data):\n    update = Update(update_data)\n    authorized = is_authorized(update.from_id, update.user_name)\n    if not authorized:",
    "suffix": ""
  },
  {
    "name": "usail-hkust/LLMTSCS:run_advanced_maxpressure.py@1154",
    "canonical_solution": "            raise error.flowFileException('Flow file does not exist.')",
    "prompt": "from utils.utils import oneline_wrapper\nfrom utils import error\nfrom multiprocessing import Process\nimport os\nimport time\nimport argparse\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--memo\", type=str, default='AdvancedMaxPressure')\n    parser.add_argument(\"--model\", type=str, default=\"AdvancedMaxPressure\")\n    parser.add_argument(\"--proj_name\", type=str, default=\"chatgpt-TSCS\")\n    parser.add_argument(\"--eightphase\", action=\"store_true\", default=False)\n    parser.add_argument(\"--multi_process\", action=\"store_true\", default=True)\n    parser.add_argument(\"--workers\", type=int, default=1)\n    parser.add_argument(\"--dataset\", type=str, default=\"template\")\n    parser.add_argument(\"--traffic_file\", type=str, default=\"flow_main_stream.json\")\n\n    return parser.parse_args()\n\n\ndef main(in_args):\n    traffic_file_list = []\n\n    if in_args.dataset == 'jinan':\n        count = 3600\n        road_net = \"3_4\"\n        traffic_file_list = [\"anon_3_4_jinan_real.json\", \"anon_3_4_jinan_real_2000.json\", \"anon_3_4_jinan_real_2500.json\"]\n        template = \"Jinan\"\n    elif in_args.dataset == 'hangzhou':\n        count = 3600\n        road_net = \"4_4\"\n        traffic_file_list = [\"anon_4_4_hangzhou_real.json\", \"anon_4_4_hangzhou_real_5816.json\"]\n        template = \"Hangzhou\"\n    elif in_args.dataset == 'newyork_16x3':\n        count = 3600\n        road_net = \"16_3\"\n        traffic_file_list = [\"anon_16_3_newyork_real.json\"]\n        template = \"NewYork\"\n    elif in_args.dataset == 'newyork_28x7':\n        count = 3600\n        road_net = \"28_7\"\n        traffic_file_list = [\"anon_28_7_newyork_real_double.json\", \"anon_28_7_newyork_real_triple.json\"]\n        template = \"NewYork\"\n    elif in_args.dataset == 'template':\n        count = 3600\n        road_net = \"1_1\"\n        traffic_file_list = [\"flow_main_stream.json\"]\n        template = \"template\"\n\n    # flow_file error\n    try:\n        if in_args.traffic_file not in traffic_file_list:",
    "prefix": "from utils.utils import oneline_wrapper\nfrom utils import error\nfrom multiprocessing import Process\nimport os\nimport time\nimport argparse\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--memo\", type=str, default='AdvancedMaxPressure')\n    parser.add_argument(\"--model\", type=str, default=\"AdvancedMaxPressure\")\n    parser.add_argument(\"--proj_name\", type=str, default=\"chatgpt-TSCS\")\n    parser.add_argument(\"--eightphase\", action=\"store_true\", default=False)\n    parser.add_argument(\"--multi_process\", action=\"store_true\", default=True)\n    parser.add_argument(\"--workers\", type=int, default=1)\n    parser.add_argument(\"--dataset\", type=str, default=\"template\")\n    parser.add_argument(\"--traffic_file\", type=str, default=\"flow_main_stream.json\")\n\n    return parser.parse_args()\n\n\ndef main(in_args):\n    traffic_file_list = []\n\n    if in_args.dataset == 'jinan':\n        count = 3600\n        road_net = \"3_4\"\n        traffic_file_list = [\"anon_3_4_jinan_real.json\", \"anon_3_4_jinan_real_2000.json\", \"anon_3_4_jinan_real_2500.json\"]\n        template = \"Jinan\"\n    elif in_args.dataset == 'hangzhou':\n        count = 3600\n        road_net = \"4_4\"\n        traffic_file_list = [\"anon_4_4_hangzhou_real.json\", \"anon_4_4_hangzhou_real_5816.json\"]\n        template = \"Hangzhou\"\n    elif in_args.dataset == 'newyork_16x3':\n        count = 3600\n        road_net = \"16_3\"\n        traffic_file_list = [\"anon_16_3_newyork_real.json\"]\n        template = \"NewYork\"\n    elif in_args.dataset == 'newyork_28x7':\n        count = 3600\n        road_net = \"28_7\"\n        traffic_file_list = [\"anon_28_7_newyork_real_double.json\", \"anon_28_7_newyork_real_triple.json\"]\n        template = \"NewYork\"\n    elif in_args.dataset == 'template':\n        count = 3600\n        road_net = \"1_1\"\n        traffic_file_list = [\"flow_main_stream.json\"]\n        template = \"template\"\n\n    # flow_file error\n    try:\n        if in_args.traffic_file not in traffic_file_list:",
    "suffix": ""
  },
  {
    "name": "ohadmata/shmessy:src/shmessy/types/unix_timestamp.py@669",
    "canonical_solution": "    def validate(self, data: ndarray) -> Optional[InferredField]:",
    "prompt": "import logging\nimport math\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Optional\nfrom numpy import ndarray\nfrom pandas import Series, to_datetime\nfrom ..schema import InferredField, ValidatorTypes\nfrom .base import BaseType\n\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass TimestampResolution(str, Enum):\n    SECONDS = \"s\"\n    MILLISECONDS = \"ms\"\n    NANOSECONDS = \"ns\"\n\n\nclass UnixTimestampType(BaseType):\n    weight = 4\n    validator_types = (ValidatorTypes.NUMERIC,)\n    min_valid_year: int = 1980\n    max_valid_year: int = 2100\n\n    @staticmethod\n    def _unix_timestamp_resolution(value: float) -> TimestampResolution:\n        number_of_digits = len(str(int(value)))\n        if number_of_digits == 10:\n            return TimestampResolution.SECONDS\n        if number_of_digits == 13:\n            return TimestampResolution.MILLISECONDS\n        if number_of_digits == 16:\n            return TimestampResolution.NANOSECONDS\n\n    @staticmethod\n    def _fix_input_resolution(\n        value: float, selected_resolution: TimestampResolution\n    ) -> float:\n        if selected_resolution == TimestampResolution.SECONDS:\n            return value\n        if selected_resolution == TimestampResolution.MILLISECONDS:\n            return value / 1000\n        if selected_resolution == TimestampResolution.NANOSECONDS:\n            return value / 1000 / 1000\n",
    "prefix": "import logging\nimport math\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Optional\nfrom numpy import ndarray\nfrom pandas import Series, to_datetime\nfrom ..schema import InferredField, ValidatorTypes\nfrom .base import BaseType\n\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass TimestampResolution(str, Enum):\n    SECONDS = \"s\"\n    MILLISECONDS = \"ms\"\n    NANOSECONDS = \"ns\"\n\n\nclass UnixTimestampType(BaseType):\n    weight = 4\n    validator_types = (ValidatorTypes.NUMERIC,)\n    min_valid_year: int = 1980\n    max_valid_year: int = 2100\n\n    @staticmethod\n    def _unix_timestamp_resolution(value: float) -> TimestampResolution:\n        number_of_digits = len(str(int(value)))\n        if number_of_digits == 10:\n            return TimestampResolution.SECONDS\n        if number_of_digits == 13:\n            return TimestampResolution.MILLISECONDS\n        if number_of_digits == 16:\n            return TimestampResolution.NANOSECONDS\n\n    @staticmethod\n    def _fix_input_resolution(\n        value: float, selected_resolution: TimestampResolution\n    ) -> float:\n        if selected_resolution == TimestampResolution.SECONDS:\n            return value\n        if selected_resolution == TimestampResolution.MILLISECONDS:\n            return value / 1000\n        if selected_resolution == TimestampResolution.NANOSECONDS:\n            return value / 1000 / 1000\n",
    "suffix": ""
  },
  {
    "name": "kokiez/solana-sniper:monitor_price_strategy.py@1376",
    "canonical_solution": "        bought_token_curr_price = get_price(desired_token_address)\r",
    "prompt": "import time\r\nfrom birdeye import get_price, getSymbol\r\nfrom webhook import sendWebhook\r\n\r\n\r\n\r\n\"\"\"If you have ton of trades then best to use Simulate Transaction and modify this part of code to your needs\"\"\"\r\n\r\n\r\n\"\"\"\r\nOnly Take Profit\r\n\"\"\"\r\ndef limit_order(bought_token_price,desired_token_address, take_profit_ratio, execution_time, txB):\r\n    token_symbol, SOl_Symbol = getSymbol(desired_token_address)\r\n    # CALCULATE SELL LIMIT\r\n    sell_limit_token_price = bought_token_price  *  take_profit_ratio\r\n    \r\n    print(\"-\" * 79)\r\n    print(f\"| {'Bought Price':<12} | {'Sell Limit':<12} |  {'Tx Buy':<50} |\")\r\n    print(\"-\" * 79)\r\n    print(f\"|{bought_token_price:.12f} | {sell_limit_token_price:.12f}  {txB:<50} |\")\r\n    print(\"-\" * 79)\r\n\r\n    sendWebhook(f\"msg_b|BUY INFO {token_symbol}\",f\"Bought Price: {bought_token_price:.12f}\\n**Sell Limit: {sell_limit_token_price:.15f}**\\nTotal Buy Execution time: {execution_time} seconds\\nBuy TXN: https://solscan.io/tx/{txB} |\")\r\n\r\n    # LOOP = CHECK IF PRICE >= SELL LIMIT |  checks price every 5 seconds\r\n    priceLow = True\r\n    # while priceLow and isTimePassed(time_limit) == False:\r\n    while priceLow:\r\n        # Check if time limit has been passed for the token bought or not\r",
    "prefix": "import time\r\nfrom birdeye import get_price, getSymbol\r\nfrom webhook import sendWebhook\r\n\r\n\r\n\r\n\"\"\"If you have ton of trades then best to use Simulate Transaction and modify this part of code to your needs\"\"\"\r\n\r\n\r\n\"\"\"\r\nOnly Take Profit\r\n\"\"\"\r\ndef limit_order(bought_token_price,desired_token_address, take_profit_ratio, execution_time, txB):\r\n    token_symbol, SOl_Symbol = getSymbol(desired_token_address)\r\n    # CALCULATE SELL LIMIT\r\n    sell_limit_token_price = bought_token_price  *  take_profit_ratio\r\n    \r\n    print(\"-\" * 79)\r\n    print(f\"| {'Bought Price':<12} | {'Sell Limit':<12} |  {'Tx Buy':<50} |\")\r\n    print(\"-\" * 79)\r\n    print(f\"|{bought_token_price:.12f} | {sell_limit_token_price:.12f}  {txB:<50} |\")\r\n    print(\"-\" * 79)\r\n\r\n    sendWebhook(f\"msg_b|BUY INFO {token_symbol}\",f\"Bought Price: {bought_token_price:.12f}\\n**Sell Limit: {sell_limit_token_price:.15f}**\\nTotal Buy Execution time: {execution_time} seconds\\nBuy TXN: https://solscan.io/tx/{txB} |\")\r\n\r\n    # LOOP = CHECK IF PRICE >= SELL LIMIT |  checks price every 5 seconds\r\n    priceLow = True\r\n    # while priceLow and isTimePassed(time_limit) == False:\r\n    while priceLow:\r\n        # Check if time limit has been passed for the token bought or not\r",
    "suffix": ""
  },
  {
    "name": "enochyearn/MLX_RoBERTa:mlx_roberta.py@1439",
    "canonical_solution": "        self.LayerNorm = LayerNormTorchAlike(config.hidden_size, eps=config.layer_norm_eps, correction=True)",
    "prompt": "import argparse\nimport time\nimport mlx.core as mx\nimport mlx.nn as nn\nimport numpy as np\nimport math\nfrom mlx.utils import tree_unflatten\nfrom collections import OrderedDict\nfrom custom.nn.layers.normalization import LayerNormBasselCorrected, LayerNormTorchAlike\nfrom transformers import RobertaTokenizer\nfrom dataclasses import dataclass\n\n\n\n\n\n\n# utils\n\n\n@dataclass\nclass ModelConfig:\n    intermediate_size: int = 3072\n    hidden_size: int = 768\n    no_heads: int = 12\n    hidden_layers: int = 12\n    vocab_size: int = 50265\n    attention_probs_dropout_prob: float = 0.1\n    hidden_dropout_prob: float = 0.1\n    layer_norm_eps: float = 1e-5\n    max_position_embeddings: int = 514\n    # QA model's parameters\n    num_labels: int = 2\n    type_vocab_size: int = 2\n    pad_token_id: int = 1\n    chunk_size_feed_forward: int = 0\n\n\nmodel_configs = {\n    \"deepset/roberta-base-squad2\": ModelConfig(),\n    \"roberta-base\": ModelConfig(),\n}\n\nmodel_types = {\n    \"deepset/roberta-base-squad2\": \"qa\",\n    \"roberta-base\": \"base\",\n}\n\nclass RobertaEmbeddings(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "prefix": "import argparse\nimport time\nimport mlx.core as mx\nimport mlx.nn as nn\nimport numpy as np\nimport math\nfrom mlx.utils import tree_unflatten\nfrom collections import OrderedDict\nfrom custom.nn.layers.normalization import LayerNormBasselCorrected, LayerNormTorchAlike\nfrom transformers import RobertaTokenizer\nfrom dataclasses import dataclass\n\n\n\n\n\n\n# utils\n\n\n@dataclass\nclass ModelConfig:\n    intermediate_size: int = 3072\n    hidden_size: int = 768\n    no_heads: int = 12\n    hidden_layers: int = 12\n    vocab_size: int = 50265\n    attention_probs_dropout_prob: float = 0.1\n    hidden_dropout_prob: float = 0.1\n    layer_norm_eps: float = 1e-5\n    max_position_embeddings: int = 514\n    # QA model's parameters\n    num_labels: int = 2\n    type_vocab_size: int = 2\n    pad_token_id: int = 1\n    chunk_size_feed_forward: int = 0\n\n\nmodel_configs = {\n    \"deepset/roberta-base-squad2\": ModelConfig(),\n    \"roberta-base\": ModelConfig(),\n}\n\nmodel_types = {\n    \"deepset/roberta-base-squad2\": \"qa\",\n    \"roberta-base\": \"base\",\n}\n\nclass RobertaEmbeddings(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "suffix": ""
  },
  {
    "name": "zy7y/dfs-generate:main.py@789",
    "canonical_solution": "@app.get(\"/tables\", response_model=RList[Table])",
    "prompt": "from fastapi import FastAPI, Query\nfrom fastapi.requests import Request\nfrom fastapi.responses import FileResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom entity import CodeGen, Conf, DBConf, R, RList, Table\nfrom generate.main import generate_code\n    import uvicorn\n\n\napp = FastAPI(\n    title=\"dfs-generate\", description=\"FastAPI SQLModel \u9006\u5411\u751f\u6210\u4ee3\u7801\", docs_url=None\n)\n\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n\n@app.get(\"/\", include_in_schema=False)\ndef index():\n    return FileResponse(\"static/index.html\")\n\n",
    "prefix": "from fastapi import FastAPI, Query\nfrom fastapi.requests import Request\nfrom fastapi.responses import FileResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom entity import CodeGen, Conf, DBConf, R, RList, Table\nfrom generate.main import generate_code\n    import uvicorn\n\n\napp = FastAPI(\n    title=\"dfs-generate\", description=\"FastAPI SQLModel \u9006\u5411\u751f\u6210\u4ee3\u7801\", docs_url=None\n)\n\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n\n@app.get(\"/\", include_in_schema=False)\ndef index():\n    return FileResponse(\"static/index.html\")\n\n",
    "suffix": ""
  },
  {
    "name": "CrawlScript/Torch-MGDCF:torch_mgdcf/evaluation/ranking.py@765",
    "canonical_solution": "    v_search = VectorSearchEngine(item_embedding)",
    "prompt": "from tqdm import tqdm\nfrom torch_mgdcf.metrics.ranking import ndcg_score, precision_score, recall_score\nfrom torch_mgdcf.vector_search.vector_search import VectorSearchEngine\nimport numpy as np\nimport torch\n# coding=utf-8\n# The code is from our another project GRecX: https://github.com/maenzhier/grecx_datasets\n\n\n\n\ndef score(ground_truth, pred_items, k_list, metrics):\n    pred_match = [1 if item in ground_truth else 0 for item in pred_items]\n\n    max_k = k_list[-1]\n    if len(ground_truth) > max_k:\n        ndcg_gold = [1] * max_k\n    else:\n        ndcg_gold = [1] * len(ground_truth) + [0] * (max_k - len(ground_truth))\n\n    res_score = []\n    for metric in metrics:\n        if metric == \"ndcg\":\n            score_func = ndcg_score\n        elif metric == \"precision\":\n            score_func = precision_score\n        elif metric == \"recall\":\n            score_func = recall_score\n        else:\n            raise Exception(\"Not Found Metric : {}\".format(metric))\n\n        for k in k_list:\n            if metric == \"ndcg\":\n                res_score.append(score_func(ndcg_gold[:k], pred_match[:k]))\n            else:\n                res_score.append(score_func(ground_truth, pred_match[:k]))\n\n    return res_score\n\n\ndef evaluate_mean_global_metrics(user_items_dict, user_mask_items_dict,\n                                 user_embedding, item_embedding,\n                                 k_list=[10, 20], metrics=[\"ndcg\"]):\n",
    "prefix": "from tqdm import tqdm\nfrom torch_mgdcf.metrics.ranking import ndcg_score, precision_score, recall_score\nfrom torch_mgdcf.vector_search.vector_search import VectorSearchEngine\nimport numpy as np\nimport torch\n# coding=utf-8\n# The code is from our another project GRecX: https://github.com/maenzhier/grecx_datasets\n\n\n\n\ndef score(ground_truth, pred_items, k_list, metrics):\n    pred_match = [1 if item in ground_truth else 0 for item in pred_items]\n\n    max_k = k_list[-1]\n    if len(ground_truth) > max_k:\n        ndcg_gold = [1] * max_k\n    else:\n        ndcg_gold = [1] * len(ground_truth) + [0] * (max_k - len(ground_truth))\n\n    res_score = []\n    for metric in metrics:\n        if metric == \"ndcg\":\n            score_func = ndcg_score\n        elif metric == \"precision\":\n            score_func = precision_score\n        elif metric == \"recall\":\n            score_func = recall_score\n        else:\n            raise Exception(\"Not Found Metric : {}\".format(metric))\n\n        for k in k_list:\n            if metric == \"ndcg\":\n                res_score.append(score_func(ndcg_gold[:k], pred_match[:k]))\n            else:\n                res_score.append(score_func(ground_truth, pred_match[:k]))\n\n    return res_score\n\n\ndef evaluate_mean_global_metrics(user_items_dict, user_mask_items_dict,\n                                 user_embedding, item_embedding,\n                                 k_list=[10, 20], metrics=[\"ndcg\"]):\n",
    "suffix": ""
  },
  {
    "name": "KyanChen/TTP:opencd/models/data_preprocessor.py@1234",
    "canonical_solution": "@MODELS.register_module()",
    "prompt": "from numbers import Number\nfrom typing import Any, Dict, List, Optional, Sequence, Union\nfrom mmengine.model import BaseDataPreprocessor\nfrom mmseg.utils import SampleList\nfrom opencd.registry import MODELS\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n# Copyright (c) Open-CD. All rights reserved.\n\n\n\n\ndef stack_batch(inputs: List[torch.Tensor],\n                data_samples: Optional[SampleList] = None,\n                size: Optional[tuple] = None,\n                size_divisor: Optional[int] = None,\n                pad_val: Union[int, float] = 0,\n                seg_pad_val: Union[int, float] = 255) -> torch.Tensor:\n    \"\"\"Stack multiple inputs to form a batch and pad the images and gt_sem_segs\n    to the max shape use the right bottom padding mode.\n\n    Args:\n        inputs (List[Tensor]): The input multiple tensors. each is a\n            CHW 3D-tensor.\n        data_samples (list[:obj:`SegDataSample`]): The list of data samples.\n            It usually includes information such as `gt_sem_seg`.\n        size (tuple, optional): Fixed padding size.\n        size_divisor (int, optional): The divisor of padded size.\n        pad_val (int, float): The padding value. Defaults to 0\n        seg_pad_val (int, float): The padding value. Defaults to 255\n\n    Returns:\n       Tensor: The 4D-tensor.\n       List[:obj:`SegDataSample`]: After the padding of the gt_seg_map.\n    \"\"\"\n    assert isinstance(inputs, list), \\\n        f'Expected input type to be list, but got {type(inputs)}'\n    assert len({tensor.ndim for tensor in inputs}) == 1, \\\n        f'Expected the dimensions of all inputs must be the same, ' \\\n        f'but got {[tensor.ndim for tensor in inputs]}'\n    assert inputs[0].ndim == 3, f'Expected tensor dimension to be 3, ' \\\n        f'but got {inputs[0].ndim}'\n    assert len({tensor.shape[0] for tensor in inputs}) == 1, \\\n        f'Expected the channels of all inputs must be the same, ' \\\n        f'but got {[tensor.shape[0] for tensor in inputs]}'\n\n    # only one of size and size_divisor should be valid\n    assert (size is not None) ^ (size_divisor is not None), \\\n        'only one of size and size_divisor should be valid'\n\n    padded_inputs = []\n    padded_samples = []\n    inputs_sizes = [(img.shape[-2], img.shape[-1]) for img in inputs]\n    max_size = np.stack(inputs_sizes).max(0)\n    if size_divisor is not None and size_divisor > 1:\n        # the last two dims are H,W, both subject to divisibility requirement\n        max_size = (max_size +\n                    (size_divisor - 1)) // size_divisor * size_divisor\n\n    for i in range(len(inputs)):\n        tensor = inputs[i]\n        if size is not None:\n            width = max(size[-1] - tensor.shape[-1], 0)\n            height = max(size[-2] - tensor.shape[-2], 0)\n            # (padding_left, padding_right, padding_top, padding_bottom)\n            padding_size = (0, width, 0, height)\n        elif size_divisor is not None:\n            width = max(max_size[-1] - tensor.shape[-1], 0)\n            height = max(max_size[-2] - tensor.shape[-2], 0)\n            padding_size = (0, width, 0, height)\n        else:\n            padding_size = [0, 0, 0, 0]\n\n        # pad img\n        pad_img = F.pad(tensor, padding_size, value=pad_val)\n        padded_inputs.append(pad_img)\n        # pad gt_sem_seg\n        if data_samples is not None:\n            data_sample = data_samples[i]\n            gt_sem_seg = data_sample.gt_sem_seg.data\n            del data_sample.gt_sem_seg.data\n            data_sample.gt_sem_seg.data = F.pad(\n                gt_sem_seg, padding_size, value=seg_pad_val)\n            if 'gt_edge_map' in data_sample:\n                gt_edge_map = data_sample.gt_edge_map.data\n                del data_sample.gt_edge_map.data\n                data_sample.gt_edge_map.data = F.pad(\n                    gt_edge_map, padding_size, value=seg_pad_val)\n            if 'gt_seg_map_from' in data_sample:\n                gt_seg_map_from = data_sample.gt_seg_map_from.data\n                del data_sample.gt_seg_map_from.data\n                data_sample.gt_seg_map_from.data = F.pad(\n                    gt_seg_map_from, padding_size, value=seg_pad_val)\n            if 'gt_seg_map_to' in data_sample:\n                gt_seg_map_to = data_sample.gt_seg_map_to.data\n                del data_sample.gt_seg_map_to.data\n                data_sample.gt_seg_map_to.data = F.pad(\n                    gt_seg_map_to, padding_size, value=seg_pad_val)\n            data_sample.set_metainfo({\n                'img_shape': tensor.shape[-2:],\n                'pad_shape': data_sample.gt_sem_seg.shape,\n                'padding_size': padding_size\n            })\n            padded_samples.append(data_sample)\n        else:\n            padded_samples.append(\n                dict(\n                    img_padding_size=padding_size,\n                    pad_shape=pad_img.shape[-2:]))\n\n    return torch.stack(padded_inputs, dim=0), padded_samples\n\n",
    "prefix": "from numbers import Number\nfrom typing import Any, Dict, List, Optional, Sequence, Union\nfrom mmengine.model import BaseDataPreprocessor\nfrom mmseg.utils import SampleList\nfrom opencd.registry import MODELS\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n# Copyright (c) Open-CD. All rights reserved.\n\n\n\n\ndef stack_batch(inputs: List[torch.Tensor],\n                data_samples: Optional[SampleList] = None,\n                size: Optional[tuple] = None,\n                size_divisor: Optional[int] = None,\n                pad_val: Union[int, float] = 0,\n                seg_pad_val: Union[int, float] = 255) -> torch.Tensor:\n    \"\"\"Stack multiple inputs to form a batch and pad the images and gt_sem_segs\n    to the max shape use the right bottom padding mode.\n\n    Args:\n        inputs (List[Tensor]): The input multiple tensors. each is a\n            CHW 3D-tensor.\n        data_samples (list[:obj:`SegDataSample`]): The list of data samples.\n            It usually includes information such as `gt_sem_seg`.\n        size (tuple, optional): Fixed padding size.\n        size_divisor (int, optional): The divisor of padded size.\n        pad_val (int, float): The padding value. Defaults to 0\n        seg_pad_val (int, float): The padding value. Defaults to 255\n\n    Returns:\n       Tensor: The 4D-tensor.\n       List[:obj:`SegDataSample`]: After the padding of the gt_seg_map.\n    \"\"\"\n    assert isinstance(inputs, list), \\\n        f'Expected input type to be list, but got {type(inputs)}'\n    assert len({tensor.ndim for tensor in inputs}) == 1, \\\n        f'Expected the dimensions of all inputs must be the same, ' \\\n        f'but got {[tensor.ndim for tensor in inputs]}'\n    assert inputs[0].ndim == 3, f'Expected tensor dimension to be 3, ' \\\n        f'but got {inputs[0].ndim}'\n    assert len({tensor.shape[0] for tensor in inputs}) == 1, \\\n        f'Expected the channels of all inputs must be the same, ' \\\n        f'but got {[tensor.shape[0] for tensor in inputs]}'\n\n    # only one of size and size_divisor should be valid\n    assert (size is not None) ^ (size_divisor is not None), \\\n        'only one of size and size_divisor should be valid'\n\n    padded_inputs = []\n    padded_samples = []\n    inputs_sizes = [(img.shape[-2], img.shape[-1]) for img in inputs]\n    max_size = np.stack(inputs_sizes).max(0)\n    if size_divisor is not None and size_divisor > 1:\n        # the last two dims are H,W, both subject to divisibility requirement\n        max_size = (max_size +\n                    (size_divisor - 1)) // size_divisor * size_divisor\n\n    for i in range(len(inputs)):\n        tensor = inputs[i]\n        if size is not None:\n            width = max(size[-1] - tensor.shape[-1], 0)\n            height = max(size[-2] - tensor.shape[-2], 0)\n            # (padding_left, padding_right, padding_top, padding_bottom)\n            padding_size = (0, width, 0, height)\n        elif size_divisor is not None:\n            width = max(max_size[-1] - tensor.shape[-1], 0)\n            height = max(max_size[-2] - tensor.shape[-2], 0)\n            padding_size = (0, width, 0, height)\n        else:\n            padding_size = [0, 0, 0, 0]\n\n        # pad img\n        pad_img = F.pad(tensor, padding_size, value=pad_val)\n        padded_inputs.append(pad_img)\n        # pad gt_sem_seg\n        if data_samples is not None:\n            data_sample = data_samples[i]\n            gt_sem_seg = data_sample.gt_sem_seg.data\n            del data_sample.gt_sem_seg.data\n            data_sample.gt_sem_seg.data = F.pad(\n                gt_sem_seg, padding_size, value=seg_pad_val)\n            if 'gt_edge_map' in data_sample:\n                gt_edge_map = data_sample.gt_edge_map.data\n                del data_sample.gt_edge_map.data\n                data_sample.gt_edge_map.data = F.pad(\n                    gt_edge_map, padding_size, value=seg_pad_val)\n            if 'gt_seg_map_from' in data_sample:\n                gt_seg_map_from = data_sample.gt_seg_map_from.data\n                del data_sample.gt_seg_map_from.data\n                data_sample.gt_seg_map_from.data = F.pad(\n                    gt_seg_map_from, padding_size, value=seg_pad_val)\n            if 'gt_seg_map_to' in data_sample:\n                gt_seg_map_to = data_sample.gt_seg_map_to.data\n                del data_sample.gt_seg_map_to.data\n                data_sample.gt_seg_map_to.data = F.pad(\n                    gt_seg_map_to, padding_size, value=seg_pad_val)\n            data_sample.set_metainfo({\n                'img_shape': tensor.shape[-2:],\n                'pad_shape': data_sample.gt_sem_seg.shape,\n                'padding_size': padding_size\n            })\n            padded_samples.append(data_sample)\n        else:\n            padded_samples.append(\n                dict(\n                    img_padding_size=padding_size,\n                    pad_shape=pad_img.shape[-2:]))\n\n    return torch.stack(padded_inputs, dim=0), padded_samples\n\n",
    "suffix": ""
  },
  {
    "name": "N0rz3/Phunter:lib/lookup.py@809",
    "canonical_solution": "    await free(str(phone_number).replace(\"+\", \"\"))\r",
    "prompt": "import phonenumbers\r\nimport json\r\nfrom phonenumbers import carrier\r\nfrom .reputation import *\r\nfrom .free_lookup import free\r\nfrom .spam import spamcalls\r\nfrom lib.text import *\r\n\r\n\r\nasync def lookup(phone_number):\r\n    print()\r\n    parsed = phonenumbers.parse(phone_number)\r\n\r\n    operator = carrier.name_for_number(parsed, \"fr\")\r\n    line = phonenumbers.number_type(parsed)\r\n\r\n    if line == phonenumbers.PhoneNumberType.FIXED_LINE:\r\n        ligne = f\" [{GREEN}>{WHITE}] Line type: Fixed\"\r\n\r\n    elif line == phonenumbers.PhoneNumberType.MOBILE:\r\n        ligne = f\" [{GREEN}>{WHITE}] Line type: Mobile\"\r\n\r\n    else:\r\n        ligne = \" [-] Line not found\"\r\n\r\n    possible = phonenumbers.is_possible_number(parsed)\r\n    valid = phonenumbers.is_valid_number(parsed)\r\n\r\n    with open(\"lib/country.json\", \"r\") as file:\r\n        read = json.load(file)\r\n\r\n    d = 0\r\n    countrys = []\r\n\r\n    for country, code in read.items():\r\n        d += 1 \r\n\r\n        if phone_number.startswith(code):\r\n            countrys.append(country)\r\n\r\n            if d == 153:\r\n                break\r\n            else:\r\n                continue\r\n        else:\r\n            continue\r\n\r\n    print(f\"{WHITE}\ud83d\udcde Phone number: {BLUE}{phone_number}{WHITE}\")\r\n\r\n    if possible == True:\r\n        pos = {\"possible\": \"\u2714\ufe0f\"}\r\n    else:\r\n        pos = {\"possible\": \"\u274c\"}\r\n\r\n    if valid == True:\r\n        val = {\"valid\": \"\u2714\ufe0f\"}\r\n    else:\r\n        val = {\"valid\": \"\u274c\"}\r\n\r\n    print(f\" [{GREEN}>{WHITE}] Possible: {pos['possible']}\")\r\n    print(f\" [{GREEN}>{WHITE}] Valid: {val['valid']}\")\r\n    print()\r\n\r\n    if operator != \"\":\r\n        print(f\" [{GREEN}>{WHITE}] Operator: {operator}\")\r\n    else:\r\n        print(f\" [-] Not Operator\")\r\n    try:\r\n        print(f\" [{GREEN}>{WHITE}] Possible location: \" + str(countrys).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\"))\r\n    except:\r\n        print(f\" [-] Not location\")\r\n\r\n    print(ligne)\r\n\r\n    await reputation(phone_number)\r\n\r",
    "prefix": "import phonenumbers\r\nimport json\r\nfrom phonenumbers import carrier\r\nfrom .reputation import *\r\nfrom .free_lookup import free\r\nfrom .spam import spamcalls\r\nfrom lib.text import *\r\n\r\n\r\nasync def lookup(phone_number):\r\n    print()\r\n    parsed = phonenumbers.parse(phone_number)\r\n\r\n    operator = carrier.name_for_number(parsed, \"fr\")\r\n    line = phonenumbers.number_type(parsed)\r\n\r\n    if line == phonenumbers.PhoneNumberType.FIXED_LINE:\r\n        ligne = f\" [{GREEN}>{WHITE}] Line type: Fixed\"\r\n\r\n    elif line == phonenumbers.PhoneNumberType.MOBILE:\r\n        ligne = f\" [{GREEN}>{WHITE}] Line type: Mobile\"\r\n\r\n    else:\r\n        ligne = \" [-] Line not found\"\r\n\r\n    possible = phonenumbers.is_possible_number(parsed)\r\n    valid = phonenumbers.is_valid_number(parsed)\r\n\r\n    with open(\"lib/country.json\", \"r\") as file:\r\n        read = json.load(file)\r\n\r\n    d = 0\r\n    countrys = []\r\n\r\n    for country, code in read.items():\r\n        d += 1 \r\n\r\n        if phone_number.startswith(code):\r\n            countrys.append(country)\r\n\r\n            if d == 153:\r\n                break\r\n            else:\r\n                continue\r\n        else:\r\n            continue\r\n\r\n    print(f\"{WHITE}\ud83d\udcde Phone number: {BLUE}{phone_number}{WHITE}\")\r\n\r\n    if possible == True:\r\n        pos = {\"possible\": \"\u2714\ufe0f\"}\r\n    else:\r\n        pos = {\"possible\": \"\u274c\"}\r\n\r\n    if valid == True:\r\n        val = {\"valid\": \"\u2714\ufe0f\"}\r\n    else:\r\n        val = {\"valid\": \"\u274c\"}\r\n\r\n    print(f\" [{GREEN}>{WHITE}] Possible: {pos['possible']}\")\r\n    print(f\" [{GREEN}>{WHITE}] Valid: {val['valid']}\")\r\n    print()\r\n\r\n    if operator != \"\":\r\n        print(f\" [{GREEN}>{WHITE}] Operator: {operator}\")\r\n    else:\r\n        print(f\" [-] Not Operator\")\r\n    try:\r\n        print(f\" [{GREEN}>{WHITE}] Possible location: \" + str(countrys).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\"))\r\n    except:\r\n        print(f\" [-] Not location\")\r\n\r\n    print(ligne)\r\n\r\n    await reputation(phone_number)\r\n\r",
    "suffix": ""
  },
  {
    "name": "dan-r/HomeAssistant-Ohme:custom_components/ohme/binary_sensor.py@823",
    "canonical_solution": "    coordinator = hass.data[DOMAIN][DATA_COORDINATORS][COORDINATOR_CHARGESESSIONS]",
    "prompt": "import logging\nfrom homeassistant.components.binary_sensor import (\n    BinarySensorDeviceClass,\n    BinarySensorEntity\n)\nfrom homeassistant.helpers.update_coordinator import CoordinatorEntity\nfrom homeassistant.core import HomeAssistant, callback\nfrom homeassistant.helpers.entity import generate_entity_id\nfrom homeassistant.util.dt import (utcnow)\nfrom .const import DOMAIN, DATA_COORDINATORS, COORDINATOR_CHARGESESSIONS, COORDINATOR_ADVANCED, DATA_CLIENT\nfrom .coordinator import OhmeChargeSessionsCoordinator, OhmeAdvancedSettingsCoordinator\nfrom .utils import charge_graph_in_slot\n\"\"\"Platform for sensor integration.\"\"\"\nfrom __future__ import annotations\n\n_LOGGER = logging.getLogger(__name__)\n\nasync def async_setup_entry(\n    hass: core.HomeAssistant,\n    config_entry: config_entries.ConfigEntry,\n    async_add_entities,\n):\n    \"\"\"Setup sensors and configure coordinator.\"\"\"\n    client = hass.data[DOMAIN][DATA_CLIENT]",
    "prefix": "import logging\nfrom homeassistant.components.binary_sensor import (\n    BinarySensorDeviceClass,\n    BinarySensorEntity\n)\nfrom homeassistant.helpers.update_coordinator import CoordinatorEntity\nfrom homeassistant.core import HomeAssistant, callback\nfrom homeassistant.helpers.entity import generate_entity_id\nfrom homeassistant.util.dt import (utcnow)\nfrom .const import DOMAIN, DATA_COORDINATORS, COORDINATOR_CHARGESESSIONS, COORDINATOR_ADVANCED, DATA_CLIENT\nfrom .coordinator import OhmeChargeSessionsCoordinator, OhmeAdvancedSettingsCoordinator\nfrom .utils import charge_graph_in_slot\n\"\"\"Platform for sensor integration.\"\"\"\nfrom __future__ import annotations\n\n_LOGGER = logging.getLogger(__name__)\n\nasync def async_setup_entry(\n    hass: core.HomeAssistant,\n    config_entry: config_entries.ConfigEntry,\n    async_add_entities,\n):\n    \"\"\"Setup sensors and configure coordinator.\"\"\"\n    client = hass.data[DOMAIN][DATA_CLIENT]",
    "suffix": ""
  },
  {
    "name": "Almas-Ali/SpyIP:spyip/backend.py@1207",
    "canonical_solution": ") -> Union[IPResponse, None]:",
    "prompt": "from typing import List, Union\nfrom .exceptions import (\n    TooManyRequests,\n    ConnectionTimeout,\n    StatusError,\n)\nfrom .models import (\n    IPResponse,\n    DNSResponse,\n)\nimport asyncio\nimport random\nimport string\nimport httpx\n\n\n\n\ndef get_random_string(length: int = 32) -> str:\n    \"\"\"Generate a random string of fixed length.\"\"\"\n    letters = string.ascii_lowercase + string.digits\n    return ''.join(random.sample(letters, length))\n\n\n# API endpoints for IP address lookup\ntrace_me_url = 'http://ip-api.com/json/'\ntrace_ip_url = 'http://ip-api.com/json/%(query)s'\ntrace_dns_url = f'http://{get_random_string(32)}.edns.ip-api.com/json/'\ntrace_ip_batch_url = 'http://ip-api.com/batch'\n\nheaders = {\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n    'Accept-Encoding': 'gzip, deflate',\n    'Accept-Language': 'en-US,en;q=0.5',\n    'Connection': 'keep-alive',\n    'Upgrade-Insecure-Requests': '1',\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n}\n\n\ndef trace_me(\n    timeout: int = 5,\n    lang: str = 'en',",
    "prefix": "from typing import List, Union\nfrom .exceptions import (\n    TooManyRequests,\n    ConnectionTimeout,\n    StatusError,\n)\nfrom .models import (\n    IPResponse,\n    DNSResponse,\n)\nimport asyncio\nimport random\nimport string\nimport httpx\n\n\n\n\ndef get_random_string(length: int = 32) -> str:\n    \"\"\"Generate a random string of fixed length.\"\"\"\n    letters = string.ascii_lowercase + string.digits\n    return ''.join(random.sample(letters, length))\n\n\n# API endpoints for IP address lookup\ntrace_me_url = 'http://ip-api.com/json/'\ntrace_ip_url = 'http://ip-api.com/json/%(query)s'\ntrace_dns_url = f'http://{get_random_string(32)}.edns.ip-api.com/json/'\ntrace_ip_batch_url = 'http://ip-api.com/batch'\n\nheaders = {\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n    'Accept-Encoding': 'gzip, deflate',\n    'Accept-Language': 'en-US,en;q=0.5',\n    'Connection': 'keep-alive',\n    'Upgrade-Insecure-Requests': '1',\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n}\n\n\ndef trace_me(\n    timeout: int = 5,\n    lang: str = 'en',",
    "suffix": ""
  },
  {
    "name": "leopedroso45/Stable-Diffusion-ImageGen:tests/test_process_task.py@991",
    "canonical_solution": "        process_task(fake_job, fake_pipeline, fake_executor, fake_path, parallel_exec=True)",
    "prompt": "import unittest\nimport sys\nfrom unittest.mock import patch, MagicMock\nfrom sevsd.process_task import check_cuda_and_clear_cache, process_task, check_os_path\nsys.path.append('../')\n\nclass TestProcessTask(unittest.TestCase):\n\n    @patch('sevsd.process_task.generate_image')\n    def test_process_task(self, mock_generate_image):\n        mock_image = MagicMock()\n        mock_image.save = MagicMock()\n        mock_generate_image.return_value = [mock_image]\n\n        fake_job = {\"prompt\": \"prompt\", \"details\": (None, 50, 1, 7.5)}\n        fake_pipeline = MagicMock()\n        fake_executor = {\"num_of_exec\": 1, \"cfg_scale\": 7}\n        fake_path = \"test_path\"\n",
    "prefix": "import unittest\nimport sys\nfrom unittest.mock import patch, MagicMock\nfrom sevsd.process_task import check_cuda_and_clear_cache, process_task, check_os_path\nsys.path.append('../')\n\nclass TestProcessTask(unittest.TestCase):\n\n    @patch('sevsd.process_task.generate_image')\n    def test_process_task(self, mock_generate_image):\n        mock_image = MagicMock()\n        mock_image.save = MagicMock()\n        mock_generate_image.return_value = [mock_image]\n\n        fake_job = {\"prompt\": \"prompt\", \"details\": (None, 50, 1, 7.5)}\n        fake_pipeline = MagicMock()\n        fake_executor = {\"num_of_exec\": 1, \"cfg_scale\": 7}\n        fake_path = \"test_path\"\n",
    "suffix": ""
  },
  {
    "name": "Emperor-WS/PyEmber:ember/autograd/numeric.py@742",
    "canonical_solution": "        hooks.append(Hook(t, lambda grad: grad.T))",
    "prompt": "import numpy as np\nimport ember\nfrom .hook import Hook\nfrom ._utils import numpy_unpad, inv_permutation\n\n\ndef _T(t):\n    \"\"\"\n    Transpose operation on the input tensor.\n\n    Args:\n    - t: Input tensor.\n\n    Returns:\n    - Tensor: Resultant tensor with the transpose operation applied.\n    \"\"\"\n    t = ember.to_tensor(t)  # Convert the input tensor to a Tensor\n    data = t.data.T  # Transpose operation\n    requires_grad = t.requires_grad  # Set requires_grad based on input tensor\n    hooks = []\n\n    # Register a hook for gradient computation if the input tensor requires it\n    if requires_grad:",
    "prefix": "import numpy as np\nimport ember\nfrom .hook import Hook\nfrom ._utils import numpy_unpad, inv_permutation\n\n\ndef _T(t):\n    \"\"\"\n    Transpose operation on the input tensor.\n\n    Args:\n    - t: Input tensor.\n\n    Returns:\n    - Tensor: Resultant tensor with the transpose operation applied.\n    \"\"\"\n    t = ember.to_tensor(t)  # Convert the input tensor to a Tensor\n    data = t.data.T  # Transpose operation\n    requires_grad = t.requires_grad  # Set requires_grad based on input tensor\n    hooks = []\n\n    # Register a hook for gradient computation if the input tensor requires it\n    if requires_grad:",
    "suffix": ""
  },
  {
    "name": "Hassi34/iot-device-identification:src/stage_03_preprocess_data.py@1022",
    "canonical_solution": "    labels_dict = read_yaml(parsed_args.params)[\"labels_mapping\"]",
    "prompt": "import argparse\nimport joblib\nimport pandas as pd\nfrom src.utils.common import read_yaml\nfrom src.utils.sys_logging import get_logger\nfrom sklearn.preprocessing import LabelEncoder\nfrom src.utils.common import write_dict_to_yaml\nfrom src.utils.data_ops import gzip_np_arr\nfrom sklearn.model_selection import train_test_split\nfrom src.utils.data_ops import get_fitted_pipeline\nfrom pathlib import Path\n\nSTAGE = \"Preprocess Data\"\n\n\ndef preprocess_data():\n    complete_df = pd.read_parquet(RAW_DATA_FILE_PATH)\n    logger.info(\n        f'The raw data file has been loaded from \"{RAW_DATA_FILE_PATH}\" with the shape \"{complete_df.shape}\"'\n    )\n    duplicate_rows = complete_df.duplicated().sum()\n    if duplicate_rows > 0:\n        logger.warning(\n            f\"Found {duplicate_rows} duplicate rows, removing duplicate rows...\"\n        )\n        complete_df = complete_df.drop_duplicates(keep=\"first\")\n    X = complete_df.drop([TARGET_COLUMN_NAME], axis=1)\n    y = complete_df[TARGET_COLUMN_NAME]\n    feature_cols = params[\"input_features_schema\"]\n    feature_cols = list(feature_cols.keys())\n    logger.info(f\"Read {len(feature_cols)} feature columns from params\")\n    data_processing_pipeline = get_fitted_pipeline(\n    X, feature_cols, KNN_IMPUTER_NEIGHBORS=KNN_IMPUTER_NEIGHBORS\n    )\n    Path(DATA_PREPROCESSING_PIPELINE_FILE_PATH).parent.absolute().mkdir(parents=True, exist_ok=True)\n    joblib.dump(data_processing_pipeline, DATA_PREPROCESSING_PIPELINE_FILE_PATH, compress=1)\n    logger.info(f\"Saved the preprocessing pipeline to {DATA_PREPROCESSING_PIPELINE_FILE_PATH}\")\n    data_processing_pipeline = joblib.load(DATA_PREPROCESSING_PIPELINE_FILE_PATH)\n    data_processing_pipeline\n    data_processing_pipeline = joblib.load(DATA_PREPROCESSING_PIPELINE_FILE_PATH)\n    logger.info(\n        f'Loaded sklearn data preprocessing pipeline from \"{DATA_PREPROCESSING_PIPELINE_FILE_PATH}\"'\n    )\n    X_transformed = data_processing_pipeline.transform(X)\n    logger.info(f'Dataframe shape after transformation is \"{X_transformed.shape}\"')\n\n    le = LabelEncoder()\n    le.fit(y)\n    labels_mapping_dict = {\"labels_mapping\": \"\"}\n    le_dict = dict(zip(le.transform(le.classes_), le.classes_))\n    le_dict = {int(k): v for k, v in le_dict.items()}\n\n    labels_mapping_dict[\"labels_mapping\"] = le_dict\n    logger.info(f\"Label encoding map has the dictionary: {le_dict}\")\n    write_dict_to_yaml(labels_mapping_dict, parsed_args.params)\n    logger.info(f'Updated the label encoding map in the file at \"{parsed_args.params}\"')",
    "prefix": "import argparse\nimport joblib\nimport pandas as pd\nfrom src.utils.common import read_yaml\nfrom src.utils.sys_logging import get_logger\nfrom sklearn.preprocessing import LabelEncoder\nfrom src.utils.common import write_dict_to_yaml\nfrom src.utils.data_ops import gzip_np_arr\nfrom sklearn.model_selection import train_test_split\nfrom src.utils.data_ops import get_fitted_pipeline\nfrom pathlib import Path\n\nSTAGE = \"Preprocess Data\"\n\n\ndef preprocess_data():\n    complete_df = pd.read_parquet(RAW_DATA_FILE_PATH)\n    logger.info(\n        f'The raw data file has been loaded from \"{RAW_DATA_FILE_PATH}\" with the shape \"{complete_df.shape}\"'\n    )\n    duplicate_rows = complete_df.duplicated().sum()\n    if duplicate_rows > 0:\n        logger.warning(\n            f\"Found {duplicate_rows} duplicate rows, removing duplicate rows...\"\n        )\n        complete_df = complete_df.drop_duplicates(keep=\"first\")\n    X = complete_df.drop([TARGET_COLUMN_NAME], axis=1)\n    y = complete_df[TARGET_COLUMN_NAME]\n    feature_cols = params[\"input_features_schema\"]\n    feature_cols = list(feature_cols.keys())\n    logger.info(f\"Read {len(feature_cols)} feature columns from params\")\n    data_processing_pipeline = get_fitted_pipeline(\n    X, feature_cols, KNN_IMPUTER_NEIGHBORS=KNN_IMPUTER_NEIGHBORS\n    )\n    Path(DATA_PREPROCESSING_PIPELINE_FILE_PATH).parent.absolute().mkdir(parents=True, exist_ok=True)\n    joblib.dump(data_processing_pipeline, DATA_PREPROCESSING_PIPELINE_FILE_PATH, compress=1)\n    logger.info(f\"Saved the preprocessing pipeline to {DATA_PREPROCESSING_PIPELINE_FILE_PATH}\")\n    data_processing_pipeline = joblib.load(DATA_PREPROCESSING_PIPELINE_FILE_PATH)\n    data_processing_pipeline\n    data_processing_pipeline = joblib.load(DATA_PREPROCESSING_PIPELINE_FILE_PATH)\n    logger.info(\n        f'Loaded sklearn data preprocessing pipeline from \"{DATA_PREPROCESSING_PIPELINE_FILE_PATH}\"'\n    )\n    X_transformed = data_processing_pipeline.transform(X)\n    logger.info(f'Dataframe shape after transformation is \"{X_transformed.shape}\"')\n\n    le = LabelEncoder()\n    le.fit(y)\n    labels_mapping_dict = {\"labels_mapping\": \"\"}\n    le_dict = dict(zip(le.transform(le.classes_), le.classes_))\n    le_dict = {int(k): v for k, v in le_dict.items()}\n\n    labels_mapping_dict[\"labels_mapping\"] = le_dict\n    logger.info(f\"Label encoding map has the dictionary: {le_dict}\")\n    write_dict_to_yaml(labels_mapping_dict, parsed_args.params)\n    logger.info(f'Updated the label encoding map in the file at \"{parsed_args.params}\"')",
    "suffix": ""
  },
  {
    "name": "see2023/Bert-VITS2-ext:for_deploy/infer_utils.py@1223",
    "canonical_solution": "            text = \"\".join(text2sep_kata(text)[0])",
    "prompt": "import sys\nimport torch\nfrom transformers import (\n    AutoModelForMaskedLM,\n    AutoTokenizer,\n    DebertaV2Model,\n    DebertaV2Tokenizer,\n    ClapModel,\n    ClapProcessor,\n)\nfrom config import config\nfrom text.japanese import text2sep_kata\n\n\n\n\nclass BertFeature:\n    def __init__(self, model_path, language=\"ZH\"):\n        self.model_path = model_path\n        self.language = language\n        self.tokenizer = None\n        self.model = None\n        self.device = None\n\n        self._prepare()\n\n    def _get_device(self, device=config.bert_gen_config.device):\n        if (\n            sys.platform == \"darwin\"\n            and torch.backends.mps.is_available()\n            and device == \"cpu\"\n        ):\n            device = \"mps\"\n        if not device:\n            device = \"cuda\"\n        return device\n\n    def _prepare(self):\n        self.device = self._get_device()\n\n        if self.language == \"EN\":\n            self.tokenizer = DebertaV2Tokenizer.from_pretrained(self.model_path)\n            self.model = DebertaV2Model.from_pretrained(self.model_path).to(self.device)\n        else:\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n            self.model = AutoModelForMaskedLM.from_pretrained(self.model_path).to(\n                self.device\n            )\n        self.model.eval()\n\n    def get_bert_feature(self, text, word2ph):\n        if self.language == \"JP\":",
    "prefix": "import sys\nimport torch\nfrom transformers import (\n    AutoModelForMaskedLM,\n    AutoTokenizer,\n    DebertaV2Model,\n    DebertaV2Tokenizer,\n    ClapModel,\n    ClapProcessor,\n)\nfrom config import config\nfrom text.japanese import text2sep_kata\n\n\n\n\nclass BertFeature:\n    def __init__(self, model_path, language=\"ZH\"):\n        self.model_path = model_path\n        self.language = language\n        self.tokenizer = None\n        self.model = None\n        self.device = None\n\n        self._prepare()\n\n    def _get_device(self, device=config.bert_gen_config.device):\n        if (\n            sys.platform == \"darwin\"\n            and torch.backends.mps.is_available()\n            and device == \"cpu\"\n        ):\n            device = \"mps\"\n        if not device:\n            device = \"cuda\"\n        return device\n\n    def _prepare(self):\n        self.device = self._get_device()\n\n        if self.language == \"EN\":\n            self.tokenizer = DebertaV2Tokenizer.from_pretrained(self.model_path)\n            self.model = DebertaV2Model.from_pretrained(self.model_path).to(self.device)\n        else:\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n            self.model = AutoModelForMaskedLM.from_pretrained(self.model_path).to(\n                self.device\n            )\n        self.model.eval()\n\n    def get_bert_feature(self, text, word2ph):\n        if self.language == \"JP\":",
    "suffix": ""
  },
  {
    "name": "chinhsuanwu/ifusion-threestudio:threestudio/models/materials/no_material.py@1291",
    "canonical_solution": "            color = get_activation(self.cfg.color_activation)(features)",
    "prompt": "import random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport threestudio\nfrom dataclasses import dataclass, field\nfrom threestudio.models.materials.base import BaseMaterial\nfrom threestudio.models.networks import get_encoding, get_mlp\nfrom threestudio.utils.ops import dot, get_activation\nfrom threestudio.utils.typing import *\n\n\n\n\n@threestudio.register(\"no-material\")\nclass NoMaterial(BaseMaterial):\n    @dataclass\n    class Config(BaseMaterial.Config):\n        n_output_dims: int = 3\n        color_activation: str = \"sigmoid\"\n        input_feature_dims: Optional[int] = None\n        mlp_network_config: Optional[dict] = None\n        requires_normal: bool = False\n\n    cfg: Config\n\n    def configure(self) -> None:\n        self.use_network = False\n        if (\n            self.cfg.input_feature_dims is not None\n            and self.cfg.mlp_network_config is not None\n        ):\n            self.network = get_mlp(\n                self.cfg.input_feature_dims,\n                self.cfg.n_output_dims,\n                self.cfg.mlp_network_config,\n            )\n            self.use_network = True\n        self.requires_normal = self.cfg.requires_normal\n\n    def forward(\n        self, features: Float[Tensor, \"B ... Nf\"], **kwargs\n    ) -> Float[Tensor, \"B ... Nc\"]:\n        if not self.use_network:\n            assert (\n                features.shape[-1] == self.cfg.n_output_dims\n            ), f\"Expected {self.cfg.n_output_dims} output dims, only got {features.shape[-1]} dims input.\"",
    "prefix": "import random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport threestudio\nfrom dataclasses import dataclass, field\nfrom threestudio.models.materials.base import BaseMaterial\nfrom threestudio.models.networks import get_encoding, get_mlp\nfrom threestudio.utils.ops import dot, get_activation\nfrom threestudio.utils.typing import *\n\n\n\n\n@threestudio.register(\"no-material\")\nclass NoMaterial(BaseMaterial):\n    @dataclass\n    class Config(BaseMaterial.Config):\n        n_output_dims: int = 3\n        color_activation: str = \"sigmoid\"\n        input_feature_dims: Optional[int] = None\n        mlp_network_config: Optional[dict] = None\n        requires_normal: bool = False\n\n    cfg: Config\n\n    def configure(self) -> None:\n        self.use_network = False\n        if (\n            self.cfg.input_feature_dims is not None\n            and self.cfg.mlp_network_config is not None\n        ):\n            self.network = get_mlp(\n                self.cfg.input_feature_dims,\n                self.cfg.n_output_dims,\n                self.cfg.mlp_network_config,\n            )\n            self.use_network = True\n        self.requires_normal = self.cfg.requires_normal\n\n    def forward(\n        self, features: Float[Tensor, \"B ... Nf\"], **kwargs\n    ) -> Float[Tensor, \"B ... Nc\"]:\n        if not self.use_network:\n            assert (\n                features.shape[-1] == self.cfg.n_output_dims\n            ), f\"Expected {self.cfg.n_output_dims} output dims, only got {features.shape[-1]} dims input.\"",
    "suffix": ""
  },
  {
    "name": "jasursadikov/mud:commands.py@880",
    "canonical_solution": "                    origin_sync += f'{TEXT[\"bright_green\"]}{glyph(\"ahead\")} {ahead}{RESET}'",
    "prompt": "import utils\nimport asyncio\nimport subprocess\nfrom utils import TEXT, BACK, RESET, STYLES, END_STYLES, glyph\nfrom typing import List, Dict\nfrom collections import Counter\nfrom prettytable import PrettyTable, PLAIN_COLUMNS\n\n\n\nclass Commands:\n    def __init__(self, repos):\n        self.repos = repos\n        self.label_color_cache = {}\n        self.current_color_index = 0\n\n    # `mud status` command implementation\n    def status(self, repos: Dict[str, List[str]]) -> None:\n        table = self._get_table()\n        for path, tags in repos.items():\n            formatted_path = self._get_formatted_path(path)\n            branch = self._get_branch_status(path)\n            author = self._get_authors_name(path)\n            commit = self._get_commit_message(path, 30)\n            colored_labels = self._get_formatted_labels(tags)\n\n            # Sync with origin status\n            ahead_behind_cmd = subprocess.run(['git', 'rev-list', '--left-right', '--count', 'HEAD...@{upstream}'],\n                                              text=True, cwd=path, capture_output=True)\n            stdout = ahead_behind_cmd.stdout.strip().split()\n            if len(stdout) >= 2:\n                ahead, behind = stdout[0], stdout[1]\n                origin_sync = ''\n                if ahead and ahead != '0':",
    "prefix": "import utils\nimport asyncio\nimport subprocess\nfrom utils import TEXT, BACK, RESET, STYLES, END_STYLES, glyph\nfrom typing import List, Dict\nfrom collections import Counter\nfrom prettytable import PrettyTable, PLAIN_COLUMNS\n\n\n\nclass Commands:\n    def __init__(self, repos):\n        self.repos = repos\n        self.label_color_cache = {}\n        self.current_color_index = 0\n\n    # `mud status` command implementation\n    def status(self, repos: Dict[str, List[str]]) -> None:\n        table = self._get_table()\n        for path, tags in repos.items():\n            formatted_path = self._get_formatted_path(path)\n            branch = self._get_branch_status(path)\n            author = self._get_authors_name(path)\n            commit = self._get_commit_message(path, 30)\n            colored_labels = self._get_formatted_labels(tags)\n\n            # Sync with origin status\n            ahead_behind_cmd = subprocess.run(['git', 'rev-list', '--left-right', '--count', 'HEAD...@{upstream}'],\n                                              text=True, cwd=path, capture_output=True)\n            stdout = ahead_behind_cmd.stdout.strip().split()\n            if len(stdout) >= 2:\n                ahead, behind = stdout[0], stdout[1]\n                origin_sync = ''\n                if ahead and ahead != '0':",
    "suffix": ""
  },
  {
    "name": "Q-MM/PureMM:model/PureMM_arch.py@837",
    "canonical_solution": "            self.mm_projector = build_vision_projector(config)",
    "prompt": "from abc import ABC, abstractmethod\nfrom .multimodal_encoder.builder import build_vision_tower\nfrom .multimodal_projector.builder import build_vision_projector\nimport torch\nimport torch.nn as nn\n#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\n\n\n\nIGNORE_INDEX = -100\nIMAGE_TOKEN_INDEX = -200\nDEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\n\n\ndef rank0_print(rank, *args):\n    if rank == 0:\n        print(*args)\n\n\nclass PureMMMetaModel:\n\n    def __init__(self, config):\n        super(PureMMMetaModel, self).__init__(config)\n\n        if hasattr(config, \"mm_vision_tower\"):\n            self.vision_tower = build_vision_tower(config, delay_load=True)\n            # self.mm_projector = nn.Linear(config.mm_hidden_size, config.hidden_size)",
    "prefix": "from abc import ABC, abstractmethod\nfrom .multimodal_encoder.builder import build_vision_tower\nfrom .multimodal_projector.builder import build_vision_projector\nimport torch\nimport torch.nn as nn\n#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\n\n\n\nIGNORE_INDEX = -100\nIMAGE_TOKEN_INDEX = -200\nDEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\n\n\ndef rank0_print(rank, *args):\n    if rank == 0:\n        print(*args)\n\n\nclass PureMMMetaModel:\n\n    def __init__(self, config):\n        super(PureMMMetaModel, self).__init__(config)\n\n        if hasattr(config, \"mm_vision_tower\"):\n            self.vision_tower = build_vision_tower(config, delay_load=True)\n            # self.mm_projector = nn.Linear(config.mm_hidden_size, config.hidden_size)",
    "suffix": ""
  },
  {
    "name": "Ananya2001-an/spotify-py-sdk:tests/endpoints/test_recommendations.py@1007",
    "canonical_solution": "    return SpotifyApi(os.getenv(\"CLIENT_ID\"), os.getenv(\"CLIENT_SECRET\"))",
    "prompt": "import json\nimport pytest\nimport os\nfrom spotify_py_sdk import SpotifyApi\nfrom spotify_py_sdk.endpoints.recommendations import RecommendationsRequestRequiredArguments\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\n@pytest.fixture\ndef api():",
    "prefix": "import json\nimport pytest\nimport os\nfrom spotify_py_sdk import SpotifyApi\nfrom spotify_py_sdk.endpoints.recommendations import RecommendationsRequestRequiredArguments\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\n@pytest.fixture\ndef api():",
    "suffix": ""
  },
  {
    "name": "kyleliang919/Optimizer-Zoo:optimizer_zoo/Trainer/utils.py@1215",
    "canonical_solution": "        return AsyncSeq2SeqTrainer if training_args.async_grad else Seq2SeqTrainer",
    "prompt": "from transformers import Trainer, Seq2SeqTrainer\nfrom trl import SFTTrainer, DPOTrainer\nfrom .async_trainer import AsyncTrainer, AsyncSFTTrainer, AsyncDPOTrainer, AsyncSeq2SeqTrainer\ndef create_trainer(training_args):\n    if training_args.task == \"pretraining\":\n        return AsyncTrainer if training_args.async_grad else Trainer\n    elif training_args.task == \"sft\":\n        return AsyncSFTTrainer if training_args.async_grad else SFTTrainer\n    elif training_args.task == \"dpo\":\n        return AsyncDPOTrainer if training_args.async_grad else DPOTrainer\n    elif training_args.task == \"seq2seq\":",
    "prefix": "from transformers import Trainer, Seq2SeqTrainer\nfrom trl import SFTTrainer, DPOTrainer\nfrom .async_trainer import AsyncTrainer, AsyncSFTTrainer, AsyncDPOTrainer, AsyncSeq2SeqTrainer\ndef create_trainer(training_args):\n    if training_args.task == \"pretraining\":\n        return AsyncTrainer if training_args.async_grad else Trainer\n    elif training_args.task == \"sft\":\n        return AsyncSFTTrainer if training_args.async_grad else SFTTrainer\n    elif training_args.task == \"dpo\":\n        return AsyncDPOTrainer if training_args.async_grad else DPOTrainer\n    elif training_args.task == \"seq2seq\":",
    "suffix": ""
  },
  {
    "name": "giaminhgist/3D-DAM:lib/model/DuoAttention.py@804",
    "canonical_solution": "            residual_block(channel_size=16),",
    "prompt": "import numpy as np\nimport torch\nfrom torch import nn\nfrom lib.model.attention_block import SpatialAttention3D, ChannelAttention3D, residual_block\n\n\nclass DAM(nn.Module):\n    def __init__(self, channels=64):\n        super(DAM, self).__init__()\n\n        self.sa = SpatialAttention3D(out_channel=channels)\n        self.ca = ChannelAttention3D(in_planes=channels)\n\n    def forward(self, x):\n        residual = x\n        out = self.ca(x)\n        out = self.sa(out)\n        out = out + residual\n        return out\n\n\nclass Duo_Attention(nn.Module):\n    def __init__(\n            self, input_size=(1, 169, 208, 179), num_classes=3, dropout=0\n    ):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(input_size[0], 8, 3, padding=1),\n            nn.BatchNorm3d(8),\n            nn.ReLU(),\n            # nn.MaxPool3d(2, 2),\n\n            nn.Conv3d(8, 16, 3, padding=1, stride=2),\n            nn.BatchNorm3d(16),\n            nn.ReLU(),",
    "prefix": "import numpy as np\nimport torch\nfrom torch import nn\nfrom lib.model.attention_block import SpatialAttention3D, ChannelAttention3D, residual_block\n\n\nclass DAM(nn.Module):\n    def __init__(self, channels=64):\n        super(DAM, self).__init__()\n\n        self.sa = SpatialAttention3D(out_channel=channels)\n        self.ca = ChannelAttention3D(in_planes=channels)\n\n    def forward(self, x):\n        residual = x\n        out = self.ca(x)\n        out = self.sa(out)\n        out = out + residual\n        return out\n\n\nclass Duo_Attention(nn.Module):\n    def __init__(\n            self, input_size=(1, 169, 208, 179), num_classes=3, dropout=0\n    ):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(input_size[0], 8, 3, padding=1),\n            nn.BatchNorm3d(8),\n            nn.ReLU(),\n            # nn.MaxPool3d(2, 2),\n\n            nn.Conv3d(8, 16, 3, padding=1, stride=2),\n            nn.BatchNorm3d(16),\n            nn.ReLU(),",
    "suffix": ""
  },
  {
    "name": "itsluminous/EasyEncryption:script.py@783",
    "canonical_solution": "        decrypted_message = decrypt_message(encrypted_input.encode(), key)",
    "prompt": "from core import generate_key, encrypt_message, decrypt_message, encrypt_file, decrypt_file\n\"\"\"\nScript providing a user interface for encryption and decryption operations.\n\"\"\"\n\n\ndef generate_new_key():\n    \"\"\"\n    Generate a new encryption key.\n    \n    Returns:\n    - bytes: New encryption key.\n    \"\"\"\n    key = generate_key()\n    print(f\"\\nGenerated Key: {key.decode()}\")\n    return key\n\ndef enter_user_key():\n    \"\"\"\n    Prompt user to enter a key.\n    \n    Returns:\n    - bytes: User-entered key.\n    \"\"\"\n    print(\"\\nEnter the key:\")\n    return input().encode()\n\ndef encrypt_user_message(key):\n    \"\"\"\n    Encrypt a user-entered message.\n    \n    Parameters:\n    - key (bytes): Encryption key.\n    \"\"\"\n    if key is None:\n        print(\"\\nPlease generate or enter a key first.\")\n    else:\n        print(\"\\nEnter a message to encrypt (press Enter twice to finish):\")\n        lines = []\n        while True:\n            line = input()\n            if not line:\n                break\n            lines.append(line)\n        user_input = '\\n'.join(lines)\n        encrypted_message = encrypt_message(user_input, key)\n        print(f\"\\nEncrypted message: {encrypted_message}\")\n\ndef decrypt_user_message(key):\n    \"\"\"\n    Decrypt a user-entered message.\n    \n    Parameters:\n    - key (bytes): Decryption key.\n    \"\"\"\n    if key is None:\n        print(\"\\nPlease generate or enter a key first.\")\n    else:\n        print(\"\\nEnter the encrypted message (press Enter twice to finish):\")\n        lines = []\n        while True:\n            line = input()\n            if not line:\n                break\n            lines.append(line)\n        encrypted_input = '\\n'.join(lines)",
    "prefix": "from core import generate_key, encrypt_message, decrypt_message, encrypt_file, decrypt_file\n\"\"\"\nScript providing a user interface for encryption and decryption operations.\n\"\"\"\n\n\ndef generate_new_key():\n    \"\"\"\n    Generate a new encryption key.\n    \n    Returns:\n    - bytes: New encryption key.\n    \"\"\"\n    key = generate_key()\n    print(f\"\\nGenerated Key: {key.decode()}\")\n    return key\n\ndef enter_user_key():\n    \"\"\"\n    Prompt user to enter a key.\n    \n    Returns:\n    - bytes: User-entered key.\n    \"\"\"\n    print(\"\\nEnter the key:\")\n    return input().encode()\n\ndef encrypt_user_message(key):\n    \"\"\"\n    Encrypt a user-entered message.\n    \n    Parameters:\n    - key (bytes): Encryption key.\n    \"\"\"\n    if key is None:\n        print(\"\\nPlease generate or enter a key first.\")\n    else:\n        print(\"\\nEnter a message to encrypt (press Enter twice to finish):\")\n        lines = []\n        while True:\n            line = input()\n            if not line:\n                break\n            lines.append(line)\n        user_input = '\\n'.join(lines)\n        encrypted_message = encrypt_message(user_input, key)\n        print(f\"\\nEncrypted message: {encrypted_message}\")\n\ndef decrypt_user_message(key):\n    \"\"\"\n    Decrypt a user-entered message.\n    \n    Parameters:\n    - key (bytes): Decryption key.\n    \"\"\"\n    if key is None:\n        print(\"\\nPlease generate or enter a key first.\")\n    else:\n        print(\"\\nEnter the encrypted message (press Enter twice to finish):\")\n        lines = []\n        while True:\n            line = input()\n            if not line:\n                break\n            lines.append(line)\n        encrypted_input = '\\n'.join(lines)",
    "suffix": ""
  },
  {
    "name": "gardenifi/server:tests/api/resource_not_found_test.py@712",
    "canonical_solution": "        response = await resource_not_found(obj, exc)",
    "prompt": "import json\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom fastapi import HTTPException, Request\nfrom fastapi.responses import JSONResponse\nfrom app.main_app import app\nfrom app.main_app import resource_not_found\n\"\"\"MIT License\n\nCopyright (c) 2023, Marios Karagiannopoulos\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\n**Attribution Requirement:**\nWhen using or distributing the software, an attribution to Marios Karagiannopoulos must be included.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\"\"\"\n\n\nclient = TestClient(app)\nscope = {\"type\": \"http\", \"http_version\": \"1.1\", \"method\": \"GET\", \"path\": \"/\"}\n\n\n@pytest.fixture(scope=\"function\")\nasync def request_obj():\n    \"\"\"Request object creation fixture\"\"\"\n    return Request(scope)\n\n\nclass TestResourceNotFound:\n    \"\"\"\n    Test class for the 'resource_not_found' error handler function.\n    \"\"\"\n\n    @pytest.mark.asyncio\n    async def test_returns_json_response_with_status_code_404_and_detail_of_httpexception(self, obj=request_obj):\n        \"\"\"\n        Test for returning a JSONResponse object with status code 404 and the detail of the HTTPException passed as an argument.\n        \"\"\"\n        exc = HTTPException(status_code=404, detail=\"Not found\")",
    "prefix": "import json\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom fastapi import HTTPException, Request\nfrom fastapi.responses import JSONResponse\nfrom app.main_app import app\nfrom app.main_app import resource_not_found\n\"\"\"MIT License\n\nCopyright (c) 2023, Marios Karagiannopoulos\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\n**Attribution Requirement:**\nWhen using or distributing the software, an attribution to Marios Karagiannopoulos must be included.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\"\"\"\n\n\nclient = TestClient(app)\nscope = {\"type\": \"http\", \"http_version\": \"1.1\", \"method\": \"GET\", \"path\": \"/\"}\n\n\n@pytest.fixture(scope=\"function\")\nasync def request_obj():\n    \"\"\"Request object creation fixture\"\"\"\n    return Request(scope)\n\n\nclass TestResourceNotFound:\n    \"\"\"\n    Test class for the 'resource_not_found' error handler function.\n    \"\"\"\n\n    @pytest.mark.asyncio\n    async def test_returns_json_response_with_status_code_404_and_detail_of_httpexception(self, obj=request_obj):\n        \"\"\"\n        Test for returning a JSONResponse object with status code 404 and the detail of the HTTPException passed as an argument.\n        \"\"\"\n        exc = HTTPException(status_code=404, detail=\"Not found\")",
    "suffix": ""
  },
  {
    "name": "xiaoye0x0/pfgo_tg_bot:utils/task/set_args.py@838",
    "canonical_solution": "    Logmanager(args.log)",
    "prompt": "import os\nimport argparse\nfrom .model import Task\nfrom ..log import Logmanager\n\n\n\ndef is_file_exists(file_path) -> bool:\n    r = os.path.exists(file_path)\n    if not r:\n        LOGGER.error(f\"\u6587\u4ef6{file_path}\u4e0d\u5b58\u5728\")\n    return r\n\n\ndef create_folder_if_not_exists(folder_path):\n    if not folder_path:\n        return\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path)\n\n\ndef parse_command_line_args():\n    \"\"\"\n    -c --config: \u914d\u7f6e\u6587\u4ef6\n    --log: \u65e5\u5fd7\u5b58\u653e\u4f4d\u7f6e\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"\u8fd0\u884c\u53c2\u6570\")\n\n    parser.add_argument(\"--config\", \"-c\", type=str, default=\"./config.ini\", help=\"\u914d\u7f6e\u6587\u4ef6\")\n    parser.add_argument(\"--log\", type=str, default=\"./\", help=\"\u65e5\u5fd7\u5b58\u653e\u6587\u4ef6\u5939\u7684\u4f4d\u7f6e,\u9ed8\u8ba4\u653e\u5230\u5f53\u524d\u8def\u5f84\")\n    args = parser.parse_args()\n\n    # \u521d\u59cb\u5316\u65e5\u5fd7\u6a21\u5757\n    global LOGGER\n    create_folder_if_not_exists(args.log)",
    "prefix": "import os\nimport argparse\nfrom .model import Task\nfrom ..log import Logmanager\n\n\n\ndef is_file_exists(file_path) -> bool:\n    r = os.path.exists(file_path)\n    if not r:\n        LOGGER.error(f\"\u6587\u4ef6{file_path}\u4e0d\u5b58\u5728\")\n    return r\n\n\ndef create_folder_if_not_exists(folder_path):\n    if not folder_path:\n        return\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path)\n\n\ndef parse_command_line_args():\n    \"\"\"\n    -c --config: \u914d\u7f6e\u6587\u4ef6\n    --log: \u65e5\u5fd7\u5b58\u653e\u4f4d\u7f6e\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"\u8fd0\u884c\u53c2\u6570\")\n\n    parser.add_argument(\"--config\", \"-c\", type=str, default=\"./config.ini\", help=\"\u914d\u7f6e\u6587\u4ef6\")\n    parser.add_argument(\"--log\", type=str, default=\"./\", help=\"\u65e5\u5fd7\u5b58\u653e\u6587\u4ef6\u5939\u7684\u4f4d\u7f6e,\u9ed8\u8ba4\u653e\u5230\u5f53\u524d\u8def\u5f84\")\n    args = parser.parse_args()\n\n    # \u521d\u59cb\u5316\u65e5\u5fd7\u6a21\u5757\n    global LOGGER\n    create_folder_if_not_exists(args.log)",
    "suffix": ""
  },
  {
    "name": "shibing624/chatgpt-webui:src/index_func.py@1337",
    "canonical_solution": "    text_splitter = ChineseRecursiveTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)",
    "prompt": "import os\nimport re\n                import PyPDF2\nfrom typing import List, Optional, Any\nfrom langchain.schema import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom loguru import logger\nfrom tqdm import tqdm\nfrom src.config import local_embedding, retrieve_proxy, chunk_overlap, chunk_size, hf_emb_model_name\nfrom src.presets import OPENAI_API_BASE\nfrom src.utils import excel_to_string, get_files_hash, load_pkl, save_pkl\n                    from src.pdf_func import parse_pdf\n                    from src.config import advance_docs\n                from langchain.document_loaders import UnstructuredWordDocumentLoader\n                from langchain.document_loaders import UnstructuredPowerPointLoader\n                from langchain.document_loaders import UnstructuredEPubLoader\n                from langchain.document_loaders import TextLoader\n    from langchain.vectorstores import FAISS\n    from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n        from langchain.embeddings import OpenAIEmbeddings\n\n\n\npwd_path = os.path.abspath(os.path.dirname(__file__))\n\n\nclass ChineseRecursiveTextSplitter(RecursiveCharacterTextSplitter):\n    \"\"\"Recursive text splitter for Chinese text.\n    copy from: https://github.com/chatchat-space/Langchain-Chatchat/tree/master\n    \"\"\"\n\n    def __init__(\n            self,\n            separators: Optional[List[str]] = None,\n            keep_separator: bool = True,\n            is_separator_regex: bool = True,\n            **kwargs: Any,\n    ) -> None:\n        \"\"\"Create a new TextSplitter.\"\"\"\n        super().__init__(keep_separator=keep_separator, **kwargs)\n        self._separators = separators or [\n            \"\\n\\n\",\n            \"\\n\",\n            \"\u3002|\uff01|\uff1f\",\n            \"\\.\\s|\\!\\s|\\?\\s\",\n            \"\uff1b|;\\s\",\n            \"\uff0c|,\\s\"\n        ]\n        self._is_separator_regex = is_separator_regex\n\n    @staticmethod\n    def _split_text_with_regex_from_end(\n            text: str, separator: str, keep_separator: bool\n    ) -> List[str]:\n        # Now that we have the separator, split the text\n        if separator:\n            if keep_separator:\n                # The parentheses in the pattern keep the delimiters in the result.\n                _splits = re.split(f\"({separator})\", text)\n                splits = [\"\".join(i) for i in zip(_splits[0::2], _splits[1::2])]\n                if len(_splits) % 2 == 1:\n                    splits += _splits[-1:]\n            else:\n                splits = re.split(separator, text)\n        else:\n            splits = list(text)\n        return [s for s in splits if s != \"\"]\n\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\n        \"\"\"Split incoming text and return chunks.\"\"\"\n        final_chunks = []\n        # Get appropriate separator to use\n        separator = separators[-1]\n        new_separators = []\n        for i, _s in enumerate(separators):\n            _separator = _s if self._is_separator_regex else re.escape(_s)\n            if _s == \"\":\n                separator = _s\n                break\n            if re.search(_separator, text):\n                separator = _s\n                new_separators = separators[i + 1:]\n                break\n\n        _separator = separator if self._is_separator_regex else re.escape(separator)\n        splits = self._split_text_with_regex_from_end(text, _separator, self._keep_separator)\n\n        # Now go merging things, recursively splitting longer texts.\n        _good_splits = []\n        _separator = \"\" if self._keep_separator else separator\n        for s in splits:\n            if self._length_function(s) < self._chunk_size:\n                _good_splits.append(s)\n            else:\n                if _good_splits:\n                    merged_text = self._merge_splits(_good_splits, _separator)\n                    final_chunks.extend(merged_text)\n                    _good_splits = []\n                if not new_separators:\n                    final_chunks.append(s)\n                else:\n                    other_info = self._split_text(s, new_separators)\n                    final_chunks.extend(other_info)\n        if _good_splits:\n            merged_text = self._merge_splits(_good_splits, _separator)\n            final_chunks.extend(merged_text)\n        return [re.sub(r\"\\n{2,}\", \"\\n\", chunk.strip()) for chunk in final_chunks if chunk.strip() != \"\"]\n\n\ndef get_documents(file_paths):",
    "prefix": "import os\nimport re\n                import PyPDF2\nfrom typing import List, Optional, Any\nfrom langchain.schema import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom loguru import logger\nfrom tqdm import tqdm\nfrom src.config import local_embedding, retrieve_proxy, chunk_overlap, chunk_size, hf_emb_model_name\nfrom src.presets import OPENAI_API_BASE\nfrom src.utils import excel_to_string, get_files_hash, load_pkl, save_pkl\n                    from src.pdf_func import parse_pdf\n                    from src.config import advance_docs\n                from langchain.document_loaders import UnstructuredWordDocumentLoader\n                from langchain.document_loaders import UnstructuredPowerPointLoader\n                from langchain.document_loaders import UnstructuredEPubLoader\n                from langchain.document_loaders import TextLoader\n    from langchain.vectorstores import FAISS\n    from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n        from langchain.embeddings import OpenAIEmbeddings\n\n\n\npwd_path = os.path.abspath(os.path.dirname(__file__))\n\n\nclass ChineseRecursiveTextSplitter(RecursiveCharacterTextSplitter):\n    \"\"\"Recursive text splitter for Chinese text.\n    copy from: https://github.com/chatchat-space/Langchain-Chatchat/tree/master\n    \"\"\"\n\n    def __init__(\n            self,\n            separators: Optional[List[str]] = None,\n            keep_separator: bool = True,\n            is_separator_regex: bool = True,\n            **kwargs: Any,\n    ) -> None:\n        \"\"\"Create a new TextSplitter.\"\"\"\n        super().__init__(keep_separator=keep_separator, **kwargs)\n        self._separators = separators or [\n            \"\\n\\n\",\n            \"\\n\",\n            \"\u3002|\uff01|\uff1f\",\n            \"\\.\\s|\\!\\s|\\?\\s\",\n            \"\uff1b|;\\s\",\n            \"\uff0c|,\\s\"\n        ]\n        self._is_separator_regex = is_separator_regex\n\n    @staticmethod\n    def _split_text_with_regex_from_end(\n            text: str, separator: str, keep_separator: bool\n    ) -> List[str]:\n        # Now that we have the separator, split the text\n        if separator:\n            if keep_separator:\n                # The parentheses in the pattern keep the delimiters in the result.\n                _splits = re.split(f\"({separator})\", text)\n                splits = [\"\".join(i) for i in zip(_splits[0::2], _splits[1::2])]\n                if len(_splits) % 2 == 1:\n                    splits += _splits[-1:]\n            else:\n                splits = re.split(separator, text)\n        else:\n            splits = list(text)\n        return [s for s in splits if s != \"\"]\n\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\n        \"\"\"Split incoming text and return chunks.\"\"\"\n        final_chunks = []\n        # Get appropriate separator to use\n        separator = separators[-1]\n        new_separators = []\n        for i, _s in enumerate(separators):\n            _separator = _s if self._is_separator_regex else re.escape(_s)\n            if _s == \"\":\n                separator = _s\n                break\n            if re.search(_separator, text):\n                separator = _s\n                new_separators = separators[i + 1:]\n                break\n\n        _separator = separator if self._is_separator_regex else re.escape(separator)\n        splits = self._split_text_with_regex_from_end(text, _separator, self._keep_separator)\n\n        # Now go merging things, recursively splitting longer texts.\n        _good_splits = []\n        _separator = \"\" if self._keep_separator else separator\n        for s in splits:\n            if self._length_function(s) < self._chunk_size:\n                _good_splits.append(s)\n            else:\n                if _good_splits:\n                    merged_text = self._merge_splits(_good_splits, _separator)\n                    final_chunks.extend(merged_text)\n                    _good_splits = []\n                if not new_separators:\n                    final_chunks.append(s)\n                else:\n                    other_info = self._split_text(s, new_separators)\n                    final_chunks.extend(other_info)\n        if _good_splits:\n            merged_text = self._merge_splits(_good_splits, _separator)\n            final_chunks.extend(merged_text)\n        return [re.sub(r\"\\n{2,}\", \"\\n\", chunk.strip()) for chunk in final_chunks if chunk.strip() != \"\"]\n\n\ndef get_documents(file_paths):",
    "suffix": ""
  },
  {
    "name": "ConnectAI-E/GitMaya:server/tasks/lark/pull_request.py@930",
    "canonical_solution": "        bot, _ = get_bot_by_application_id(app_id)",
    "prompt": "import json\nimport logging\nfrom celery_app import app, celery\nfrom connectai.lark.sdk import FeishuTextMessage\nfrom model.schema import (\n    ChatGroup,\n    CodeApplication,\n    CodeUser,\n    IMUser,\n    PullRequest,\n    Repo,\n    Team,\n    TeamMember,\n    db,\n)\nfrom model.team import get_assignees_by_openid\nfrom utils.github.repo import GitHubAppRepo\nfrom utils.lark.pr_card import PullCard\nfrom utils.lark.pr_manual import (\n    PrManual,\n    PullRequestDiff,\n    PullRequestLog,\n    PullRequestView,\n)\nfrom utils.lark.pr_tip_failed import PrTipFailed\nfrom utils.lark.pr_tip_success import PrTipSuccess\nfrom .base import (\n    get_bot_by_application_id,\n    get_git_object_by_message_id,\n    with_authenticated_github,\n)\n\n\n\n\n@celery.task()\ndef send_pull_request_failed_tip(\n    content, app_id, message_id, *args, bot=None, **kwargs\n):\n    \"\"\"send new card message to user.\n\n    Args:\n        app_id: IMApplication.app_id.\n        message_id: lark message id.\n        content: error message\n    \"\"\"\n    if not bot:",
    "prefix": "import json\nimport logging\nfrom celery_app import app, celery\nfrom connectai.lark.sdk import FeishuTextMessage\nfrom model.schema import (\n    ChatGroup,\n    CodeApplication,\n    CodeUser,\n    IMUser,\n    PullRequest,\n    Repo,\n    Team,\n    TeamMember,\n    db,\n)\nfrom model.team import get_assignees_by_openid\nfrom utils.github.repo import GitHubAppRepo\nfrom utils.lark.pr_card import PullCard\nfrom utils.lark.pr_manual import (\n    PrManual,\n    PullRequestDiff,\n    PullRequestLog,\n    PullRequestView,\n)\nfrom utils.lark.pr_tip_failed import PrTipFailed\nfrom utils.lark.pr_tip_success import PrTipSuccess\nfrom .base import (\n    get_bot_by_application_id,\n    get_git_object_by_message_id,\n    with_authenticated_github,\n)\n\n\n\n\n@celery.task()\ndef send_pull_request_failed_tip(\n    content, app_id, message_id, *args, bot=None, **kwargs\n):\n    \"\"\"send new card message to user.\n\n    Args:\n        app_id: IMApplication.app_id.\n        message_id: lark message id.\n        content: error message\n    \"\"\"\n    if not bot:",
    "suffix": ""
  },
  {
    "name": "camenduru/AnyDoor-online-hf:dinov2/dinov2/layers/block.py@1475",
    "canonical_solution": "        ffn_layer: Callable[..., nn.Module] = Mlp,",
    "prompt": "import logging\nimport torch\nfrom typing import Callable, List, Any, Tuple, Dict\nfrom torch import nn, Tensor\nfrom .attention import Attention, MemEffAttention\nfrom .drop_path import DropPath\nfrom .layer_scale import LayerScale\nfrom .mlp import Mlp\n    from xformers.ops import fmha\n    from xformers.ops import scaled_index_add, index_select_cat\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# References:\n#   https://github.com/facebookresearch/dino/blob/master/vision_transformer.py\n#   https://github.com/rwightman/pytorch-image-models/tree/master/timm/layers/patch_embed.py\n\n\n\n\n\nlogger = logging.getLogger(\"dinov2\")\n\n\ntry:\n\n    XFORMERS_AVAILABLE = True\nexcept ImportError:\n    logger.warning(\"xFormers not available\")\n    XFORMERS_AVAILABLE = False\n\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        proj_bias: bool = True,\n        ffn_bias: bool = True,\n        drop: float = 0.0,\n        attn_drop: float = 0.0,\n        init_values=None,\n        drop_path: float = 0.0,\n        act_layer: Callable[..., nn.Module] = nn.GELU,\n        norm_layer: Callable[..., nn.Module] = nn.LayerNorm,\n        attn_class: Callable[..., nn.Module] = Attention,",
    "prefix": "import logging\nimport torch\nfrom typing import Callable, List, Any, Tuple, Dict\nfrom torch import nn, Tensor\nfrom .attention import Attention, MemEffAttention\nfrom .drop_path import DropPath\nfrom .layer_scale import LayerScale\nfrom .mlp import Mlp\n    from xformers.ops import fmha\n    from xformers.ops import scaled_index_add, index_select_cat\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# References:\n#   https://github.com/facebookresearch/dino/blob/master/vision_transformer.py\n#   https://github.com/rwightman/pytorch-image-models/tree/master/timm/layers/patch_embed.py\n\n\n\n\n\nlogger = logging.getLogger(\"dinov2\")\n\n\ntry:\n\n    XFORMERS_AVAILABLE = True\nexcept ImportError:\n    logger.warning(\"xFormers not available\")\n    XFORMERS_AVAILABLE = False\n\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        proj_bias: bool = True,\n        ffn_bias: bool = True,\n        drop: float = 0.0,\n        attn_drop: float = 0.0,\n        init_values=None,\n        drop_path: float = 0.0,\n        act_layer: Callable[..., nn.Module] = nn.GELU,\n        norm_layer: Callable[..., nn.Module] = nn.LayerNorm,\n        attn_class: Callable[..., nn.Module] = Attention,",
    "suffix": ""
  },
  {
    "name": "OmchainFoundation/evm-indexer:tests/test_range.py@1584",
    "canonical_solution": "decoder = Decoder(fetcher=fetcher)",
    "prompt": "import sys\nimport os\nfrom evm_indexer.fetcher import Fetcher\nfrom evm_indexer.decoder import Decoder\nfrom evm_indexer.internal_tracer import InternalTracer\nfrom web3 import Web3\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\n\nNODE_URL = 'https://seed.omchain.io'\n\nfetcher = Fetcher(NODE_URL, is_poa=True)",
    "prefix": "import sys\nimport os\nfrom evm_indexer.fetcher import Fetcher\nfrom evm_indexer.decoder import Decoder\nfrom evm_indexer.internal_tracer import InternalTracer\nfrom web3 import Web3\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\n\nNODE_URL = 'https://seed.omchain.io'\n\nfetcher = Fetcher(NODE_URL, is_poa=True)",
    "suffix": ""
  },
  {
    "name": "omkarcloud/google-scraper:src/google_scraper.py@1171",
    "canonical_solution": "    def search(query:  Union[str, List[str]],  max: Optional[int] = None, key: Optional[str] =None,  use_cache: bool = True) -> Dict:",
    "prompt": "from typing import List,Optional, Union, Dict\nfrom botasaurus import bt\nfrom .write_output import write_output\nfrom .search import FAILED_DUE_TO_CREDITS_EXHAUSTED, FAILED_DUE_TO_NO_KEY,FAILED_DUE_TO_NOT_SUBSCRIBED, FAILED_DUE_TO_UNKNOWN_ERROR, search\n\n\n\ndef clean_data(social_details):\n    success, credits_exhausted, not_subscribed, unknown_error, no_key = [], [], [], [], []\n\n    for detail in social_details:\n        if detail.get(\"error\") is None:\n            success.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_CREDITS_EXHAUSTED:\n            credits_exhausted.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_NOT_SUBSCRIBED:\n            not_subscribed.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_UNKNOWN_ERROR:\n            unknown_error.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_NO_KEY:\n            no_key.append(detail)\n\n    return success, credits_exhausted, not_subscribed, unknown_error, no_key\n\ndef print_data_errors(credits_exhausted, not_subscribed, unknown_error, no_key):\n    \n    if credits_exhausted:\n        name = \"queries\" if len(credits_exhausted) > 1 else \"query\"\n        print(f\"Could not get data for {len(credits_exhausted)} {name} due to credit exhaustion. Please consider upgrading your plan by visiting https://rapidapi.com/Chetan11dev/api/google-scraper/pricing to continue scraping data.\")\n\n    if not_subscribed:\n        name = \"queries\" if len(not_subscribed) > 1 else \"query\"\n        print(f\"Could not get data for {len(not_subscribed)} {name} as you are not subscribed to Google Scraper API. Please subscribe to a free plan by visiting https://rapidapi.com/Chetan11dev/api/google-scraper/pricing\")\n\n    if unknown_error:\n        name = \"queries\" if len(unknown_error) > 1 else \"query\"\n        print(f\"Could not get data for {len(unknown_error)} {name} due to Unknown Error.\")\n\n    if no_key:\n        name = \"queries\" if len(no_key) > 1 else \"query\"\n        print(f\"Could not get data for {len(no_key)} {name} as you are not subscribed to Google Scraper API. Please subscribe to a free plan by visiting https://rapidapi.com/Chetan11dev/api/google-scraper/pricing\")\n\n      \nclass Google:\n    \n    @staticmethod",
    "prefix": "from typing import List,Optional, Union, Dict\nfrom botasaurus import bt\nfrom .write_output import write_output\nfrom .search import FAILED_DUE_TO_CREDITS_EXHAUSTED, FAILED_DUE_TO_NO_KEY,FAILED_DUE_TO_NOT_SUBSCRIBED, FAILED_DUE_TO_UNKNOWN_ERROR, search\n\n\n\ndef clean_data(social_details):\n    success, credits_exhausted, not_subscribed, unknown_error, no_key = [], [], [], [], []\n\n    for detail in social_details:\n        if detail.get(\"error\") is None:\n            success.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_CREDITS_EXHAUSTED:\n            credits_exhausted.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_NOT_SUBSCRIBED:\n            not_subscribed.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_UNKNOWN_ERROR:\n            unknown_error.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_NO_KEY:\n            no_key.append(detail)\n\n    return success, credits_exhausted, not_subscribed, unknown_error, no_key\n\ndef print_data_errors(credits_exhausted, not_subscribed, unknown_error, no_key):\n    \n    if credits_exhausted:\n        name = \"queries\" if len(credits_exhausted) > 1 else \"query\"\n        print(f\"Could not get data for {len(credits_exhausted)} {name} due to credit exhaustion. Please consider upgrading your plan by visiting https://rapidapi.com/Chetan11dev/api/google-scraper/pricing to continue scraping data.\")\n\n    if not_subscribed:\n        name = \"queries\" if len(not_subscribed) > 1 else \"query\"\n        print(f\"Could not get data for {len(not_subscribed)} {name} as you are not subscribed to Google Scraper API. Please subscribe to a free plan by visiting https://rapidapi.com/Chetan11dev/api/google-scraper/pricing\")\n\n    if unknown_error:\n        name = \"queries\" if len(unknown_error) > 1 else \"query\"\n        print(f\"Could not get data for {len(unknown_error)} {name} due to Unknown Error.\")\n\n    if no_key:\n        name = \"queries\" if len(no_key) > 1 else \"query\"\n        print(f\"Could not get data for {len(no_key)} {name} as you are not subscribed to Google Scraper API. Please subscribe to a free plan by visiting https://rapidapi.com/Chetan11dev/api/google-scraper/pricing\")\n\n      \nclass Google:\n    \n    @staticmethod",
    "suffix": ""
  },
  {
    "name": "AI2lab/comfyUI-tool-2lab:nodes/tool/preview.py@812",
    "canonical_solution": "        file_path = downloadFileToTempFolder(url)",
    "prompt": "import numpy as np\nimport torch\nfrom PIL import Image\nfrom ..common.utils import downloadFileToTempFolder\nfrom ..constants import get_project_name, get_project_category\n\n\nNODE_CATEGORY = get_project_category(\"util/preview\")\n\nclass ShowText:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"string\": (\"STRING\", {\"forceInput\": True}),\n            },\n            \"hidden\": {\n                \"unique_id\": \"UNIQUE_ID\",\n                \"extra_pnginfo\": \"EXTRA_PNGINFO\",},\n        }\n\n    NAME = get_project_name('show_text')\n    CATEGORY = NODE_CATEGORY\n    RETURN_TYPES = (\"STRING\",)\n    RETURN_NAMES = (\"string\",)\n    OUTPUT_NODE = True\n    FUNCTION = \"doWork\"\n\n    def doWork(self, string, unique_id=None, extra_pnginfo=None):\n        return {\"ui\": {\"string\": [string, ]}, \"result\": (string,)}\n\nclass ShowWebImage:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image_url\": (\"STRING\", {\"multiline\": False}),\n                \"RGBA\": ([\"false\", \"true\"],{\"default\":False}),\n            },\n        }\n\n    NAME = get_project_name('show_web_image')\n    CATEGORY = NODE_CATEGORY\n    RETURN_TYPES = (\"IMAGE\", \"MASK\",\"TEXT\",\"filePath\")\n    RETURN_NAMES = (\"image\", \"mask\",\"image_url\",\"filePath\")\n    OUTPUT_NODE = True\n    FUNCTION = \"doWork\"\n\n    def doWork(self, image_url, RGBA):\n        print(image_url)\n        i = None\n        file_path = ''\n        try:\n            if image_url.startswith('http'):\n                file_path,i = self.download_image(image_url)\n            else:\n                file_path = image_url\n                i = Image.open(image_url)\n\n            if not i:\n                return\n\n            image = i\n            if not RGBA:\n                image = image.convert('RGB')\n            image = np.array(image).astype(np.float32) / 255.0\n            image = torch.from_numpy(image)[None,]\n\n            # RGBA - mask\n            if 'A' in i.getbands():\n                mask = np.array(i.getchannel('A')).astype(np.float32) / 255.0\n                mask = 1. - torch.from_numpy(mask)\n            else:\n                mask = torch.zeros((64, 64), dtype=torch.float32, device=\"cpu\")\n\n            return (image, mask, image_url,file_path)\n\n        except :\n            pass\n        return (None, None, image_url,file_path)\n\n    def download_image(self, url):",
    "prefix": "import numpy as np\nimport torch\nfrom PIL import Image\nfrom ..common.utils import downloadFileToTempFolder\nfrom ..constants import get_project_name, get_project_category\n\n\nNODE_CATEGORY = get_project_category(\"util/preview\")\n\nclass ShowText:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"string\": (\"STRING\", {\"forceInput\": True}),\n            },\n            \"hidden\": {\n                \"unique_id\": \"UNIQUE_ID\",\n                \"extra_pnginfo\": \"EXTRA_PNGINFO\",},\n        }\n\n    NAME = get_project_name('show_text')\n    CATEGORY = NODE_CATEGORY\n    RETURN_TYPES = (\"STRING\",)\n    RETURN_NAMES = (\"string\",)\n    OUTPUT_NODE = True\n    FUNCTION = \"doWork\"\n\n    def doWork(self, string, unique_id=None, extra_pnginfo=None):\n        return {\"ui\": {\"string\": [string, ]}, \"result\": (string,)}\n\nclass ShowWebImage:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image_url\": (\"STRING\", {\"multiline\": False}),\n                \"RGBA\": ([\"false\", \"true\"],{\"default\":False}),\n            },\n        }\n\n    NAME = get_project_name('show_web_image')\n    CATEGORY = NODE_CATEGORY\n    RETURN_TYPES = (\"IMAGE\", \"MASK\",\"TEXT\",\"filePath\")\n    RETURN_NAMES = (\"image\", \"mask\",\"image_url\",\"filePath\")\n    OUTPUT_NODE = True\n    FUNCTION = \"doWork\"\n\n    def doWork(self, image_url, RGBA):\n        print(image_url)\n        i = None\n        file_path = ''\n        try:\n            if image_url.startswith('http'):\n                file_path,i = self.download_image(image_url)\n            else:\n                file_path = image_url\n                i = Image.open(image_url)\n\n            if not i:\n                return\n\n            image = i\n            if not RGBA:\n                image = image.convert('RGB')\n            image = np.array(image).astype(np.float32) / 255.0\n            image = torch.from_numpy(image)[None,]\n\n            # RGBA - mask\n            if 'A' in i.getbands():\n                mask = np.array(i.getchannel('A')).astype(np.float32) / 255.0\n                mask = 1. - torch.from_numpy(mask)\n            else:\n                mask = torch.zeros((64, 64), dtype=torch.float32, device=\"cpu\")\n\n            return (image, mask, image_url,file_path)\n\n        except :\n            pass\n        return (None, None, image_url,file_path)\n\n    def download_image(self, url):",
    "suffix": ""
  },
  {
    "name": "Amirtheahmed/ddd-cqrs-fastapi:src/contexts/photostore/photo/application/createone/PhotoCreator.py@891",
    "canonical_solution": "    async def run(self, photo_id: PhotoId, name: PhotoName, user_id: UserId, file: PhotoFile):",
    "prompt": "from src.contexts.photostore.photo.domain.PhotoRepository import PhotoRepository\nfrom src.contexts.photostore.photo.domain.entities.Photo import Photo\nfrom src.contexts.photostore.photo.domain.entities.PhotoFile import PhotoFile\nfrom src.contexts.photostore.photo.domain.entities.PhotoId import PhotoId\nfrom src.contexts.photostore.photo.domain.entities.PhotoName import PhotoName\nfrom src.contexts.photostore.photo.domain.entities.UserId import UserId\nfrom src.contexts.shared.domain.EventBus import EventBus\n\n\nclass PhotoCreator:\n\n    def __init__(self, photo_repository: PhotoRepository, event_bus: EventBus):\n        self.__photo_repository = photo_repository\n        self.__event_bus = event_bus\n",
    "prefix": "from src.contexts.photostore.photo.domain.PhotoRepository import PhotoRepository\nfrom src.contexts.photostore.photo.domain.entities.Photo import Photo\nfrom src.contexts.photostore.photo.domain.entities.PhotoFile import PhotoFile\nfrom src.contexts.photostore.photo.domain.entities.PhotoId import PhotoId\nfrom src.contexts.photostore.photo.domain.entities.PhotoName import PhotoName\nfrom src.contexts.photostore.photo.domain.entities.UserId import UserId\nfrom src.contexts.shared.domain.EventBus import EventBus\n\n\nclass PhotoCreator:\n\n    def __init__(self, photo_repository: PhotoRepository, event_bus: EventBus):\n        self.__photo_repository = photo_repository\n        self.__event_bus = event_bus\n",
    "suffix": ""
  },
  {
    "name": "JINO-ROHIT/RAG-with-Memory:vlite_db/main.py@1156",
    "canonical_solution": "        chunks = chop_and_chunk(text)",
    "prompt": "import numpy as np\nimport datetime\nfrom uuid import uuid4\nfrom .model import EmbeddingModel\nfrom .utils import chop_and_chunk, cos_sim\n\nclass VLite:\n    '''\n    vlite is a simple vector database that stores vectors in a numpy array.\n    '''\n    def __init__(self, collection=None,device='mps',model_name=None):\n\n\t\t# Filename must be unique between runs. Saving to the same file will append vectors to previous run's vectors\n        if collection is None:\n            current_datetime = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            collection = f\"vlite_{current_datetime}.npz\"\n            \n        self.collection = collection\n        self.device = device\n        self.model = EmbeddingModel() if model_name is None else EmbeddingModel(model_name)\n        try:\n            with np.load(self.collection, allow_pickle=True) as data:\n                self.texts = data['texts'].tolist()\n                self.metadata = data['metadata'].tolist()\n                self.vectors = data['vectors']\n        except FileNotFoundError:\n            self.texts = []\n            self.metadata = {}\n            self.vectors = np.empty((0, self.model.dimension))\n    \n    def add_vector(self, vector):\n        self.vectors = np.vstack((self.vectors, vector))\n\n    def get_similar_vectors(self, vector, top_k=5):\n        sims = cos_sim(vector, self.vectors)\n        sims = sims[0]\n        # print(\"[get_similar_vectors] Sims:\", sims.shape)\n        top_k_idx = np.argsort(sims)[::-1][:top_k]\n        # print(\"[get_similar_vectors] Top k idx:\", top_k_idx)\n        # print(\"[get_similar_vectors] Top k sims:\", sims[top_k_idx])\n        return top_k_idx, sims[top_k_idx]\n\n    def memorize(self, text, id=None, metadata=None):\n        id = id or str(uuid4())",
    "prefix": "import numpy as np\nimport datetime\nfrom uuid import uuid4\nfrom .model import EmbeddingModel\nfrom .utils import chop_and_chunk, cos_sim\n\nclass VLite:\n    '''\n    vlite is a simple vector database that stores vectors in a numpy array.\n    '''\n    def __init__(self, collection=None,device='mps',model_name=None):\n\n\t\t# Filename must be unique between runs. Saving to the same file will append vectors to previous run's vectors\n        if collection is None:\n            current_datetime = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            collection = f\"vlite_{current_datetime}.npz\"\n            \n        self.collection = collection\n        self.device = device\n        self.model = EmbeddingModel() if model_name is None else EmbeddingModel(model_name)\n        try:\n            with np.load(self.collection, allow_pickle=True) as data:\n                self.texts = data['texts'].tolist()\n                self.metadata = data['metadata'].tolist()\n                self.vectors = data['vectors']\n        except FileNotFoundError:\n            self.texts = []\n            self.metadata = {}\n            self.vectors = np.empty((0, self.model.dimension))\n    \n    def add_vector(self, vector):\n        self.vectors = np.vstack((self.vectors, vector))\n\n    def get_similar_vectors(self, vector, top_k=5):\n        sims = cos_sim(vector, self.vectors)\n        sims = sims[0]\n        # print(\"[get_similar_vectors] Sims:\", sims.shape)\n        top_k_idx = np.argsort(sims)[::-1][:top_k]\n        # print(\"[get_similar_vectors] Top k idx:\", top_k_idx)\n        # print(\"[get_similar_vectors] Top k sims:\", sims[top_k_idx])\n        return top_k_idx, sims[top_k_idx]\n\n    def memorize(self, text, id=None, metadata=None):\n        id = id or str(uuid4())",
    "suffix": ""
  },
  {
    "name": "avataar/bg_electricity_regulated_pricing:custom_components/bg_electricity_regulated_pricing/sensor.py@753",
    "canonical_solution": "        price_day = config_entry.options[CONF_CUSTOM_DAY_PRICE]",
    "prompt": "from homeassistant.components.sensor import SensorEntity, SensorEntityDescription, \\\n    SensorStateClass\nfrom homeassistant.config_entries import ConfigEntry\nfrom homeassistant.core import HomeAssistant\nfrom homeassistant.helpers.entity_platform import AddEntitiesCallback\nfrom homeassistant.util import utcnow\nfrom homeassistant.helpers.device_registry import DeviceEntryType, DeviceInfo\nfrom .const import CONF_TARIFF_TYPE, CONF_PROVIDER, CONF_CUSTOM_DAY_PRICE, \\\n    CONF_CUSTOM_NIGHT_PRICE, PROVIDER_PRICES, CONF_CLOCK_OFFSET, \\\n    BGN_PER_KILOWATT_HOUR, VAT_RATE, DOMAIN\n\"\"\"Sensor platform for bg_electricity_regulated_pricing integration.\"\"\"\nfrom __future__ import annotations\n\n\n\n\n\nasync def async_setup_entry(\n        hass: HomeAssistant,\n        config_entry: ConfigEntry,\n        async_add_entities: AddEntitiesCallback,\n) -> None:\n    \"\"\"Initialize bg_electricity_regulated_pricing config entry.\"\"\"\n    name = config_entry.title\n    unique_id = config_entry.entry_id\n\n    tariff_type = config_entry.options[CONF_TARIFF_TYPE]\n    clock_offset = config_entry.options[CONF_CLOCK_OFFSET]\n    provider = config_entry.options[CONF_PROVIDER]\n    if provider == \"custom\":",
    "prefix": "from homeassistant.components.sensor import SensorEntity, SensorEntityDescription, \\\n    SensorStateClass\nfrom homeassistant.config_entries import ConfigEntry\nfrom homeassistant.core import HomeAssistant\nfrom homeassistant.helpers.entity_platform import AddEntitiesCallback\nfrom homeassistant.util import utcnow\nfrom homeassistant.helpers.device_registry import DeviceEntryType, DeviceInfo\nfrom .const import CONF_TARIFF_TYPE, CONF_PROVIDER, CONF_CUSTOM_DAY_PRICE, \\\n    CONF_CUSTOM_NIGHT_PRICE, PROVIDER_PRICES, CONF_CLOCK_OFFSET, \\\n    BGN_PER_KILOWATT_HOUR, VAT_RATE, DOMAIN\n\"\"\"Sensor platform for bg_electricity_regulated_pricing integration.\"\"\"\nfrom __future__ import annotations\n\n\n\n\n\nasync def async_setup_entry(\n        hass: HomeAssistant,\n        config_entry: ConfigEntry,\n        async_add_entities: AddEntitiesCallback,\n) -> None:\n    \"\"\"Initialize bg_electricity_regulated_pricing config entry.\"\"\"\n    name = config_entry.title\n    unique_id = config_entry.entry_id\n\n    tariff_type = config_entry.options[CONF_TARIFF_TYPE]\n    clock_offset = config_entry.options[CONF_CLOCK_OFFSET]\n    provider = config_entry.options[CONF_PROVIDER]\n    if provider == \"custom\":",
    "suffix": ""
  },
  {
    "name": "Qazalbash/jaxtro:jaxtro/main.py@981",
    "canonical_solution": "    pg = PopulationGenerator(general=general, models=models)",
    "prompt": "from .utils import PopulationGenerator, parser\n# Copyright 2023 The Jaxtro Authors\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\ndef main():\n    args = parser.cmd_parser.parse_args()\n    configuration_dict = parser.parse_config(args.my_config)\n\n    general = configuration_dict['general']\n    models = [configuration_dict.get('mass_model', None), configuration_dict.get('spin_model', None)]\n",
    "prefix": "from .utils import PopulationGenerator, parser\n# Copyright 2023 The Jaxtro Authors\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\ndef main():\n    args = parser.cmd_parser.parse_args()\n    configuration_dict = parser.parse_config(args.my_config)\n\n    general = configuration_dict['general']\n    models = [configuration_dict.get('mass_model', None), configuration_dict.get('spin_model', None)]\n",
    "suffix": ""
  },
  {
    "name": "smonsays/modular-hyperteacher:metax/learner/reptile.py@1497",
    "canonical_solution": "class Reptile(MetaGradLearner):",
    "prompt": "import jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport optax\nfrom metax.data import Dataset, batch_generator\nfrom metax.module import LearnedInit\nfrom metax.module.init import LearnedInitMetaParams\nfrom metax.utils import append_keys\nfrom .base import MetaGradLearner\n\"\"\"\nCopyright (c) Simon Schug\nAll rights reserved.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\"\"\"\n\n\n\n",
    "prefix": "import jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport optax\nfrom metax.data import Dataset, batch_generator\nfrom metax.module import LearnedInit\nfrom metax.module.init import LearnedInitMetaParams\nfrom metax.utils import append_keys\nfrom .base import MetaGradLearner\n\"\"\"\nCopyright (c) Simon Schug\nAll rights reserved.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\"\"\"\n\n\n\n",
    "suffix": ""
  },
  {
    "name": "AContesini/Convert_PDF_to_DOCX_or_vice-versa:venv/Lib/site-packages/tqdm/contrib/concurrent.py@1153",
    "canonical_solution": "                 TqdmWarning, stacklevel=2)",
    "prompt": "from contextlib import contextmanager\nfrom operator import length_hint\nfrom os import cpu_count\nfrom ..auto import tqdm as tqdm_auto\nfrom ..std import TqdmWarning\n    from concurrent.futures import ThreadPoolExecutor\n    from concurrent.futures import ProcessPoolExecutor\n            from warnings import warn\n\"\"\"\nThin wrappers around `concurrent.futures`.\n\"\"\"\n\n\n__author__ = {\"github.com/\": [\"casperdcl\"]}\n__all__ = ['thread_map', 'process_map']\n\n\n@contextmanager\ndef ensure_lock(tqdm_class, lock_name=\"\"):\n    \"\"\"get (create if necessary) and then restore `tqdm_class`'s lock\"\"\"\n    old_lock = getattr(tqdm_class, '_lock', None)  # don't create a new lock\n    lock = old_lock or tqdm_class.get_lock()  # maybe create a new lock\n    lock = getattr(lock, lock_name, lock)  # maybe subtype\n    tqdm_class.set_lock(lock)\n    yield lock\n    if old_lock is None:\n        del tqdm_class._lock\n    else:\n        tqdm_class.set_lock(old_lock)\n\n\ndef _executor_map(PoolExecutor, fn, *iterables, **tqdm_kwargs):\n    \"\"\"\n    Implementation of `thread_map` and `process_map`.\n\n    Parameters\n    ----------\n    tqdm_class  : [default: tqdm.auto.tqdm].\n    max_workers  : [default: min(32, cpu_count() + 4)].\n    chunksize  : [default: 1].\n    lock_name  : [default: \"\":str].\n    \"\"\"\n    kwargs = tqdm_kwargs.copy()\n    if \"total\" not in kwargs:\n        kwargs[\"total\"] = length_hint(iterables[0])\n    tqdm_class = kwargs.pop(\"tqdm_class\", tqdm_auto)\n    max_workers = kwargs.pop(\"max_workers\", min(32, cpu_count() + 4))\n    chunksize = kwargs.pop(\"chunksize\", 1)\n    lock_name = kwargs.pop(\"lock_name\", \"\")\n    with ensure_lock(tqdm_class, lock_name=lock_name) as lk:\n        # share lock in case workers are already using `tqdm`\n        with PoolExecutor(max_workers=max_workers, initializer=tqdm_class.set_lock,\n                          initargs=(lk,)) as ex:\n            return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))\n\n\ndef thread_map(fn, *iterables, **tqdm_kwargs):\n    \"\"\"\n    Equivalent of `list(map(fn, *iterables))`\n    driven by `concurrent.futures.ThreadPoolExecutor`.\n\n    Parameters\n    ----------\n    tqdm_class  : optional\n        `tqdm` class to use for bars [default: tqdm.auto.tqdm].\n    max_workers  : int, optional\n        Maximum number of workers to spawn; passed to\n        `concurrent.futures.ThreadPoolExecutor.__init__`.\n        [default: max(32, cpu_count() + 4)].\n    \"\"\"\n    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)\n\n\ndef process_map(fn, *iterables, **tqdm_kwargs):\n    \"\"\"\n    Equivalent of `list(map(fn, *iterables))`\n    driven by `concurrent.futures.ProcessPoolExecutor`.\n\n    Parameters\n    ----------\n    tqdm_class  : optional\n        `tqdm` class to use for bars [default: tqdm.auto.tqdm].\n    max_workers  : int, optional\n        Maximum number of workers to spawn; passed to\n        `concurrent.futures.ProcessPoolExecutor.__init__`.\n        [default: min(32, cpu_count() + 4)].\n    chunksize  : int, optional\n        Size of chunks sent to worker processes; passed to\n        `concurrent.futures.ProcessPoolExecutor.map`. [default: 1].\n    lock_name  : str, optional\n        Member of `tqdm_class.get_lock()` to use [default: mp_lock].\n    \"\"\"\n    if iterables and \"chunksize\" not in tqdm_kwargs:\n        # default `chunksize=1` has poor performance for large iterables\n        # (most time spent dispatching items to workers).\n        longest_iterable_len = max(map(length_hint, iterables))\n        if longest_iterable_len > 1000:\n            warn(\"Iterable length %d > 1000 but `chunksize` is not set.\"\n                 \" This may seriously degrade multiprocess performance.\"\n                 \" Set `chunksize=1` or more.\" % longest_iterable_len,",
    "prefix": "from contextlib import contextmanager\nfrom operator import length_hint\nfrom os import cpu_count\nfrom ..auto import tqdm as tqdm_auto\nfrom ..std import TqdmWarning\n    from concurrent.futures import ThreadPoolExecutor\n    from concurrent.futures import ProcessPoolExecutor\n            from warnings import warn\n\"\"\"\nThin wrappers around `concurrent.futures`.\n\"\"\"\n\n\n__author__ = {\"github.com/\": [\"casperdcl\"]}\n__all__ = ['thread_map', 'process_map']\n\n\n@contextmanager\ndef ensure_lock(tqdm_class, lock_name=\"\"):\n    \"\"\"get (create if necessary) and then restore `tqdm_class`'s lock\"\"\"\n    old_lock = getattr(tqdm_class, '_lock', None)  # don't create a new lock\n    lock = old_lock or tqdm_class.get_lock()  # maybe create a new lock\n    lock = getattr(lock, lock_name, lock)  # maybe subtype\n    tqdm_class.set_lock(lock)\n    yield lock\n    if old_lock is None:\n        del tqdm_class._lock\n    else:\n        tqdm_class.set_lock(old_lock)\n\n\ndef _executor_map(PoolExecutor, fn, *iterables, **tqdm_kwargs):\n    \"\"\"\n    Implementation of `thread_map` and `process_map`.\n\n    Parameters\n    ----------\n    tqdm_class  : [default: tqdm.auto.tqdm].\n    max_workers  : [default: min(32, cpu_count() + 4)].\n    chunksize  : [default: 1].\n    lock_name  : [default: \"\":str].\n    \"\"\"\n    kwargs = tqdm_kwargs.copy()\n    if \"total\" not in kwargs:\n        kwargs[\"total\"] = length_hint(iterables[0])\n    tqdm_class = kwargs.pop(\"tqdm_class\", tqdm_auto)\n    max_workers = kwargs.pop(\"max_workers\", min(32, cpu_count() + 4))\n    chunksize = kwargs.pop(\"chunksize\", 1)\n    lock_name = kwargs.pop(\"lock_name\", \"\")\n    with ensure_lock(tqdm_class, lock_name=lock_name) as lk:\n        # share lock in case workers are already using `tqdm`\n        with PoolExecutor(max_workers=max_workers, initializer=tqdm_class.set_lock,\n                          initargs=(lk,)) as ex:\n            return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))\n\n\ndef thread_map(fn, *iterables, **tqdm_kwargs):\n    \"\"\"\n    Equivalent of `list(map(fn, *iterables))`\n    driven by `concurrent.futures.ThreadPoolExecutor`.\n\n    Parameters\n    ----------\n    tqdm_class  : optional\n        `tqdm` class to use for bars [default: tqdm.auto.tqdm].\n    max_workers  : int, optional\n        Maximum number of workers to spawn; passed to\n        `concurrent.futures.ThreadPoolExecutor.__init__`.\n        [default: max(32, cpu_count() + 4)].\n    \"\"\"\n    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)\n\n\ndef process_map(fn, *iterables, **tqdm_kwargs):\n    \"\"\"\n    Equivalent of `list(map(fn, *iterables))`\n    driven by `concurrent.futures.ProcessPoolExecutor`.\n\n    Parameters\n    ----------\n    tqdm_class  : optional\n        `tqdm` class to use for bars [default: tqdm.auto.tqdm].\n    max_workers  : int, optional\n        Maximum number of workers to spawn; passed to\n        `concurrent.futures.ProcessPoolExecutor.__init__`.\n        [default: min(32, cpu_count() + 4)].\n    chunksize  : int, optional\n        Size of chunks sent to worker processes; passed to\n        `concurrent.futures.ProcessPoolExecutor.map`. [default: 1].\n    lock_name  : str, optional\n        Member of `tqdm_class.get_lock()` to use [default: mp_lock].\n    \"\"\"\n    if iterables and \"chunksize\" not in tqdm_kwargs:\n        # default `chunksize=1` has poor performance for large iterables\n        # (most time spent dispatching items to workers).\n        longest_iterable_len = max(map(length_hint, iterables))\n        if longest_iterable_len > 1000:\n            warn(\"Iterable length %d > 1000 but `chunksize` is not set.\"\n                 \" This may seriously degrade multiprocess performance.\"\n                 \" Set `chunksize=1` or more.\" % longest_iterable_len,",
    "suffix": ""
  },
  {
    "name": "willfinnigan/RetroBioCat_2:rbc2/expansion/expanders/action_getters/aizynthfinder/aizynthfinder_actions.py@1573",
    "canonical_solution": "        fingerprint = fingerprints.get_mol_fingerprint(mol, 2, nBits=len(self.policy_model))",
    "prompt": "import time\nimport numpy as np\nimport pandas as pd\nfrom rdkit import Chem\nfrom rbc2.configs.download_data_files.download_aizynthfinder import does_aizynthfinder_exist, \\\n    download_aizynthfinder_model\nfrom rbc2.utils.add_logger import add_logger\nfrom rbc2.configs.data_path import path_to_data_folder\nfrom rbc2.configs.expansion_config import Expansion_Config\nfrom rbc2.utils import load_keras_models, fingerprints\n\n\n\n\ndata_folder = f'{path_to_data_folder}/aizynthfinder'\n\nclass AizynthfinderActionGetter():\n\n    def __init__(self,\n                 template_column='retro_template',\n                 cutoff_cumulative=0.995,\n                 cutoff_number=50,\n                 log_level='WARNING'):\n\n        self.logger = add_logger('AIZynthfinder_Actions', level=log_level)\n        self.policy_model = None\n        self.templates = None\n\n        self.template_column = template_column\n        self.cutoff_cumulative = cutoff_cumulative\n        self.cutoff_number = cutoff_number\n\n        if does_aizynthfinder_exist() == False:\n            download_aizynthfinder_model()\n\n    def load_model(self):\n        if self.policy_model == None:\n            policy_path = data_folder + '/uspto_model.hdf5'\n            self.policy_model = load_keras_models.LocalKerasModel(policy_path)\n        if self.templates == None:\n            templates_path = data_folder + '/uspto_templates.hdf5'\n            self.templates = pd.read_hdf(templates_path, \"table\")\n\n    def get_actions(self, smi):\n        reactions = []\n        priors = []\n        template_column = self.template_column\n\n        mol = Chem.MolFromSmiles(smi)\n\n        all_transforms_prop = self._predict(mol)\n\n        probable_transforms_idx = self._cutoff_predictions(all_transforms_prop)\n\n        possible_moves = self.templates.iloc[probable_transforms_idx]\n        probs = all_transforms_prop[probable_transforms_idx]\n\n        priors.extend(probs)\n        for idx, (move_index, move) in enumerate(possible_moves.iterrows()):\n            metadata = dict(move)\n            del metadata[template_column]\n            metadata[\"policy_probability\"] = round(float(probs[idx]), 5)\n            metadata[\"template_code\"] = move_index\n\n            reaction = {'smarts': move[template_column],\n                        'metadata': metadata,\n                        'prior': priors[idx]}\n\n            reactions.append(reaction)\n\n        return reactions\n\n    def get_rxns(self, smile):\n        if self.policy_model == None:\n            self.load_model()\n\n        reactions = self.get_actions(smile)\n        rxns = {}\n        metadata = {}\n\n        for reaction in reactions:\n            name = f\"Chem_{reaction['metadata']['classification']}\"\n            num = 1\n            extra_string = f\"__{num}\"\n            while name+extra_string in rxns:\n                extra_string = f\"__{num}\"\n                num += 1\n            name = name+extra_string\n            smarts = reaction['smarts']\n            if self._does_smarts_only_one_reactants(smarts):\n                rxns[name] = [smarts]\n            else:\n                rxns[name] = []\n            metadata[name] = reaction['metadata']\n        return rxns, metadata\n\n    def _predict(self, mol):",
    "prefix": "import time\nimport numpy as np\nimport pandas as pd\nfrom rdkit import Chem\nfrom rbc2.configs.download_data_files.download_aizynthfinder import does_aizynthfinder_exist, \\\n    download_aizynthfinder_model\nfrom rbc2.utils.add_logger import add_logger\nfrom rbc2.configs.data_path import path_to_data_folder\nfrom rbc2.configs.expansion_config import Expansion_Config\nfrom rbc2.utils import load_keras_models, fingerprints\n\n\n\n\ndata_folder = f'{path_to_data_folder}/aizynthfinder'\n\nclass AizynthfinderActionGetter():\n\n    def __init__(self,\n                 template_column='retro_template',\n                 cutoff_cumulative=0.995,\n                 cutoff_number=50,\n                 log_level='WARNING'):\n\n        self.logger = add_logger('AIZynthfinder_Actions', level=log_level)\n        self.policy_model = None\n        self.templates = None\n\n        self.template_column = template_column\n        self.cutoff_cumulative = cutoff_cumulative\n        self.cutoff_number = cutoff_number\n\n        if does_aizynthfinder_exist() == False:\n            download_aizynthfinder_model()\n\n    def load_model(self):\n        if self.policy_model == None:\n            policy_path = data_folder + '/uspto_model.hdf5'\n            self.policy_model = load_keras_models.LocalKerasModel(policy_path)\n        if self.templates == None:\n            templates_path = data_folder + '/uspto_templates.hdf5'\n            self.templates = pd.read_hdf(templates_path, \"table\")\n\n    def get_actions(self, smi):\n        reactions = []\n        priors = []\n        template_column = self.template_column\n\n        mol = Chem.MolFromSmiles(smi)\n\n        all_transforms_prop = self._predict(mol)\n\n        probable_transforms_idx = self._cutoff_predictions(all_transforms_prop)\n\n        possible_moves = self.templates.iloc[probable_transforms_idx]\n        probs = all_transforms_prop[probable_transforms_idx]\n\n        priors.extend(probs)\n        for idx, (move_index, move) in enumerate(possible_moves.iterrows()):\n            metadata = dict(move)\n            del metadata[template_column]\n            metadata[\"policy_probability\"] = round(float(probs[idx]), 5)\n            metadata[\"template_code\"] = move_index\n\n            reaction = {'smarts': move[template_column],\n                        'metadata': metadata,\n                        'prior': priors[idx]}\n\n            reactions.append(reaction)\n\n        return reactions\n\n    def get_rxns(self, smile):\n        if self.policy_model == None:\n            self.load_model()\n\n        reactions = self.get_actions(smile)\n        rxns = {}\n        metadata = {}\n\n        for reaction in reactions:\n            name = f\"Chem_{reaction['metadata']['classification']}\"\n            num = 1\n            extra_string = f\"__{num}\"\n            while name+extra_string in rxns:\n                extra_string = f\"__{num}\"\n                num += 1\n            name = name+extra_string\n            smarts = reaction['smarts']\n            if self._does_smarts_only_one_reactants(smarts):\n                rxns[name] = [smarts]\n            else:\n                rxns[name] = []\n            metadata[name] = reaction['metadata']\n        return rxns, metadata\n\n    def _predict(self, mol):",
    "suffix": ""
  },
  {
    "name": "DomingoJoseCab/AutoTube:utils/edition/edit.py@943",
    "canonical_solution": "    intro = generate_intro(videos, audio_intro)\r",
    "prompt": "import os\r\nimport json\r\nfrom moviepy.editor import CompositeVideoClip\r\nfrom utils.edition.autoediting import load_videos, load_audio, generate_product, generate_intro, generate_outro\r\nfrom utils.edition.autotext import title_intro\r\n    from moviepy.config import change_settings\r\n# ==============================================================================\r\n# AutoTube Script\r\n# Creado por: Domingo Caballero\r\n# Canal de YouTube: https://www.youtube.com/@emprendedomingo?=sub_confirmation=1\r\n# Lista de Correo: https://emprendecondomingo.substack.com/\r\n# ==============================================================================\r\n\r\n\r\n\r\ndef main(videos_path, audios_path, output_path, names, base_path):\r\n    videos = load_videos(videos_path)\r\n    audios = load_audio(audios_path)\r\n\r\n    audio_intro = audios.pop(0)\r\n    audio_outro = audios.pop(-1)\r\n    \r",
    "prefix": "import os\r\nimport json\r\nfrom moviepy.editor import CompositeVideoClip\r\nfrom utils.edition.autoediting import load_videos, load_audio, generate_product, generate_intro, generate_outro\r\nfrom utils.edition.autotext import title_intro\r\n    from moviepy.config import change_settings\r\n# ==============================================================================\r\n# AutoTube Script\r\n# Creado por: Domingo Caballero\r\n# Canal de YouTube: https://www.youtube.com/@emprendedomingo?=sub_confirmation=1\r\n# Lista de Correo: https://emprendecondomingo.substack.com/\r\n# ==============================================================================\r\n\r\n\r\n\r\ndef main(videos_path, audios_path, output_path, names, base_path):\r\n    videos = load_videos(videos_path)\r\n    audios = load_audio(audios_path)\r\n\r\n    audio_intro = audios.pop(0)\r\n    audio_outro = audios.pop(-1)\r\n    \r",
    "suffix": ""
  },
  {
    "name": "gregorybchris/typogenetics:tests/test_search.py@993",
    "canonical_solution": "        strand = Strand.from_str(\"ACGT\")",
    "prompt": "import numpy as np\nfrom typogenetics.search import Editor, EditType\nfrom typogenetics.typogenetics import Strand\n\n\n\nclass TestSearch:\n    def test_select_edit_type(self) -> None:\n        rng = np.random.default_rng(42)\n        assert Editor.select_edit_type(rng) == EditType.INSERT\n\n    def test_mutate(self) -> None:\n        rng = np.random.default_rng(42)",
    "prefix": "import numpy as np\nfrom typogenetics.search import Editor, EditType\nfrom typogenetics.typogenetics import Strand\n\n\n\nclass TestSearch:\n    def test_select_edit_type(self) -> None:\n        rng = np.random.default_rng(42)\n        assert Editor.select_edit_type(rng) == EditType.INSERT\n\n    def test_mutate(self) -> None:\n        rng = np.random.default_rng(42)",
    "suffix": ""
  },
  {
    "name": "chaoren2357/gsplatstudio:gsplatstudio/data/processor/colmapWcam_processor.py@1346",
    "canonical_solution": "            focal_length = fov_to_focal_length(intrinsics['fov'], intrinsics['width'])",
    "prompt": "import gsplatstudio\nimport sqlite3\nfrom gsplatstudio.utils.type_utils import *\nfrom gsplatstudio.data.processor.base_processor import BaseDataProcessor\nfrom pathlib import Path\nfrom gsplatstudio.utils.general_utils import load_json\nfrom gsplatstudio.utils.camera_utils import transform_camera_from_carla_matrix_to_colmap_quaternion, fov_to_focal_length\n\n\n\n@dataclass\nclass ColmapWithCamProcessorConfig:\n    use_gpu: bool = True\n    camera: str = \"OPENCV\"\n    map_ba_global_function_tolerance: float = 0.000001\n\n@gsplatstudio.register(\"colmap_with_cam-processor\")\nclass ColmapWithCamProcessor(BaseDataProcessor):\n    def __init__(self, cfg, logger, source_path) -> None:\n        super().__init__(cfg, logger, source_path)\n    \n    @property\n    def config_class(self):\n        return ColmapWithCamProcessorConfig\n    \n    @property\n    def should_skip(self):\n        cameras_file = Path(self.source_path_str) / \"sparse\" / \"0\" / \"cameras.bin\"\n        images_file = Path(self.source_path_str) / \"sparse\" / \"0\" / \"images.bin\"\n        points3D_file = Path(self.source_path_str) / \"sparse\" / \"0\" / \"points3D.bin\"\n        return cameras_file.exists() and images_file.exists() and points3D_file.exists()\n    \n    def run(self):\n        self.logger.info(\"Start running ColmapWithCamProcessorConfig...\")\n        project_folder = Path(self.source_path_str) / \"distorted\"\n        project_folder.mkdir(parents=True, exist_ok=True)\n        database_path = Path(self.source_path_str) / \"distorted\" / \"database.db\"\n        image_distorted_folder = Path(self.source_path_str) / \"input\"\n        camera_folder = Path(self.source_path_str) / \"camera\"\n\n        ## Feature extraction\n        feature_extractor_cmd = \"colmap feature_extractor\" + \\\n                    f\" --database_path {str(database_path)}\" + \\\n                    f\" --image_path {str(image_distorted_folder)}\" + \\\n                    f\" --ImageReader.single_camera 1\" + \\\n                    f\" --ImageReader.camera_model {self.cfg.camera}\" + \\\n                    f\" --SiftExtraction.use_gpu {int(self.cfg.use_gpu)}\"\n        \n        exit_code = self.run_command_with_realtime_output(feature_extractor_cmd)\n        if exit_code != 0:\n            self.logger.error(f\"Feature extraction failed with code {exit_code}. Exiting.\")\n            exit(exit_code)\n        self.logger.info(\"Finish feature extraction...\")\n\n        ## Create points3D.txt\n        points3D_txt_path = project_folder / 'points3D.txt'\n        open(str(points3D_txt_path), 'w').close()\n\n        ## Create camera.txt\n        camera_txt_path = project_folder / 'cameras.txt'\n        open(str(camera_txt_path), 'w').close()\n\n        unique_cameras = {}\n        camera_id = 1\n        for camera_file in camera_folder.glob('*.json'):\n            camera_data = load_json(camera_file)\n            intrinsics = camera_data['intrinsics']",
    "prefix": "import gsplatstudio\nimport sqlite3\nfrom gsplatstudio.utils.type_utils import *\nfrom gsplatstudio.data.processor.base_processor import BaseDataProcessor\nfrom pathlib import Path\nfrom gsplatstudio.utils.general_utils import load_json\nfrom gsplatstudio.utils.camera_utils import transform_camera_from_carla_matrix_to_colmap_quaternion, fov_to_focal_length\n\n\n\n@dataclass\nclass ColmapWithCamProcessorConfig:\n    use_gpu: bool = True\n    camera: str = \"OPENCV\"\n    map_ba_global_function_tolerance: float = 0.000001\n\n@gsplatstudio.register(\"colmap_with_cam-processor\")\nclass ColmapWithCamProcessor(BaseDataProcessor):\n    def __init__(self, cfg, logger, source_path) -> None:\n        super().__init__(cfg, logger, source_path)\n    \n    @property\n    def config_class(self):\n        return ColmapWithCamProcessorConfig\n    \n    @property\n    def should_skip(self):\n        cameras_file = Path(self.source_path_str) / \"sparse\" / \"0\" / \"cameras.bin\"\n        images_file = Path(self.source_path_str) / \"sparse\" / \"0\" / \"images.bin\"\n        points3D_file = Path(self.source_path_str) / \"sparse\" / \"0\" / \"points3D.bin\"\n        return cameras_file.exists() and images_file.exists() and points3D_file.exists()\n    \n    def run(self):\n        self.logger.info(\"Start running ColmapWithCamProcessorConfig...\")\n        project_folder = Path(self.source_path_str) / \"distorted\"\n        project_folder.mkdir(parents=True, exist_ok=True)\n        database_path = Path(self.source_path_str) / \"distorted\" / \"database.db\"\n        image_distorted_folder = Path(self.source_path_str) / \"input\"\n        camera_folder = Path(self.source_path_str) / \"camera\"\n\n        ## Feature extraction\n        feature_extractor_cmd = \"colmap feature_extractor\" + \\\n                    f\" --database_path {str(database_path)}\" + \\\n                    f\" --image_path {str(image_distorted_folder)}\" + \\\n                    f\" --ImageReader.single_camera 1\" + \\\n                    f\" --ImageReader.camera_model {self.cfg.camera}\" + \\\n                    f\" --SiftExtraction.use_gpu {int(self.cfg.use_gpu)}\"\n        \n        exit_code = self.run_command_with_realtime_output(feature_extractor_cmd)\n        if exit_code != 0:\n            self.logger.error(f\"Feature extraction failed with code {exit_code}. Exiting.\")\n            exit(exit_code)\n        self.logger.info(\"Finish feature extraction...\")\n\n        ## Create points3D.txt\n        points3D_txt_path = project_folder / 'points3D.txt'\n        open(str(points3D_txt_path), 'w').close()\n\n        ## Create camera.txt\n        camera_txt_path = project_folder / 'cameras.txt'\n        open(str(camera_txt_path), 'w').close()\n\n        unique_cameras = {}\n        camera_id = 1\n        for camera_file in camera_folder.glob('*.json'):\n            camera_data = load_json(camera_file)\n            intrinsics = camera_data['intrinsics']",
    "suffix": ""
  },
  {
    "name": "ddjerqq/beam:src/util.py@661",
    "canonical_solution": "def video_info_to_webhook_payload(author: User, video: Video) -> dict[str, str]:",
    "prompt": "import os\nimport httpx\nfrom src.types.user import User\nfrom src.types.video import Video\n\n\n\n\ndef get_env(key: str, default: str = None) -> str:\n    \"\"\"\n    gets the environment variable with the given key,\n    or raises an exception if the default is not supplied.\n    \"\"\"\n    var = os.getenv(\"APP_ID\", default)\n\n    if var is not None:\n        return var\n\n    raise Exception(f\"Environment variable {key} not found.\")\n\n\ndef humanize(num: int) -> str:\n    \"\"\"\n    converts a number to a human readable format.\n    \"\"\"\n    if num < 1000:\n        return str(num)\n\n    num = num / 1000\n\n    if num < 1000:\n        return f\"{num:.1f}k\"\n\n    num = num / 1000\n\n    if num < 1000:\n        return f\"{num:.1f}m\"\n\n    num = num / 1000\n\n    return f\"{num:.1f}b\"\n\n",
    "prefix": "import os\nimport httpx\nfrom src.types.user import User\nfrom src.types.video import Video\n\n\n\n\ndef get_env(key: str, default: str = None) -> str:\n    \"\"\"\n    gets the environment variable with the given key,\n    or raises an exception if the default is not supplied.\n    \"\"\"\n    var = os.getenv(\"APP_ID\", default)\n\n    if var is not None:\n        return var\n\n    raise Exception(f\"Environment variable {key} not found.\")\n\n\ndef humanize(num: int) -> str:\n    \"\"\"\n    converts a number to a human readable format.\n    \"\"\"\n    if num < 1000:\n        return str(num)\n\n    num = num / 1000\n\n    if num < 1000:\n        return f\"{num:.1f}k\"\n\n    num = num / 1000\n\n    if num < 1000:\n        return f\"{num:.1f}m\"\n\n    num = num / 1000\n\n    return f\"{num:.1f}b\"\n\n",
    "suffix": ""
  },
  {
    "name": "onestepai/api_rag:service.py@1153",
    "canonical_solution": "  config = ServiceApiConfig()",
    "prompt": "import logging\nfrom src.config.ServiceApiConfig import ServiceApiConfig\nfrom src.config.DockerConfig import DockerConfig\nfrom src.api_rag.ModelHandler import ModelHandler\n\n\n\nlogging.getLogger().setLevel(logging.INFO)\nlogging.getLogger('boto3').setLevel(logging.CRITICAL)\nlogging.getLogger('botocore').setLevel(logging.CRITICAL)\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\nif __name__ == '__main__':",
    "prefix": "import logging\nfrom src.config.ServiceApiConfig import ServiceApiConfig\nfrom src.config.DockerConfig import DockerConfig\nfrom src.api_rag.ModelHandler import ModelHandler\n\n\n\nlogging.getLogger().setLevel(logging.INFO)\nlogging.getLogger('boto3').setLevel(logging.CRITICAL)\nlogging.getLogger('botocore').setLevel(logging.CRITICAL)\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\nif __name__ == '__main__':",
    "suffix": ""
  },
  {
    "name": "DerwenAI/textgraphs:textgraphs/graph.py@1287",
    "canonical_solution": "        self.edges: typing.Dict[ str, Edge ] = {}",
    "prompt": "from collections import OrderedDict\nfrom icecream import ic  # pylint: disable=E0401\nfrom .elem import Edge, Node, NodeEnum, RelEnum\nimport json\nimport typing\nimport networkx as nx  # pylint: disable=E0401\nimport spacy  # pylint: disable=E0401\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\nThis class implements a generic, in-memory graph data structure used\nto represent the _lemma graph_.\n\nsee copyright/license https://huggingface.co/spaces/DerwenAI/textgraphs/blob/main/README.md\n\"\"\"\n\n\n\n\n\n######################################################################\n## class definitions\n\nclass SimpleGraph:\n    \"\"\"\nAn in-memory graph used to build a `MultiDiGraph` in NetworkX.\n    \"\"\"\n\n    def __init__ (\n        self\n        ) -> None:\n        \"\"\"\nConstructor.\n        \"\"\"\n        self.nodes: typing.Dict[ str, Node ] = OrderedDict()",
    "prefix": "from collections import OrderedDict\nfrom icecream import ic  # pylint: disable=E0401\nfrom .elem import Edge, Node, NodeEnum, RelEnum\nimport json\nimport typing\nimport networkx as nx  # pylint: disable=E0401\nimport spacy  # pylint: disable=E0401\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\nThis class implements a generic, in-memory graph data structure used\nto represent the _lemma graph_.\n\nsee copyright/license https://huggingface.co/spaces/DerwenAI/textgraphs/blob/main/README.md\n\"\"\"\n\n\n\n\n\n######################################################################\n## class definitions\n\nclass SimpleGraph:\n    \"\"\"\nAn in-memory graph used to build a `MultiDiGraph` in NetworkX.\n    \"\"\"\n\n    def __init__ (\n        self\n        ) -> None:\n        \"\"\"\nConstructor.\n        \"\"\"\n        self.nodes: typing.Dict[ str, Node ] = OrderedDict()",
    "suffix": ""
  },
  {
    "name": "Noubissie237/StockManagment:StockManagment/App/views.py@1475",
    "canonical_solution": "    cookie_panier = panier_cookie(request)",
    "prompt": "from django.shortcuts import render, redirect\nfrom django.http import JsonResponse, HttpResponse\nfrom .models import *\nfrom django.contrib.auth.decorators import login_required\nfrom datetime import datetime\nfrom .utils import panier_cookie, data_cookie, getDataFromApi\nfrom .forms import LoginForm\nfrom django.contrib.auth import authenticate, login, logout\nimport json, requests\n\n\n@login_required(login_url='/login')\ndef shop(request, *args, **kwargs):\n    \"\"\"Vue des produits\"\"\"\n\n    produits = Produit.objects.all()\n\n    data = data_cookie(request)\n    articles = data['articles']\n    commande = data['commande']\n    nombre_article = data['nombre_article']\n\n    context = {\n        'produits': produits,\n        'nombre_article': nombre_article\n    }\n\n\n    return render(request, 'shop/index.html', context)\n\n@login_required(login_url='/login')\ndef panier(request, *args, **kwargs):\n\n    data = data_cookie(request)\n    articles = data['articles']\n    commande = data['commande']\n    nombre_article = data['nombre_article']\n\n\n    context = {\n        'articles' : articles, \n        'commande': commande,\n        'nombre_article': nombre_article\n    }\n\n    return render(request, 'shop/panier.html', context)\n\n@login_required(login_url='/login')\ndef commande(request, *args, **kwargs):\n\n    data = data_cookie(request)\n    articles = data['articles']\n    commande = data['commande']\n    nombre_article = data['nombre_article']\n\n    context = {\n        'articles' : articles, \n        'commande': commande,\n        'nombre_article': nombre_article\n    }\n\n    return render(request, 'shop/commande.html', context)\n\n@login_required(login_url='/login')\ndef update_article(request, *args, **kwargs):\n\n    data = json.loads(request.body)\n    produit_id = data['produit_id']\n    action = data['action']\n    \n    produit = Produit.objects.get(id=produit_id)\n\n    client = request.user.client\n\n    commande, created = Commande.objects.get_or_create(client=client, complete=False)\n\n    commande_article, created = CommandeArticle.objects.get_or_create(commande=commande, produit=produit)\n\n    if action == \"add\":\n        commande_article.quantite += 1\n    \n    if action == \"remove\":\n        commande_article.quantite -=1\n\n    commande_article.save()\n\n    if commande_article.quantite <= 0:\n        commande_article.delete()\n    \n    return JsonResponse(\"panier modifi\u00e9\", safe=False)\n\n@login_required(login_url='/login')\ndef commandeAnonyme(request, data):\n    name = data['form']['name']\n    username = data['form']['username']\n    email = data['form']['email']\n    phone = data['form']['phone']\n",
    "prefix": "from django.shortcuts import render, redirect\nfrom django.http import JsonResponse, HttpResponse\nfrom .models import *\nfrom django.contrib.auth.decorators import login_required\nfrom datetime import datetime\nfrom .utils import panier_cookie, data_cookie, getDataFromApi\nfrom .forms import LoginForm\nfrom django.contrib.auth import authenticate, login, logout\nimport json, requests\n\n\n@login_required(login_url='/login')\ndef shop(request, *args, **kwargs):\n    \"\"\"Vue des produits\"\"\"\n\n    produits = Produit.objects.all()\n\n    data = data_cookie(request)\n    articles = data['articles']\n    commande = data['commande']\n    nombre_article = data['nombre_article']\n\n    context = {\n        'produits': produits,\n        'nombre_article': nombre_article\n    }\n\n\n    return render(request, 'shop/index.html', context)\n\n@login_required(login_url='/login')\ndef panier(request, *args, **kwargs):\n\n    data = data_cookie(request)\n    articles = data['articles']\n    commande = data['commande']\n    nombre_article = data['nombre_article']\n\n\n    context = {\n        'articles' : articles, \n        'commande': commande,\n        'nombre_article': nombre_article\n    }\n\n    return render(request, 'shop/panier.html', context)\n\n@login_required(login_url='/login')\ndef commande(request, *args, **kwargs):\n\n    data = data_cookie(request)\n    articles = data['articles']\n    commande = data['commande']\n    nombre_article = data['nombre_article']\n\n    context = {\n        'articles' : articles, \n        'commande': commande,\n        'nombre_article': nombre_article\n    }\n\n    return render(request, 'shop/commande.html', context)\n\n@login_required(login_url='/login')\ndef update_article(request, *args, **kwargs):\n\n    data = json.loads(request.body)\n    produit_id = data['produit_id']\n    action = data['action']\n    \n    produit = Produit.objects.get(id=produit_id)\n\n    client = request.user.client\n\n    commande, created = Commande.objects.get_or_create(client=client, complete=False)\n\n    commande_article, created = CommandeArticle.objects.get_or_create(commande=commande, produit=produit)\n\n    if action == \"add\":\n        commande_article.quantite += 1\n    \n    if action == \"remove\":\n        commande_article.quantite -=1\n\n    commande_article.save()\n\n    if commande_article.quantite <= 0:\n        commande_article.delete()\n    \n    return JsonResponse(\"panier modifi\u00e9\", safe=False)\n\n@login_required(login_url='/login')\ndef commandeAnonyme(request, data):\n    name = data['form']['name']\n    username = data['form']['username']\n    email = data['form']['email']\n    phone = data['form']['phone']\n",
    "suffix": ""
  },
  {
    "name": "kokiez/raydium-convert-SOLorTokens:main.py@1536",
    "canonical_solution": "        pool_keys = fetch_pool_keys(mint)\r",
    "prompt": "from solana.rpc.commitment import Commitment\r\nfrom solana.rpc.api import Client\r\nfrom solana.transaction import Transaction\r\nfrom solders.keypair import Keypair\r\nfrom pools import  fetch_pool_keys, make_simulate_pool_info_instruction\r\nfrom ast import literal_eval\r\nimport re\r\n\r\n\r\n\r\n\r\n\r\n\r\nLIQUIDITY_FEES_NUMERATOR = 25\r\nLIQUIDITY_FEES_DENOMINATOR = 10000\r\n\r\n\"\"\"\r\nRequired Variables\r\n\"\"\"\r\nendpoint = \"your_rpc_url\"\r\npayer = Keypair.from_base58_string(\"your_private_key\")\r\ntoken = \"ca of your mint/mint address\"\r\nsolana_client = Client(endpoint, commitment=Commitment(\"confirmed\"), blockhash_cache=True)\r\n\r\n\r\ndef calculateAmountOut(amount, pool_info):\r\n    status = pool_info['status']\r\n    SWAP_decimals = pool_info['coin_decimals'] #swap coin\r\n    SOL_decimals = pool_info['pc_decimals'] #SOL\r\n    COIN_lp_decimals = pool_info['lp_decimals'] #swap coin\r\n    pool_SOL_amount = pool_info['pool_pc_amount'] #sol\r\n    pool_SWAP_amount = pool_info['pool_coin_amount'] #coin\r\n    Coin_pool_lp_supply =  pool_info['pool_lp_supply'] #coin\r\n\r\n\r\n    reserve_in = pool_SOL_amount\r\n    reserve_out = pool_SWAP_amount\r\n\r\n    current_price = reserve_out / reserve_in\r\n    # print(f\"Current Price in SOL: {current_price:.12f}\")\r\n\r\n    amount_in = amount * 10 ** SOL_decimals\r\n    Fees = (amount_in * LIQUIDITY_FEES_NUMERATOR)/LIQUIDITY_FEES_DENOMINATOR\r\n    amount_in_with_fee = amount_in - Fees\r\n    amountOutRaw = (reserve_out * amount_in_with_fee) / (reserve_in + amount_in_with_fee)\r\n    # Slippage = 1 + slippage\r\n    # minimumAmountOut = amountOutRaw / slippage\r\n    return amountOutRaw / 10 ** SWAP_decimals\r\n\r\n\r\n\r\ndef calculateAmountIn(amount, pool_info):\r\n    SWAP_decimals = pool_info['coin_decimals'] #swap coin\r\n    SOL_decimals = pool_info['pc_decimals'] #SOL\r\n    COIN_lp_decimals = pool_info['lp_decimals'] #swap coin\r\n    pool_SOL_amount = pool_info['pool_pc_amount'] #sol\r\n    pool_SWAP_amount = pool_info['pool_coin_amount'] #coin\r\n    Coin_pool_lp_supply =  pool_info['pool_lp_supply'] #coin\r\n\r\n\r\n    reserve_in = pool_SWAP_amount\r\n    reserve_out = pool_SOL_amount\r\n\r\n    current_price = reserve_out / reserve_in\r\n    # print(f\"Current Price in SOL: {current_price:.12f}\")\r\n\r\n    amount_in = amount * 10 ** SWAP_decimals\r\n    Fees = (amount_in * LIQUIDITY_FEES_NUMERATOR)/LIQUIDITY_FEES_DENOMINATOR\r\n    amount_in_with_fee = amount_in - Fees\r\n    amountOutRaw = (reserve_out * amount_in_with_fee) / (reserve_in + amount_in_with_fee)\r\n    # Slippage = 1 + slippage\r\n    # minimumAmountOut = amountOutRaw / slippage\r\n    return amountOutRaw / 10 ** SOL_decimals\r\n\r\n\r\n\r\ndef PoolInfo(mint):\r\n    while True:\r\n        quote = \"\"\r",
    "prefix": "from solana.rpc.commitment import Commitment\r\nfrom solana.rpc.api import Client\r\nfrom solana.transaction import Transaction\r\nfrom solders.keypair import Keypair\r\nfrom pools import  fetch_pool_keys, make_simulate_pool_info_instruction\r\nfrom ast import literal_eval\r\nimport re\r\n\r\n\r\n\r\n\r\n\r\n\r\nLIQUIDITY_FEES_NUMERATOR = 25\r\nLIQUIDITY_FEES_DENOMINATOR = 10000\r\n\r\n\"\"\"\r\nRequired Variables\r\n\"\"\"\r\nendpoint = \"your_rpc_url\"\r\npayer = Keypair.from_base58_string(\"your_private_key\")\r\ntoken = \"ca of your mint/mint address\"\r\nsolana_client = Client(endpoint, commitment=Commitment(\"confirmed\"), blockhash_cache=True)\r\n\r\n\r\ndef calculateAmountOut(amount, pool_info):\r\n    status = pool_info['status']\r\n    SWAP_decimals = pool_info['coin_decimals'] #swap coin\r\n    SOL_decimals = pool_info['pc_decimals'] #SOL\r\n    COIN_lp_decimals = pool_info['lp_decimals'] #swap coin\r\n    pool_SOL_amount = pool_info['pool_pc_amount'] #sol\r\n    pool_SWAP_amount = pool_info['pool_coin_amount'] #coin\r\n    Coin_pool_lp_supply =  pool_info['pool_lp_supply'] #coin\r\n\r\n\r\n    reserve_in = pool_SOL_amount\r\n    reserve_out = pool_SWAP_amount\r\n\r\n    current_price = reserve_out / reserve_in\r\n    # print(f\"Current Price in SOL: {current_price:.12f}\")\r\n\r\n    amount_in = amount * 10 ** SOL_decimals\r\n    Fees = (amount_in * LIQUIDITY_FEES_NUMERATOR)/LIQUIDITY_FEES_DENOMINATOR\r\n    amount_in_with_fee = amount_in - Fees\r\n    amountOutRaw = (reserve_out * amount_in_with_fee) / (reserve_in + amount_in_with_fee)\r\n    # Slippage = 1 + slippage\r\n    # minimumAmountOut = amountOutRaw / slippage\r\n    return amountOutRaw / 10 ** SWAP_decimals\r\n\r\n\r\n\r\ndef calculateAmountIn(amount, pool_info):\r\n    SWAP_decimals = pool_info['coin_decimals'] #swap coin\r\n    SOL_decimals = pool_info['pc_decimals'] #SOL\r\n    COIN_lp_decimals = pool_info['lp_decimals'] #swap coin\r\n    pool_SOL_amount = pool_info['pool_pc_amount'] #sol\r\n    pool_SWAP_amount = pool_info['pool_coin_amount'] #coin\r\n    Coin_pool_lp_supply =  pool_info['pool_lp_supply'] #coin\r\n\r\n\r\n    reserve_in = pool_SWAP_amount\r\n    reserve_out = pool_SOL_amount\r\n\r\n    current_price = reserve_out / reserve_in\r\n    # print(f\"Current Price in SOL: {current_price:.12f}\")\r\n\r\n    amount_in = amount * 10 ** SWAP_decimals\r\n    Fees = (amount_in * LIQUIDITY_FEES_NUMERATOR)/LIQUIDITY_FEES_DENOMINATOR\r\n    amount_in_with_fee = amount_in - Fees\r\n    amountOutRaw = (reserve_out * amount_in_with_fee) / (reserve_in + amount_in_with_fee)\r\n    # Slippage = 1 + slippage\r\n    # minimumAmountOut = amountOutRaw / slippage\r\n    return amountOutRaw / 10 ** SOL_decimals\r\n\r\n\r\n\r\ndef PoolInfo(mint):\r\n    while True:\r\n        quote = \"\"\r",
    "suffix": ""
  },
  {
    "name": "proger/nanokitchen:blockdiag_linear.py@1073",
    "canonical_solution": "        output = blockdiag_multiply(x, self.weight)",
    "prompt": "import math\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange\nfrom structured_linear import StructuredLinear\nfrom blockdiag_multiply import blockdiag_multiply\n# Adapted from https://github.com/HazyResearch/fly/tree/master/src/models/layers\n\n\n\n\nclass BlockdiagLinear(StructuredLinear):\n\n    def __init__(self, *args, nblocks=4, shuffle=False, **kwargs):\n        \"\"\"shuffle: apply channel_shuffle operation before the matmul as in ShuffleNet\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        in_blksz = int(math.ceil(self.in_features / nblocks))\n        out_blksz = int(math.ceil(self.out_features / nblocks))\n        self.in_features_extended = in_blksz * nblocks\n        self.out_features_extended = out_blksz * nblocks\n        self.shuffle = shuffle\n        self.weight = nn.Parameter(torch.empty(nblocks, out_blksz, in_blksz))\n        self.reset_parameters()\n\n    def set_weights_from_dense_init(self, dense_init_fn_):\n        dense_weight = torch.empty(self.out_features_extended, self.in_features_extended,\n                                   device=self.weight.device, dtype=self.weight.dtype)\n        dense_init_fn_(dense_weight)\n        # Scale by sqrt because the weight is sparse\n        scaling = math.sqrt(dense_weight.numel() / self.weight.numel())\n        dense_weight *= scaling\n        with torch.no_grad():\n            nblocks = self.weight.shape[0]\n            self.weight.copy_(rearrange(dense_weight, '(b o) (b1 i) -> b b1 o i',\n                                        b=nblocks, b1=nblocks)[0])\n\n    @property\n    def saving(self):\n        return self.weight.numel() / (self.in_features * self.out_features)\n\n    def forward_matmul(self, x):\n        x = self.preprocess(x)\n        if self.shuffle:\n            x = rearrange(x, '... (group c_per_group) -> ... (c_per_group group)',\n                          group=self.weight.shape[0])  # group=nblocks",
    "prefix": "import math\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange\nfrom structured_linear import StructuredLinear\nfrom blockdiag_multiply import blockdiag_multiply\n# Adapted from https://github.com/HazyResearch/fly/tree/master/src/models/layers\n\n\n\n\nclass BlockdiagLinear(StructuredLinear):\n\n    def __init__(self, *args, nblocks=4, shuffle=False, **kwargs):\n        \"\"\"shuffle: apply channel_shuffle operation before the matmul as in ShuffleNet\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        in_blksz = int(math.ceil(self.in_features / nblocks))\n        out_blksz = int(math.ceil(self.out_features / nblocks))\n        self.in_features_extended = in_blksz * nblocks\n        self.out_features_extended = out_blksz * nblocks\n        self.shuffle = shuffle\n        self.weight = nn.Parameter(torch.empty(nblocks, out_blksz, in_blksz))\n        self.reset_parameters()\n\n    def set_weights_from_dense_init(self, dense_init_fn_):\n        dense_weight = torch.empty(self.out_features_extended, self.in_features_extended,\n                                   device=self.weight.device, dtype=self.weight.dtype)\n        dense_init_fn_(dense_weight)\n        # Scale by sqrt because the weight is sparse\n        scaling = math.sqrt(dense_weight.numel() / self.weight.numel())\n        dense_weight *= scaling\n        with torch.no_grad():\n            nblocks = self.weight.shape[0]\n            self.weight.copy_(rearrange(dense_weight, '(b o) (b1 i) -> b b1 o i',\n                                        b=nblocks, b1=nblocks)[0])\n\n    @property\n    def saving(self):\n        return self.weight.numel() / (self.in_features * self.out_features)\n\n    def forward_matmul(self, x):\n        x = self.preprocess(x)\n        if self.shuffle:\n            x = rearrange(x, '... (group c_per_group) -> ... (c_per_group group)',\n                          group=self.weight.shape[0])  # group=nblocks",
    "suffix": ""
  },
  {
    "name": "karloskar/homeassistant-goecontroller-mqtt:custom_components/goecontroller_mqtt/switch.py@776",
    "canonical_solution": "    entity_description: GoEControllerSwitchEntityDescription",
    "prompt": "import logging\nfrom homeassistant import config_entries, core\nfrom homeassistant.components import mqtt\nfrom homeassistant.components.switch import SwitchEntity\nfrom homeassistant.core import callback\nfrom .definitions.switch import SWITCHES, GoEControllerSwitchEntityDescription\nfrom .entity import GoEControllerEntity\n\"\"\"The go-eController (MQTT) switch.\"\"\"\n\n\n\n_LOGGER = logging.getLogger(__name__)\n\n\nasync def async_setup_entry(\n    hass: core.HomeAssistant,\n    config_entry: config_entries.ConfigEntry,\n    async_add_entities,\n):\n    \"\"\"Config entry setup.\"\"\"\n    async_add_entities(\n        GoEControllerSwitch(config_entry, description)\n        for description in SWITCHES\n        if not description.disabled\n    )\n\n\nclass GoEControllerSwitch(GoEControllerEntity, SwitchEntity):\n    \"\"\"Representation of a go-eController switch that is updated via MQTT.\"\"\"\n",
    "prefix": "import logging\nfrom homeassistant import config_entries, core\nfrom homeassistant.components import mqtt\nfrom homeassistant.components.switch import SwitchEntity\nfrom homeassistant.core import callback\nfrom .definitions.switch import SWITCHES, GoEControllerSwitchEntityDescription\nfrom .entity import GoEControllerEntity\n\"\"\"The go-eController (MQTT) switch.\"\"\"\n\n\n\n_LOGGER = logging.getLogger(__name__)\n\n\nasync def async_setup_entry(\n    hass: core.HomeAssistant,\n    config_entry: config_entries.ConfigEntry,\n    async_add_entities,\n):\n    \"\"\"Config entry setup.\"\"\"\n    async_add_entities(\n        GoEControllerSwitch(config_entry, description)\n        for description in SWITCHES\n        if not description.disabled\n    )\n\n\nclass GoEControllerSwitch(GoEControllerEntity, SwitchEntity):\n    \"\"\"Representation of a go-eController switch that is updated via MQTT.\"\"\"\n",
    "suffix": ""
  },
  {
    "name": "T0kyoB0y/PotatoWidgets:PotatoWidgets/Widget/_Common/_BasicProps.py@1380",
    "canonical_solution": "                if isinstance(i, (Listener, Variable, Poll)):",
    "prompt": "from ...__Import import *\nfrom ...Variable import Listener, Poll, Variable\n\n\nclass BasicProps(Gtk.Widget):\n    def __init__(\n        self,\n        halign,\n        valign,\n        hexpand,\n        vexpand,\n        active,\n        visible,\n        classname,\n        # tooltip,\n        css,\n        size=[10, 10],\n    ):\n        Gtk.Widget.__init__(self)\n        self.set_hexpand(True if hexpand else False)\n        self.set_vexpand(True if vexpand else False)\n        self.set_halign(halign)\n        self.set_valign(valign)\n        self.set_visible(visible)\n        self.set_sensitive(active) if active is not None else None\n        self.set_classname(classname)\n        self.__clasif_size(size)\n        self.apply_css(css) if css else None\n\n        for key, value in locals().items():\n            callback = {\n                \"halign\": self.set_halign,\n                \"valign\": self.set_valign,\n                \"hexpand\": self.set_hexpand,\n                \"vexpand\": self.set_vexpand,\n                \"active\": self.set_sensitive,\n                \"visible\": self.set_visible,\n                \"size\": self.set_size,\n                \"classname\": self.set_classname,\n            }.get(key)\n\n            self.bind(value, callback) if callback else None\n\n    def set_size(self, size):\n        self.__clasif_size(size)\n\n    def set_halign(self, param):\n        super().set_halign(self.__clasif_align(str(param)))\n\n    def set_valign(self, param):\n        super().set_valign(self.__clasif_align(str(param)))\n\n    def __clasif_size(self, size):\n        if isinstance(size, int):\n            self.set_size_request(size, size)\n        elif isinstance(size, list):\n            if len(size) == 2:\n                self.set_size_request(size[0], size[1])\n            elif len(size) == 1:\n                self.set_size_request(size[0], size[0])\n\n    def __clasif_align(self, param):\n        dict = {\n            \"fill\": Gtk.Align.FILL,\n            \"start\": Gtk.Align.START,\n            \"end\": Gtk.Align.END,\n            \"center\": Gtk.Align.CENTER,\n            \"baseline\": Gtk.Align.BASELINE,\n        }\n        return dict.get(param.lower(), Gtk.Align.FILL)\n\n    def set_classname(self, param):\n        if isinstance(param, (str)):\n            context = self.get_style_context()\n            [context.add_class(i) for i in param.split(\" \") if i != \" \"]\n        elif isinstance(param, (list)):\n            for i in param:",
    "prefix": "from ...__Import import *\nfrom ...Variable import Listener, Poll, Variable\n\n\nclass BasicProps(Gtk.Widget):\n    def __init__(\n        self,\n        halign,\n        valign,\n        hexpand,\n        vexpand,\n        active,\n        visible,\n        classname,\n        # tooltip,\n        css,\n        size=[10, 10],\n    ):\n        Gtk.Widget.__init__(self)\n        self.set_hexpand(True if hexpand else False)\n        self.set_vexpand(True if vexpand else False)\n        self.set_halign(halign)\n        self.set_valign(valign)\n        self.set_visible(visible)\n        self.set_sensitive(active) if active is not None else None\n        self.set_classname(classname)\n        self.__clasif_size(size)\n        self.apply_css(css) if css else None\n\n        for key, value in locals().items():\n            callback = {\n                \"halign\": self.set_halign,\n                \"valign\": self.set_valign,\n                \"hexpand\": self.set_hexpand,\n                \"vexpand\": self.set_vexpand,\n                \"active\": self.set_sensitive,\n                \"visible\": self.set_visible,\n                \"size\": self.set_size,\n                \"classname\": self.set_classname,\n            }.get(key)\n\n            self.bind(value, callback) if callback else None\n\n    def set_size(self, size):\n        self.__clasif_size(size)\n\n    def set_halign(self, param):\n        super().set_halign(self.__clasif_align(str(param)))\n\n    def set_valign(self, param):\n        super().set_valign(self.__clasif_align(str(param)))\n\n    def __clasif_size(self, size):\n        if isinstance(size, int):\n            self.set_size_request(size, size)\n        elif isinstance(size, list):\n            if len(size) == 2:\n                self.set_size_request(size[0], size[1])\n            elif len(size) == 1:\n                self.set_size_request(size[0], size[0])\n\n    def __clasif_align(self, param):\n        dict = {\n            \"fill\": Gtk.Align.FILL,\n            \"start\": Gtk.Align.START,\n            \"end\": Gtk.Align.END,\n            \"center\": Gtk.Align.CENTER,\n            \"baseline\": Gtk.Align.BASELINE,\n        }\n        return dict.get(param.lower(), Gtk.Align.FILL)\n\n    def set_classname(self, param):\n        if isinstance(param, (str)):\n            context = self.get_style_context()\n            [context.add_class(i) for i in param.split(\" \") if i != \" \"]\n        elif isinstance(param, (list)):\n            for i in param:",
    "suffix": ""
  },
  {
    "name": "Zerohertz/Streamlit-Quant:lib/visual.py@714",
    "canonical_solution": "    colors = _color(4, 0.5, \"Set1\")",
    "prompt": "import plotly.graph_objs as go\nimport streamlit as st\nimport zerohertzLib as zz\nfrom plotly.subplots import make_subplots\nfrom lib.layout import _main, _transaction\nfrom lib.util import _color\n\n\n\ndef candle():\n    data, xdata = st.session_state[\"cache\"][\"data\"], st.session_state[\"cache\"][\"xdata\"]\n    st.session_state[\"cache\"][\"candle\"] = go.Candlestick(\n        x=xdata,\n        open=data.Open,\n        high=data.High,\n        low=data.Low,\n        close=data.Close,\n        increasing={\"line\": {\"color\": \"red\"}},\n        decreasing={\"line\": {\"color\": \"blue\"}},\n        name=st.session_state[\"cache\"][\"name\"],\n    )\n    st.session_state[\"logger\"].info(\n        f\"\"\"[Plot] Candle Chart: {st.session_state[\"cache\"][\"name\"]} ({st.session_state[\"cache\"][\"symbol\"]})\"\"\"\n    )\n\n\ndef moving_average():\n    xdata = st.session_state[\"cache\"][\"xdata\"]\n    st.session_state[\"cache\"][\"ma\"] = []",
    "prefix": "import plotly.graph_objs as go\nimport streamlit as st\nimport zerohertzLib as zz\nfrom plotly.subplots import make_subplots\nfrom lib.layout import _main, _transaction\nfrom lib.util import _color\n\n\n\ndef candle():\n    data, xdata = st.session_state[\"cache\"][\"data\"], st.session_state[\"cache\"][\"xdata\"]\n    st.session_state[\"cache\"][\"candle\"] = go.Candlestick(\n        x=xdata,\n        open=data.Open,\n        high=data.High,\n        low=data.Low,\n        close=data.Close,\n        increasing={\"line\": {\"color\": \"red\"}},\n        decreasing={\"line\": {\"color\": \"blue\"}},\n        name=st.session_state[\"cache\"][\"name\"],\n    )\n    st.session_state[\"logger\"].info(\n        f\"\"\"[Plot] Candle Chart: {st.session_state[\"cache\"][\"name\"]} ({st.session_state[\"cache\"][\"symbol\"]})\"\"\"\n    )\n\n\ndef moving_average():\n    xdata = st.session_state[\"cache\"][\"xdata\"]\n    st.session_state[\"cache\"][\"ma\"] = []",
    "suffix": ""
  },
  {
    "name": "acman/py_june:comments/views.py@779",
    "canonical_solution": "        comment = get_object_or_404(Comment, pk=comment_pk)",
    "prompt": "from django.contrib.auth.mixins import LoginRequiredMixin, UserPassesTestMixin\nfrom django.http import HttpRequest, HttpResponse\nfrom django.shortcuts import get_object_or_404, redirect, render\nfrom django.views import View\nfrom posts.models import Post\nfrom .forms import CommentForm\nfrom .models import Comment\n\n\n\n\nclass CreateCommentView(LoginRequiredMixin, View):\n    template_name = \"comments/comment_form.html\"\n    login_url = \"/users/login/\"\n\n    def get(self, request: HttpRequest, post_slug: str) -> HttpResponse:\n        post = get_object_or_404(Post, slug=post_slug)\n        form = CommentForm()\n        return render(request, self.template_name, {\"form\": form, \"post\": post})\n\n    def post(self, request: HttpRequest, post_slug: str) -> HttpResponse:\n        form = CommentForm(request.POST)\n        post = get_object_or_404(Post, slug=post_slug)\n\n        if form.is_valid():\n            comment = form.save(commit=False)\n            comment.author = self.request.user\n            comment.post_id = post.pk\n            comment.save()\n            return redirect(\"categories:detail\", category_slug=post.category.slug)\n\n        return render(request, self.template_name, {\"form\": form, \"post\": post})\n\n\nclass UpdateCommentView(UserPassesTestMixin, View):\n    template_name = \"comments/comment_update.html\"\n\n    def test_func(self) -> bool:\n        comment_pk = self.kwargs.get(\"comment_pk\")",
    "prefix": "from django.contrib.auth.mixins import LoginRequiredMixin, UserPassesTestMixin\nfrom django.http import HttpRequest, HttpResponse\nfrom django.shortcuts import get_object_or_404, redirect, render\nfrom django.views import View\nfrom posts.models import Post\nfrom .forms import CommentForm\nfrom .models import Comment\n\n\n\n\nclass CreateCommentView(LoginRequiredMixin, View):\n    template_name = \"comments/comment_form.html\"\n    login_url = \"/users/login/\"\n\n    def get(self, request: HttpRequest, post_slug: str) -> HttpResponse:\n        post = get_object_or_404(Post, slug=post_slug)\n        form = CommentForm()\n        return render(request, self.template_name, {\"form\": form, \"post\": post})\n\n    def post(self, request: HttpRequest, post_slug: str) -> HttpResponse:\n        form = CommentForm(request.POST)\n        post = get_object_or_404(Post, slug=post_slug)\n\n        if form.is_valid():\n            comment = form.save(commit=False)\n            comment.author = self.request.user\n            comment.post_id = post.pk\n            comment.save()\n            return redirect(\"categories:detail\", category_slug=post.category.slug)\n\n        return render(request, self.template_name, {\"form\": form, \"post\": post})\n\n\nclass UpdateCommentView(UserPassesTestMixin, View):\n    template_name = \"comments/comment_update.html\"\n\n    def test_func(self) -> bool:\n        comment_pk = self.kwargs.get(\"comment_pk\")",
    "suffix": ""
  },
  {
    "name": "pkariz/grin-explorer:backend/api/signals/receivers.py@1477",
    "canonical_solution": "    sender=Block,",
    "prompt": "from django.db.models.signals import post_save\nfrom django.dispatch import receiver\nfrom backend.api.models import Block, Reorg\nfrom backend.api.helpers import fix_outputs_and_inputs_from_reorg\nimport logging\n\n\n\nlogger = logging.getLogger(__name__)\n\n\n@receiver(\n    post_save,",
    "prefix": "from django.db.models.signals import post_save\nfrom django.dispatch import receiver\nfrom backend.api.models import Block, Reorg\nfrom backend.api.helpers import fix_outputs_and_inputs_from_reorg\nimport logging\n\n\n\nlogger = logging.getLogger(__name__)\n\n\n@receiver(\n    post_save,",
    "suffix": ""
  },
  {
    "name": "CodeWithEmad/num2fa:num2fa/converters/word_converter.py@1135",
    "canonical_solution": "                + _natural_words(numerator)",
    "prompt": "from decimal import Decimal\nfrom fractions import Fraction\nfrom functools import singledispatch\nfrom typing import Union\nfrom num2fa.constants import (\n    DEFAULT_SCIENTIFIC_SEPARATOR,\n    WORDS_DECIMAL_SEPARATOR,\n    WORDS_FRACTION_SEPARATOR,\n    WORDS_NEGATIVE,\n    ZERO,\n)\nfrom num2fa.utils import _natural_words, _normalize_str, _point_words\n\"\"\"Provide functions to convert a number to Persian words.\"\"\"\n\n\n\n\ndef _exp_words(\n    number: str,\n    positive: str,\n    negative: str,\n    decimal_separator: str,\n    scientific_separator: str,\n) -> str:\n    # exponent\n    base, e, exponent = number.partition(\"e\")\n    if exponent:\n        return (\n            _point_words(base, decimal_separator)\n            + scientific_separator\n            + words(int(exponent), positive, negative)\n        )\n    return _point_words(base, decimal_separator)\n\n\n@singledispatch\ndef words(\n    number: Union[int, float, str, Decimal, Fraction],\n    positive: str = \"\",\n    negative: str = WORDS_NEGATIVE,\n    decimal_separator: str = WORDS_DECIMAL_SEPARATOR,\n    fraction_separator: str = WORDS_FRACTION_SEPARATOR,\n    ordinal_denominator: bool = True,\n    scientific_separator: str = DEFAULT_SCIENTIFIC_SEPARATOR,\n) -> str:\n    \"\"\"Return the word form of number.\n\n    If input is a string it should be in the form of a valid Python\n    representation for one of the other accepted types. The only exceptions are\n    that digits can be in Persian, for example words('\u06f4\u06f2') is valid.\n\n    \"\"\"\n    raise TypeError(\"invalid input type for words function\", number)\n\n\n@words.register(str)\n@words.register(Decimal)\ndef _(\n    number: str,\n    positive: str = \"\",\n    negative: str = WORDS_NEGATIVE,\n    decimal_separator: str = WORDS_DECIMAL_SEPARATOR,\n    fraction_separator: str = WORDS_FRACTION_SEPARATOR,\n    ordinal_denominator: bool = True,\n    scientific_separator: str = DEFAULT_SCIENTIFIC_SEPARATOR,\n) -> str:\n    # Normalize the number string\n    number = _normalize_str(number)\n\n    # sign\n    c0 = number[0]\n    if c0 == \"-\":\n        sign = negative\n        number = number[1:]\n    elif c0 == \"0\":\n        sign = \"\"\n    else:\n        sign = positive\n\n    numerator, e, denominator = number.partition(\"/\")\n\n    if denominator:\n        if ordinal_denominator:\n            return (\n                sign",
    "prefix": "from decimal import Decimal\nfrom fractions import Fraction\nfrom functools import singledispatch\nfrom typing import Union\nfrom num2fa.constants import (\n    DEFAULT_SCIENTIFIC_SEPARATOR,\n    WORDS_DECIMAL_SEPARATOR,\n    WORDS_FRACTION_SEPARATOR,\n    WORDS_NEGATIVE,\n    ZERO,\n)\nfrom num2fa.utils import _natural_words, _normalize_str, _point_words\n\"\"\"Provide functions to convert a number to Persian words.\"\"\"\n\n\n\n\ndef _exp_words(\n    number: str,\n    positive: str,\n    negative: str,\n    decimal_separator: str,\n    scientific_separator: str,\n) -> str:\n    # exponent\n    base, e, exponent = number.partition(\"e\")\n    if exponent:\n        return (\n            _point_words(base, decimal_separator)\n            + scientific_separator\n            + words(int(exponent), positive, negative)\n        )\n    return _point_words(base, decimal_separator)\n\n\n@singledispatch\ndef words(\n    number: Union[int, float, str, Decimal, Fraction],\n    positive: str = \"\",\n    negative: str = WORDS_NEGATIVE,\n    decimal_separator: str = WORDS_DECIMAL_SEPARATOR,\n    fraction_separator: str = WORDS_FRACTION_SEPARATOR,\n    ordinal_denominator: bool = True,\n    scientific_separator: str = DEFAULT_SCIENTIFIC_SEPARATOR,\n) -> str:\n    \"\"\"Return the word form of number.\n\n    If input is a string it should be in the form of a valid Python\n    representation for one of the other accepted types. The only exceptions are\n    that digits can be in Persian, for example words('\u06f4\u06f2') is valid.\n\n    \"\"\"\n    raise TypeError(\"invalid input type for words function\", number)\n\n\n@words.register(str)\n@words.register(Decimal)\ndef _(\n    number: str,\n    positive: str = \"\",\n    negative: str = WORDS_NEGATIVE,\n    decimal_separator: str = WORDS_DECIMAL_SEPARATOR,\n    fraction_separator: str = WORDS_FRACTION_SEPARATOR,\n    ordinal_denominator: bool = True,\n    scientific_separator: str = DEFAULT_SCIENTIFIC_SEPARATOR,\n) -> str:\n    # Normalize the number string\n    number = _normalize_str(number)\n\n    # sign\n    c0 = number[0]\n    if c0 == \"-\":\n        sign = negative\n        number = number[1:]\n    elif c0 == \"0\":\n        sign = \"\"\n    else:\n        sign = positive\n\n    numerator, e, denominator = number.partition(\"/\")\n\n    if denominator:\n        if ordinal_denominator:\n            return (\n                sign",
    "suffix": ""
  },
  {
    "name": "the-seeds/cardinal:src/cardinal/core/extractor/base_extractor.py@780",
    "canonical_solution": "            leaf_index = LeafIndex(user_id=user_id)",
    "prompt": "import os\nfrom multiprocessing import Pool\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, List, Optional\nfrom tqdm import tqdm\nfrom ..schema import Extractor, Leaf, LeafIndex\nfrom ..splitter import CJKTextSplitter\n    from ..model import EmbedOpenAI\n    from ..schema import StringKeyedStorage, VectorStore\n    from ..model import EmbedOpenAI\n    from ..storage import RedisStorage\n    from ..vectorstore import Milvus\n\n\n\n\nif TYPE_CHECKING:\n\n\nclass BaseExtractor(Extractor):\n    def __init__(\n        self, vectorizer: \"EmbedOpenAI\", storage: \"StringKeyedStorage[Leaf]\", vectorstore: \"VectorStore[LeafIndex]\"\n    ) -> None:\n        self._vectorizer = vectorizer\n        self._storage = storage\n        self._vectorstore = vectorstore\n        self._splitter = CJKTextSplitter()\n\n    def load(self, input_files: List[Path], user_id: str, verbose: Optional[bool] = False) -> None:\n        file_contents: List[str] = []\n        for file_path in tqdm(input_files, desc=\"Extract content\", disable=(not verbose)):\n            if file_path.suffix == \".txt\":\n                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                    file_contents.append(f.read())\n            else:\n                raise NotImplementedError\n\n        text_chunks = []\n        with Pool(processes=int(os.environ.get(\"NUM_CPU_CORE\"))) as pool:\n            for chunks in tqdm(\n                pool.imap_unordered(self._splitter.split, file_contents),\n                total=len(file_contents),\n                desc=\"Split content\",\n                disable=(not verbose),\n            ):\n                text_chunks.extend(chunks)\n\n        leaf_indexes = []\n        for chunk in tqdm(text_chunks, desc=\"Build index\", disable=(not verbose)):",
    "prefix": "import os\nfrom multiprocessing import Pool\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, List, Optional\nfrom tqdm import tqdm\nfrom ..schema import Extractor, Leaf, LeafIndex\nfrom ..splitter import CJKTextSplitter\n    from ..model import EmbedOpenAI\n    from ..schema import StringKeyedStorage, VectorStore\n    from ..model import EmbedOpenAI\n    from ..storage import RedisStorage\n    from ..vectorstore import Milvus\n\n\n\n\nif TYPE_CHECKING:\n\n\nclass BaseExtractor(Extractor):\n    def __init__(\n        self, vectorizer: \"EmbedOpenAI\", storage: \"StringKeyedStorage[Leaf]\", vectorstore: \"VectorStore[LeafIndex]\"\n    ) -> None:\n        self._vectorizer = vectorizer\n        self._storage = storage\n        self._vectorstore = vectorstore\n        self._splitter = CJKTextSplitter()\n\n    def load(self, input_files: List[Path], user_id: str, verbose: Optional[bool] = False) -> None:\n        file_contents: List[str] = []\n        for file_path in tqdm(input_files, desc=\"Extract content\", disable=(not verbose)):\n            if file_path.suffix == \".txt\":\n                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                    file_contents.append(f.read())\n            else:\n                raise NotImplementedError\n\n        text_chunks = []\n        with Pool(processes=int(os.environ.get(\"NUM_CPU_CORE\"))) as pool:\n            for chunks in tqdm(\n                pool.imap_unordered(self._splitter.split, file_contents),\n                total=len(file_contents),\n                desc=\"Split content\",\n                disable=(not verbose),\n            ):\n                text_chunks.extend(chunks)\n\n        leaf_indexes = []\n        for chunk in tqdm(text_chunks, desc=\"Build index\", disable=(not verbose)):",
    "suffix": ""
  },
  {
    "name": "datrocity/pond:tests/test_conventions.py@744",
    "canonical_solution": "    location = version_data_location('abc/', 'blah.bin')",
    "prompt": "from pond.conventions import (\n    METADATA_DIRNAME,\n    MANIFEST_FILENAME,\n    version_data_location,\n    version_manifest_location,\n    version_uri,\n    urijoinpath,\n)\nfrom pond.version_name import SimpleVersionName\n\n\ndef test_urijoinpath():\n    joined = urijoinpath('a', 'b/', 'c/')\n    expected = 'a/b/c'\n    assert joined == expected\n\n\ndef test_data_location():",
    "prefix": "from pond.conventions import (\n    METADATA_DIRNAME,\n    MANIFEST_FILENAME,\n    version_data_location,\n    version_manifest_location,\n    version_uri,\n    urijoinpath,\n)\nfrom pond.version_name import SimpleVersionName\n\n\ndef test_urijoinpath():\n    joined = urijoinpath('a', 'b/', 'c/')\n    expected = 'a/b/c'\n    assert joined == expected\n\n\ndef test_data_location():",
    "suffix": ""
  },
  {
    "name": "Zitronenjoghurt/Colonaut:src/constants/locale_translator.py@1010",
    "canonical_solution": "    KEYS = Locales",
    "prompt": "from src.utils.file_operations import construct_path, files_in_directory, file_to_dict, str_to_file\nfrom .locales import Locales\n\nLOCALES_FILE_PATH = construct_path(\"src/data/locale/{language}/\")\nOUTPUT_TXT_FILE_PATH = construct_path(\"locale_{language}.txt\")\nLANGUAGES = [\"en\"]\n\nclass LocaleTranslator():\n    _instance = None",
    "prefix": "from src.utils.file_operations import construct_path, files_in_directory, file_to_dict, str_to_file\nfrom .locales import Locales\n\nLOCALES_FILE_PATH = construct_path(\"src/data/locale/{language}/\")\nOUTPUT_TXT_FILE_PATH = construct_path(\"locale_{language}.txt\")\nLANGUAGES = [\"en\"]\n\nclass LocaleTranslator():\n    _instance = None",
    "suffix": ""
  },
  {
    "name": "daojiAnime/aio_retrying:tests/test_condition_error.py@745",
    "canonical_solution": "    with pytest.raises(ConditionError):",
    "prompt": "import asyncio\nimport pytest\nfrom aio_retrying import ConditionError, retry\n\n\n\n\nasync def test_timeout_is_not_none_and_not_async():\n    @retry(timeout=0.5)\n    def not_coro():\n        pass\n",
    "prefix": "import asyncio\nimport pytest\nfrom aio_retrying import ConditionError, retry\n\n\n\n\nasync def test_timeout_is_not_none_and_not_async():\n    @retry(timeout=0.5)\n    def not_coro():\n        pass\n",
    "suffix": ""
  },
  {
    "name": "xIMRANx/secret_postcard:app/handlers/user/file.py@1167",
    "canonical_solution": "    if not await User.is_registered(user_id):",
    "prompt": "from aiogram import Router, Bot, F\nfrom aiogram.types import Message\nfrom app.db.functions import User\nfrom app.db.functions import Card\nfrom app.keyboards.inline import get_approve_keyboard\nfrom app.config import Config\n\n\nrouter = Router()\n\n\n@router.message(F.content_type.in_({\"photo\", \"video\", \"animation\"}))\nasync def get_postcard(message: Message, bot: Bot, config: Config):\n    if await Card.check_exists(message.from_user.id):\n        await message.answer(\"\u0412\u044b \u0443\u0436\u0435 \u043e\u0442\u043f\u0440\u0430\u0432\u0438\u043b\u0438 \u0441\u0432\u043e\u044e \u043e\u0442\u043a\u0440\u044b\u0442\u043a\u0443!\")\n        return\n\n    postcard_type = message.content_type\n    if message.photo is not None:\n        file_id = message.photo[-1].file_id\n    elif message.video is not None:\n        file_id = message.video.file_id\n    elif message.animation is not None:\n        file_id = message.animation.file_id\n    else:\n        file_id = None\n\n    user_id = message.from_user.id\n    chat_id = config.settings.chat_id",
    "prefix": "from aiogram import Router, Bot, F\nfrom aiogram.types import Message\nfrom app.db.functions import User\nfrom app.db.functions import Card\nfrom app.keyboards.inline import get_approve_keyboard\nfrom app.config import Config\n\n\nrouter = Router()\n\n\n@router.message(F.content_type.in_({\"photo\", \"video\", \"animation\"}))\nasync def get_postcard(message: Message, bot: Bot, config: Config):\n    if await Card.check_exists(message.from_user.id):\n        await message.answer(\"\u0412\u044b \u0443\u0436\u0435 \u043e\u0442\u043f\u0440\u0430\u0432\u0438\u043b\u0438 \u0441\u0432\u043e\u044e \u043e\u0442\u043a\u0440\u044b\u0442\u043a\u0443!\")\n        return\n\n    postcard_type = message.content_type\n    if message.photo is not None:\n        file_id = message.photo[-1].file_id\n    elif message.video is not None:\n        file_id = message.video.file_id\n    elif message.animation is not None:\n        file_id = message.animation.file_id\n    else:\n        file_id = None\n\n    user_id = message.from_user.id\n    chat_id = config.settings.chat_id",
    "suffix": ""
  },
  {
    "name": "akkoaya/ArticleSpider:ArticleSpider/spiders/cnblog.py@1196",
    "canonical_solution": "        item_loader.add_value(\"url_object_id\", get_md5(response.url))",
    "prompt": "import scrapy\nimport datetime\nimport re\nfrom scrapy.http import Request\nfrom urllib import parse\nfrom ..items import CnblogItem\nfrom ..utils.common import get_md5\nfrom scrapy.loader import ItemLoader\nfrom scrapy_redis.spiders import RedisSpider\n\n\nclass CnblogSpider(scrapy.Spider):\n    name = \"cnblog\"\n    allowed_domains = [\"www.cnblogs.com\"]\n    start_urls = [\"https://www.cnblogs.com/sitehome/p/1\"]\n    # redis_key = 'cnblog:start_urls'\n\n    next_url = \"https://www.cnblogs.com/sitehome/p/{0}\"\n    # headers = {\n    #     \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n    # }\n\n    def parse(self, response):\n\n        all_urls = response.css('div.post-list a::attr(href)').extract()\n        all_urls = [parse.urljoin(response.url, url) for url in all_urls]\n\n        for url in all_urls:\n            match_obj = re.match('(.*.cnblogs.com/(.*)/p/.*.html)',url)\n            if match_obj:\n                request_url = match_obj.group(1)\n                writer_id = match_obj.group(2)\n                yield Request(url=request_url,meta={'writer_id':writer_id},callback=self.parse_detail)\n\n        for x in range(2,100):\n            yield Request(url=self.next_url.format(x), callback=self.parse)\n\n    def parse_detail(self,response):\n        item_loader = ItemLoader(item=CnblogItem(), response=response)\n\n        item_loader.add_value(\"url\", response.url)",
    "prefix": "import scrapy\nimport datetime\nimport re\nfrom scrapy.http import Request\nfrom urllib import parse\nfrom ..items import CnblogItem\nfrom ..utils.common import get_md5\nfrom scrapy.loader import ItemLoader\nfrom scrapy_redis.spiders import RedisSpider\n\n\nclass CnblogSpider(scrapy.Spider):\n    name = \"cnblog\"\n    allowed_domains = [\"www.cnblogs.com\"]\n    start_urls = [\"https://www.cnblogs.com/sitehome/p/1\"]\n    # redis_key = 'cnblog:start_urls'\n\n    next_url = \"https://www.cnblogs.com/sitehome/p/{0}\"\n    # headers = {\n    #     \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n    # }\n\n    def parse(self, response):\n\n        all_urls = response.css('div.post-list a::attr(href)').extract()\n        all_urls = [parse.urljoin(response.url, url) for url in all_urls]\n\n        for url in all_urls:\n            match_obj = re.match('(.*.cnblogs.com/(.*)/p/.*.html)',url)\n            if match_obj:\n                request_url = match_obj.group(1)\n                writer_id = match_obj.group(2)\n                yield Request(url=request_url,meta={'writer_id':writer_id},callback=self.parse_detail)\n\n        for x in range(2,100):\n            yield Request(url=self.next_url.format(x), callback=self.parse)\n\n    def parse_detail(self,response):\n        item_loader = ItemLoader(item=CnblogItem(), response=response)\n\n        item_loader.add_value(\"url\", response.url)",
    "suffix": ""
  },
  {
    "name": "Asa-Nisi-Masa/christmas-tree:christmas_tree/calculations/compute_coords.py@1251",
    "canonical_solution": "    with open(PATH_SAVE, \"w\") as file:",
    "prompt": "from collections import defaultdict, namedtuple\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\nfrom tqdm import tqdm\nfrom christmas_tree.common.settings import PATH_SAVE, TOTAL_LEDS\nimport cv2\nimport numpy as np\n\n\n\n### Adjust these three parameters if lots of LEDs cannot be detected\nLOWER_THRESHOLD = 135\nUPPER_THRESHOLD = 255\nMAX_DIST = 40\n###\nANGLES = [0, 45, 90, 135, 180, 225, 270, 315]\n\nPoint = namedtuple(\"Point\", [\"x\", \"y\"])\n\n# get height and width of images from one of the frames\npath = Path(\"frames\") / str(ANGLES[0]) / \"0.jpg\"\nframe = cv2.imread(str(path))\nheight, width, _ = frame.shape\n\n\ndef _get_uv(center: Point, width: int, height: int) -> Point:\n    px, py = center\n\n    u = 2 / width * px - 1\n    v = -2 / height * py + 1\n\n    return Point(u, v)\n\n\ndef _compute_naive_positions(image: np.ndarray) -> List[Point]:\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    _, thresh = cv2.threshold(gray, LOWER_THRESHOLD, UPPER_THRESHOLD, cv2.THRESH_BINARY)\n    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    centers = []\n    for contour in contours:\n        M = cv2.moments(contour)\n        if M[\"m00\"] != 0:\n            cX = int(M[\"m10\"] / M[\"m00\"])\n            cY = int(M[\"m01\"] / M[\"m00\"])\n\n            centers.append(Point(cX, cY))\n\n    return centers\n\n\ndef _compute_correct_positions(contour_centers: List[Point]) -> Optional[Point]:\n    if len(contour_centers) == 0:\n        return None\n\n    if len(contour_centers) == 1:\n        return contour_centers[0]\n\n    min_dist = float(\"inf\")\n    for i in range(len(contour_centers)):\n        for j in range(i, len(contour_centers)):\n            if i == j:\n                continue\n\n            xi, yi = contour_centers[i]\n            xj, yj = contour_centers[j]\n\n            dist2 = (xi - xj) ** 2 + (yi - yj) ** 2\n\n            if dist2 < min_dist:\n                min_dist = dist2\n\n    if min_dist < MAX_DIST**2:\n        centers = np.array(contour_centers).mean(axis=0)\n        return Point(int(centers[0]), int(centers[1]))\n\n    return None\n\n\ndef _get_map_from_index_to_position(angle: int) -> Dict[int, Point]:\n    map_index_to_position = {}\n\n    total_errors = 0\n    for i in range(TOTAL_LEDS):\n        path = Path(\"frames\") / str(angle) / f\"{i}.jpg\"\n        frame = cv2.imread(str(path))\n        contour_centers = _compute_naive_positions(frame)\n\n        center = _compute_correct_positions(contour_centers)\n        if center is None:\n            total_errors += 1\n            map_index_to_position[i] = None\n        else:\n            map_index_to_position[i] = _get_uv(center, width, height)\n\n    return map_index_to_position\n\n\ndef get_map_index_to_angle_position() -> Dict[int, Dict[int, Point]]:\n    # map_index_to_angle_position = map from LED index to a map from angle to LED position\n    angles_to_centers = {}\n    map_index_to_angle_position = defaultdict(dict)\n\n    for angle in tqdm(ANGLES):\n        map_index_to_position = _get_map_from_index_to_position(angle)\n        angles_to_centers[angle] = map_index_to_position\n\n        for i in range(TOTAL_LEDS):\n            map_index_to_angle_position[i][angle] = map_index_to_position[i]\n\n    return map_index_to_angle_position\n\n\ndef validate_led_positions(map_index_to_angle_position: Dict[int, Dict[int, Point]]) -> None:\n    total_no_centers = 0\n    for i in range(TOTAL_LEDS):\n        num_angles_center_is_defined = sum(el is not None for el in map_index_to_angle_position[i].values())\n\n        if num_angles_center_is_defined < 1:\n            print(f\"No center can be found for {i} LED\")\n            total_no_centers += 1\n\n    print(\"Total no LED positions found:\", total_no_centers)\n\n\ndef get_frames_to_xyz(map_index_to_angle_position: Dict[int, Dict[int, Point]]) -> Dict[int, tuple]:\n    # frames_to_xyz = map from LED index to LED position\n    frames_to_xyz = {}\n    for i in range(TOTAL_LEDS):\n        sum_x = 0\n        sum_z = 0\n        sum_y = 0\n\n        non_nulls = 0\n        for angle in ANGLES:\n            radian = np.pi / 180 * angle\n            center = map_index_to_angle_position[i][angle]\n            if center is not None:\n                sum_x += center.x * np.cos(radian)\n                sum_z += center.x * np.sin(radian)\n                sum_y += center.y\n\n                non_nulls += 1\n\n        if non_nulls > 0:\n            x = 1 / non_nulls * sum_x\n            z = 1 / non_nulls * sum_z\n            y = 1 / non_nulls * sum_y\n\n            frames_to_xyz[i] = (x, y, z)\n        else:\n            frames_to_xyz[i] = None\n\n    return frames_to_xyz\n\n\ndef save_to_file(frames_to_xyz: Dict[int, tuple]):",
    "prefix": "from collections import defaultdict, namedtuple\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\nfrom tqdm import tqdm\nfrom christmas_tree.common.settings import PATH_SAVE, TOTAL_LEDS\nimport cv2\nimport numpy as np\n\n\n\n### Adjust these three parameters if lots of LEDs cannot be detected\nLOWER_THRESHOLD = 135\nUPPER_THRESHOLD = 255\nMAX_DIST = 40\n###\nANGLES = [0, 45, 90, 135, 180, 225, 270, 315]\n\nPoint = namedtuple(\"Point\", [\"x\", \"y\"])\n\n# get height and width of images from one of the frames\npath = Path(\"frames\") / str(ANGLES[0]) / \"0.jpg\"\nframe = cv2.imread(str(path))\nheight, width, _ = frame.shape\n\n\ndef _get_uv(center: Point, width: int, height: int) -> Point:\n    px, py = center\n\n    u = 2 / width * px - 1\n    v = -2 / height * py + 1\n\n    return Point(u, v)\n\n\ndef _compute_naive_positions(image: np.ndarray) -> List[Point]:\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    _, thresh = cv2.threshold(gray, LOWER_THRESHOLD, UPPER_THRESHOLD, cv2.THRESH_BINARY)\n    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    centers = []\n    for contour in contours:\n        M = cv2.moments(contour)\n        if M[\"m00\"] != 0:\n            cX = int(M[\"m10\"] / M[\"m00\"])\n            cY = int(M[\"m01\"] / M[\"m00\"])\n\n            centers.append(Point(cX, cY))\n\n    return centers\n\n\ndef _compute_correct_positions(contour_centers: List[Point]) -> Optional[Point]:\n    if len(contour_centers) == 0:\n        return None\n\n    if len(contour_centers) == 1:\n        return contour_centers[0]\n\n    min_dist = float(\"inf\")\n    for i in range(len(contour_centers)):\n        for j in range(i, len(contour_centers)):\n            if i == j:\n                continue\n\n            xi, yi = contour_centers[i]\n            xj, yj = contour_centers[j]\n\n            dist2 = (xi - xj) ** 2 + (yi - yj) ** 2\n\n            if dist2 < min_dist:\n                min_dist = dist2\n\n    if min_dist < MAX_DIST**2:\n        centers = np.array(contour_centers).mean(axis=0)\n        return Point(int(centers[0]), int(centers[1]))\n\n    return None\n\n\ndef _get_map_from_index_to_position(angle: int) -> Dict[int, Point]:\n    map_index_to_position = {}\n\n    total_errors = 0\n    for i in range(TOTAL_LEDS):\n        path = Path(\"frames\") / str(angle) / f\"{i}.jpg\"\n        frame = cv2.imread(str(path))\n        contour_centers = _compute_naive_positions(frame)\n\n        center = _compute_correct_positions(contour_centers)\n        if center is None:\n            total_errors += 1\n            map_index_to_position[i] = None\n        else:\n            map_index_to_position[i] = _get_uv(center, width, height)\n\n    return map_index_to_position\n\n\ndef get_map_index_to_angle_position() -> Dict[int, Dict[int, Point]]:\n    # map_index_to_angle_position = map from LED index to a map from angle to LED position\n    angles_to_centers = {}\n    map_index_to_angle_position = defaultdict(dict)\n\n    for angle in tqdm(ANGLES):\n        map_index_to_position = _get_map_from_index_to_position(angle)\n        angles_to_centers[angle] = map_index_to_position\n\n        for i in range(TOTAL_LEDS):\n            map_index_to_angle_position[i][angle] = map_index_to_position[i]\n\n    return map_index_to_angle_position\n\n\ndef validate_led_positions(map_index_to_angle_position: Dict[int, Dict[int, Point]]) -> None:\n    total_no_centers = 0\n    for i in range(TOTAL_LEDS):\n        num_angles_center_is_defined = sum(el is not None for el in map_index_to_angle_position[i].values())\n\n        if num_angles_center_is_defined < 1:\n            print(f\"No center can be found for {i} LED\")\n            total_no_centers += 1\n\n    print(\"Total no LED positions found:\", total_no_centers)\n\n\ndef get_frames_to_xyz(map_index_to_angle_position: Dict[int, Dict[int, Point]]) -> Dict[int, tuple]:\n    # frames_to_xyz = map from LED index to LED position\n    frames_to_xyz = {}\n    for i in range(TOTAL_LEDS):\n        sum_x = 0\n        sum_z = 0\n        sum_y = 0\n\n        non_nulls = 0\n        for angle in ANGLES:\n            radian = np.pi / 180 * angle\n            center = map_index_to_angle_position[i][angle]\n            if center is not None:\n                sum_x += center.x * np.cos(radian)\n                sum_z += center.x * np.sin(radian)\n                sum_y += center.y\n\n                non_nulls += 1\n\n        if non_nulls > 0:\n            x = 1 / non_nulls * sum_x\n            z = 1 / non_nulls * sum_z\n            y = 1 / non_nulls * sum_y\n\n            frames_to_xyz[i] = (x, y, z)\n        else:\n            frames_to_xyz[i] = None\n\n    return frames_to_xyz\n\n\ndef save_to_file(frames_to_xyz: Dict[int, tuple]):",
    "suffix": ""
  },
  {
    "name": "YYJeffrey/july_server:app/api/v2/message.py@1304",
    "canonical_solution": "    return Updated()",
    "prompt": "from flask import g\nfrom app import auth, db\nfrom app.lib.exception import Success, Updated\nfrom app.lib.red_print import RedPrint\nfrom app.model.message import Message\nfrom app.service.message import get_message_list\n# -*- coding: utf-8 -*-\n\"\"\"\n    :copyright: (c) 2023 by Jeffrey.\n    :license: Apache 2.0, see LICENSE for more details.\n\"\"\"\n\n\napi = RedPrint('message')\n\n\n@api.route('/', methods=['GET'])\n@auth.login_required\ndef get_messages():\n    \"\"\"\n    \u83b7\u53d6\u6d88\u606f\n    \"\"\"\n    messages = get_message_list()\n    return Success(data=messages)\n\n\n@api.route('/read', methods=['POST'])\n@auth.login_required\ndef read_messages():\n    \"\"\"\n    \u5df2\u8bfb\u4fe1\u606f\n    \"\"\"\n    with db.auto_commit():\n        db.session.query(Message).filter_by(user_id=g.user.id, is_read=False).update({Message.is_read: True})\n",
    "prefix": "from flask import g\nfrom app import auth, db\nfrom app.lib.exception import Success, Updated\nfrom app.lib.red_print import RedPrint\nfrom app.model.message import Message\nfrom app.service.message import get_message_list\n# -*- coding: utf-8 -*-\n\"\"\"\n    :copyright: (c) 2023 by Jeffrey.\n    :license: Apache 2.0, see LICENSE for more details.\n\"\"\"\n\n\napi = RedPrint('message')\n\n\n@api.route('/', methods=['GET'])\n@auth.login_required\ndef get_messages():\n    \"\"\"\n    \u83b7\u53d6\u6d88\u606f\n    \"\"\"\n    messages = get_message_list()\n    return Success(data=messages)\n\n\n@api.route('/read', methods=['POST'])\n@auth.login_required\ndef read_messages():\n    \"\"\"\n    \u5df2\u8bfb\u4fe1\u606f\n    \"\"\"\n    with db.auto_commit():\n        db.session.query(Message).filter_by(user_id=g.user.id, is_read=False).update({Message.is_read: True})\n",
    "suffix": ""
  },
  {
    "name": "lchen1019/Image_Cropper:ISAT/widgets/polygon.py@1085",
    "canonical_solution": "        if self.scene().mode == STATUSMode.CREATE: # CREATE",
    "prompt": "from PyQt5 import QtCore, QtWidgets, QtGui\nfrom ISAT.annotation import Object\nfrom ISAT.configs import STATUSMode, CLICKMode, DRAWMode, CONTOURMode\nimport typing\n# -*- coding: utf-8 -*-\n# @Author  : LG\n\n\n\nclass PromptPoint(QtWidgets.QGraphicsPathItem):\n    def __init__(self, pos, type=0):\n        super(PromptPoint, self).__init__()\n        self.color = QtGui.QColor('#0000FF') if type==0 else QtGui.QColor('#00FF00')\n        self.color.setAlpha(255)\n        self.painterpath = QtGui.QPainterPath()\n        self.painterpath.addEllipse(\n            QtCore.QRectF(-1, -1, 2, 2))\n        self.setPath(self.painterpath)\n        self.setBrush(self.color)\n        self.setPen(QtGui.QPen(self.color, 3))\n        self.setZValue(1e5)\n\n        self.setPos(pos)\n\n\nclass Vertex(QtWidgets.QGraphicsPathItem):\n    def __init__(self, polygon, color, nohover_size=2):\n        super(Vertex, self).__init__()\n        self.polygon = polygon\n        self.color = color\n        self.color.setAlpha(255)\n        self.nohover_size = nohover_size\n        self.hover_size = self.nohover_size + 2\n        self.line_width = 0\n\n        self.nohover = QtGui.QPainterPath()\n        self.nohover.addEllipse(QtCore.QRectF(-self.nohover_size//2, -self.nohover_size//2, self.nohover_size, self.nohover_size))\n        self.hover = QtGui.QPainterPath()\n        self.hover.addRect(QtCore.QRectF(-self.nohover_size//2, -self.nohover_size//2, self.nohover_size, self.nohover_size))\n\n        self.setPath(self.nohover)\n        self.setBrush(self.color)\n        self.setPen(QtGui.QPen(self.color, self.line_width))\n        self.setFlag(QtWidgets.QGraphicsItem.GraphicsItemFlag.ItemIsSelectable, True)\n        self.setFlag(QtWidgets.QGraphicsItem.GraphicsItemFlag.ItemIsMovable, True)\n        self.setFlag(QtWidgets.QGraphicsItem.GraphicsItemFlag.ItemSendsGeometryChanges, True)\n        self.setAcceptHoverEvents(True)\n        self.setZValue(1e5)\n\n    def setColor(self, color):\n        self.color = QtGui.QColor(color)\n        self.color.setAlpha(255)\n        self.setPen(QtGui.QPen(self.color, self.line_width))\n        self.setBrush(self.color)\n\n    def itemChange(self, change: 'QtWidgets.QGraphicsItem.GraphicsItemChange', value: typing.Any):\n        if change == QtWidgets.QGraphicsItem.GraphicsItemChange.ItemSelectedHasChanged:\n            self.scene().mainwindow.actionDelete.setEnabled(self.isSelected())\n            if self.isSelected():\n                selected_color = QtGui.QColor('#00A0FF')\n                self.setBrush(selected_color)\n            else:\n                self.setBrush(self.color)\n\n        if change == QtWidgets.QGraphicsItem.GraphicsItemChange.ItemPositionChange and self.isEnabled():\n            # \u9650\u5236\u9876\u70b9\u79fb\u52a8\u5230\u56fe\u5916\n            if value.x() < 0:\n                value.setX(0)\n            if value.x() > self.scene().width()-1:\n                value.setX(self.scene().width()-1)\n            if value.y() < 0:\n                value.setY(0)\n            if value.y() > self.scene().height()-1:\n                value.setY(self.scene().height()-1)\n            index = self.polygon.vertexs.index(self)\n            self.polygon.movePoint(index, value)\n\n        return super(Vertex, self).itemChange(change, value)\n    \n    def hoverEnterEvent(self, event: 'QGraphicsSceneHoverEvent'):",
    "prefix": "from PyQt5 import QtCore, QtWidgets, QtGui\nfrom ISAT.annotation import Object\nfrom ISAT.configs import STATUSMode, CLICKMode, DRAWMode, CONTOURMode\nimport typing\n# -*- coding: utf-8 -*-\n# @Author  : LG\n\n\n\nclass PromptPoint(QtWidgets.QGraphicsPathItem):\n    def __init__(self, pos, type=0):\n        super(PromptPoint, self).__init__()\n        self.color = QtGui.QColor('#0000FF') if type==0 else QtGui.QColor('#00FF00')\n        self.color.setAlpha(255)\n        self.painterpath = QtGui.QPainterPath()\n        self.painterpath.addEllipse(\n            QtCore.QRectF(-1, -1, 2, 2))\n        self.setPath(self.painterpath)\n        self.setBrush(self.color)\n        self.setPen(QtGui.QPen(self.color, 3))\n        self.setZValue(1e5)\n\n        self.setPos(pos)\n\n\nclass Vertex(QtWidgets.QGraphicsPathItem):\n    def __init__(self, polygon, color, nohover_size=2):\n        super(Vertex, self).__init__()\n        self.polygon = polygon\n        self.color = color\n        self.color.setAlpha(255)\n        self.nohover_size = nohover_size\n        self.hover_size = self.nohover_size + 2\n        self.line_width = 0\n\n        self.nohover = QtGui.QPainterPath()\n        self.nohover.addEllipse(QtCore.QRectF(-self.nohover_size//2, -self.nohover_size//2, self.nohover_size, self.nohover_size))\n        self.hover = QtGui.QPainterPath()\n        self.hover.addRect(QtCore.QRectF(-self.nohover_size//2, -self.nohover_size//2, self.nohover_size, self.nohover_size))\n\n        self.setPath(self.nohover)\n        self.setBrush(self.color)\n        self.setPen(QtGui.QPen(self.color, self.line_width))\n        self.setFlag(QtWidgets.QGraphicsItem.GraphicsItemFlag.ItemIsSelectable, True)\n        self.setFlag(QtWidgets.QGraphicsItem.GraphicsItemFlag.ItemIsMovable, True)\n        self.setFlag(QtWidgets.QGraphicsItem.GraphicsItemFlag.ItemSendsGeometryChanges, True)\n        self.setAcceptHoverEvents(True)\n        self.setZValue(1e5)\n\n    def setColor(self, color):\n        self.color = QtGui.QColor(color)\n        self.color.setAlpha(255)\n        self.setPen(QtGui.QPen(self.color, self.line_width))\n        self.setBrush(self.color)\n\n    def itemChange(self, change: 'QtWidgets.QGraphicsItem.GraphicsItemChange', value: typing.Any):\n        if change == QtWidgets.QGraphicsItem.GraphicsItemChange.ItemSelectedHasChanged:\n            self.scene().mainwindow.actionDelete.setEnabled(self.isSelected())\n            if self.isSelected():\n                selected_color = QtGui.QColor('#00A0FF')\n                self.setBrush(selected_color)\n            else:\n                self.setBrush(self.color)\n\n        if change == QtWidgets.QGraphicsItem.GraphicsItemChange.ItemPositionChange and self.isEnabled():\n            # \u9650\u5236\u9876\u70b9\u79fb\u52a8\u5230\u56fe\u5916\n            if value.x() < 0:\n                value.setX(0)\n            if value.x() > self.scene().width()-1:\n                value.setX(self.scene().width()-1)\n            if value.y() < 0:\n                value.setY(0)\n            if value.y() > self.scene().height()-1:\n                value.setY(self.scene().height()-1)\n            index = self.polygon.vertexs.index(self)\n            self.polygon.movePoint(index, value)\n\n        return super(Vertex, self).itemChange(change, value)\n    \n    def hoverEnterEvent(self, event: 'QGraphicsSceneHoverEvent'):",
    "suffix": ""
  },
  {
    "name": "aoki-h-jp/crypto-listed-detector:crypto_listed_detector/detector.py@1437",
    "canonical_solution": "        self.bybit = BybitFetch()",
    "prompt": "import json\nfrom crypto_listed_detector.fetchapi.binance import BinanceFetch\nfrom crypto_listed_detector.fetchapi.bitget import BitgetFetch\nfrom crypto_listed_detector.fetchapi.bybit import BybitFetch\nfrom crypto_listed_detector.fetchapi.gateio import GateioFetch\nfrom crypto_listed_detector.fetchapi.kucoin import KucoinFetch\nfrom crypto_listed_detector.fetchapi.mexc import MexcFetch\nfrom crypto_listed_detector.fetchapi.okx import OkxFetch\nfrom crypto_listed_detector.fetchapi.phemex import PhemexFetch\nfrom crypto_listed_detector.fetchapi.pionex import PionexFetch\nfrom crypto_listed_detector.fetchapi.xtcom import XtcomFetch\n\"\"\"\ncrypto-listed-detector\n\"\"\"\n\n\n\nclass Detector:\n    def __init__(self):\n        \"\"\"\n        Init all fetchers\n        \"\"\"",
    "prefix": "import json\nfrom crypto_listed_detector.fetchapi.binance import BinanceFetch\nfrom crypto_listed_detector.fetchapi.bitget import BitgetFetch\nfrom crypto_listed_detector.fetchapi.bybit import BybitFetch\nfrom crypto_listed_detector.fetchapi.gateio import GateioFetch\nfrom crypto_listed_detector.fetchapi.kucoin import KucoinFetch\nfrom crypto_listed_detector.fetchapi.mexc import MexcFetch\nfrom crypto_listed_detector.fetchapi.okx import OkxFetch\nfrom crypto_listed_detector.fetchapi.phemex import PhemexFetch\nfrom crypto_listed_detector.fetchapi.pionex import PionexFetch\nfrom crypto_listed_detector.fetchapi.xtcom import XtcomFetch\n\"\"\"\ncrypto-listed-detector\n\"\"\"\n\n\n\nclass Detector:\n    def __init__(self):\n        \"\"\"\n        Init all fetchers\n        \"\"\"",
    "suffix": ""
  },
  {
    "name": "harvestingmoon/StableVisionBot:bot.py@1161",
    "canonical_solution": "engine = BackEnd(model)",
    "prompt": "from telegram import ReplyKeyboardMarkup, ReplyKeyboardRemove, Update,InlineKeyboardButton,InlineKeyboardMarkup\nfrom telegram.ext import (\n    Application,\n    CommandHandler,\n    ContextTypes,\n    ConversationHandler,\n    MessageHandler,\n    CallbackQueryHandler,\n    filters,\n    CallbackContext,\n)\nfrom backend import BackEnd,post_process\nfrom PIL import Image\nimport numpy as np \nimport json \nimport logging\nimport yaml\nimport emoji\nimport asyncio\n# Simple telegram bot that takes uses stable diffusion\n''' Importing YAML'''\nwith open(\"config .yaml\", \"r\") as f:\n      config = yaml.safe_load(f)\n\nmodel = config['model']\napi_key = config['API_KEY']\n\n''' States for bot'''\nONE,TWO,DOCUMENT,PHOTO = range(4)\nSTART,T2IMG,T2IMG2,IMG2IMG,IMG2IMG2,OUTPUT= range(6)\n\n''' User logging'''\nlogging.basicConfig(\n     format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s', level = logging.INFO\n)\n\nlogger = logging.getLogger(__name__)\n\n''' Important pipeline for stable diffusion'''       ",
    "prefix": "from telegram import ReplyKeyboardMarkup, ReplyKeyboardRemove, Update,InlineKeyboardButton,InlineKeyboardMarkup\nfrom telegram.ext import (\n    Application,\n    CommandHandler,\n    ContextTypes,\n    ConversationHandler,\n    MessageHandler,\n    CallbackQueryHandler,\n    filters,\n    CallbackContext,\n)\nfrom backend import BackEnd,post_process\nfrom PIL import Image\nimport numpy as np \nimport json \nimport logging\nimport yaml\nimport emoji\nimport asyncio\n# Simple telegram bot that takes uses stable diffusion\n''' Importing YAML'''\nwith open(\"config .yaml\", \"r\") as f:\n      config = yaml.safe_load(f)\n\nmodel = config['model']\napi_key = config['API_KEY']\n\n''' States for bot'''\nONE,TWO,DOCUMENT,PHOTO = range(4)\nSTART,T2IMG,T2IMG2,IMG2IMG,IMG2IMG2,OUTPUT= range(6)\n\n''' User logging'''\nlogging.basicConfig(\n     format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s', level = logging.INFO\n)\n\nlogger = logging.getLogger(__name__)\n\n''' Important pipeline for stable diffusion'''       ",
    "suffix": ""
  },
  {
    "name": "khabbazan/Mattermost-Subscriptions:apps/chat/gql/subscriptions.py@652",
    "canonical_solution": "    message = graphene.Field(MessageQueryType)",
    "prompt": "import graphene\nfrom apps.chat.gql.types import MessageQueryType\nfrom helpers.channels_graphql_ws import subscription\n\n\n\nclass OnNewChatMessage(subscription.Subscription):\n    \"\"\"\n    GraphQL Subscription for new chat messages.\n    This subscription allows clients to listen for new messages on a specified channel.\n    \"\"\"\n\n    channel_identifier = graphene.String()",
    "prefix": "import graphene\nfrom apps.chat.gql.types import MessageQueryType\nfrom helpers.channels_graphql_ws import subscription\n\n\n\nclass OnNewChatMessage(subscription.Subscription):\n    \"\"\"\n    GraphQL Subscription for new chat messages.\n    This subscription allows clients to listen for new messages on a specified channel.\n    \"\"\"\n\n    channel_identifier = graphene.String()",
    "suffix": ""
  },
  {
    "name": "Hatins/DEOE:models/detection/yolox_extension/models/yolo_pafpn.py@1394",
    "canonical_solution": "        self.C3_p4 = CSPLayer(",
    "prompt": "from typing import Dict, Optional, Tuple\n    from torch import compile as th_compile\nfrom ...yolox.models.network_blocks import BaseConv, CSPLayer, DWConv\nfrom data.utils.types import BackboneFeatures\nimport torch as th\nimport torch.nn as nn\n\"\"\"\nOriginal Yolox PAFPN code with slight modifications\n\"\"\"\n\n\ntry:\nexcept ImportError:\n    th_compile = None\n\n\n\nclass YOLOPAFPN(nn.Module):\n    \"\"\"\n    Removed the direct dependency on the backbone.\n    \"\"\"\n\n    def __init__(\n            self,\n            depth: float = 1.0,\n            in_stages: Tuple[int, ...] = (2, 3, 4),\n            in_channels: Tuple[int, ...] = (256, 512, 1024),\n            depthwise: bool = False,\n            act: str = \"silu\",\n            compile_cfg: Optional[Dict] = None,\n    ):\n        super().__init__()\n        assert len(in_stages) == len(in_channels)\n        assert len(in_channels) == 3, 'Current implementation only for 3 feature maps'\n        self.in_features = in_stages\n        self.in_channels = in_channels\n        Conv = DWConv if depthwise else BaseConv\n\n        ###### Compile if requested ######\n        if compile_cfg is not None:\n            compile_mdl = compile_cfg['enable']\n            if compile_mdl and th_compile is not None:\n                self.forward = th_compile(self.forward, **compile_cfg['args'])\n            elif compile_mdl:\n                print('Could not compile PAFPN because torch.compile is not available')\n\n        ##################################\n\n        self.upsample = lambda x: nn.functional.interpolate(x, scale_factor=2, mode='nearest-exact')\n        self.lateral_conv0 = BaseConv(\n            in_channels[2], in_channels[1], 1, 1, act=act\n        )",
    "prefix": "from typing import Dict, Optional, Tuple\n    from torch import compile as th_compile\nfrom ...yolox.models.network_blocks import BaseConv, CSPLayer, DWConv\nfrom data.utils.types import BackboneFeatures\nimport torch as th\nimport torch.nn as nn\n\"\"\"\nOriginal Yolox PAFPN code with slight modifications\n\"\"\"\n\n\ntry:\nexcept ImportError:\n    th_compile = None\n\n\n\nclass YOLOPAFPN(nn.Module):\n    \"\"\"\n    Removed the direct dependency on the backbone.\n    \"\"\"\n\n    def __init__(\n            self,\n            depth: float = 1.0,\n            in_stages: Tuple[int, ...] = (2, 3, 4),\n            in_channels: Tuple[int, ...] = (256, 512, 1024),\n            depthwise: bool = False,\n            act: str = \"silu\",\n            compile_cfg: Optional[Dict] = None,\n    ):\n        super().__init__()\n        assert len(in_stages) == len(in_channels)\n        assert len(in_channels) == 3, 'Current implementation only for 3 feature maps'\n        self.in_features = in_stages\n        self.in_channels = in_channels\n        Conv = DWConv if depthwise else BaseConv\n\n        ###### Compile if requested ######\n        if compile_cfg is not None:\n            compile_mdl = compile_cfg['enable']\n            if compile_mdl and th_compile is not None:\n                self.forward = th_compile(self.forward, **compile_cfg['args'])\n            elif compile_mdl:\n                print('Could not compile PAFPN because torch.compile is not available')\n\n        ##################################\n\n        self.upsample = lambda x: nn.functional.interpolate(x, scale_factor=2, mode='nearest-exact')\n        self.lateral_conv0 = BaseConv(\n            in_channels[2], in_channels[1], 1, 1, act=act\n        )",
    "suffix": ""
  },
  {
    "name": "yeyingdege/ctr-din-pytorch:din/model.py@1019",
    "canonical_solution": "            FCLayer(mlp_input_dim, hidden_size=self.hid_dim[1], bias=True, batch_norm=True, activation='dice'),",
    "prompt": "import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom .embedding import EmbeddingLayer\nfrom .fc import FCLayer\nfrom .attention import DinAttentionLayer\n\n\n\nclass DeepInterestNetwork(nn.Module):\n    def __init__(self, n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_DIM=[162,200,80,2]):\n        super(DeepInterestNetwork, self).__init__()\n        self.embedding_dim = EMBEDDING_DIM\n        self.hid_dim = HIDDEN_DIM\n\n        # embeddings\n        self.uid_embeddings = EmbeddingLayer(n_uid, self.embedding_dim)\n        self.mid_embeddings = EmbeddingLayer(n_mid, self.embedding_dim)\n        self.cat_embeddings = EmbeddingLayer(n_cat, self.embedding_dim)\n\n        self.attn = DinAttentionLayer(embedding_dim=self.embedding_dim*2)\n        mlp_input_dim = self.embedding_dim * 9\n        self.mlp = nn.Sequential(",
    "prefix": "import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom .embedding import EmbeddingLayer\nfrom .fc import FCLayer\nfrom .attention import DinAttentionLayer\n\n\n\nclass DeepInterestNetwork(nn.Module):\n    def __init__(self, n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_DIM=[162,200,80,2]):\n        super(DeepInterestNetwork, self).__init__()\n        self.embedding_dim = EMBEDDING_DIM\n        self.hid_dim = HIDDEN_DIM\n\n        # embeddings\n        self.uid_embeddings = EmbeddingLayer(n_uid, self.embedding_dim)\n        self.mid_embeddings = EmbeddingLayer(n_mid, self.embedding_dim)\n        self.cat_embeddings = EmbeddingLayer(n_cat, self.embedding_dim)\n\n        self.attn = DinAttentionLayer(embedding_dim=self.embedding_dim*2)\n        mlp_input_dim = self.embedding_dim * 9\n        self.mlp = nn.Sequential(",
    "suffix": ""
  },
  {
    "name": "iamlooper/VIC-TG-Bot:app/core/client/filters.py@867",
    "canonical_solution": "    return bool(cmd in Config.CMD_DICT.keys())",
    "prompt": "from pyrogram import filters as _filters\nfrom pyrogram.types import Message\nfrom app import Config\nfrom app.core.client.conversation import Conversation\n\n\n# Overall BOT filters\n\nconvo_filter = _filters.create(\n    lambda _, __, message: (message.chat.id in Conversation.CONVO_DICT.keys())\n    and (not message.reactions)\n)\n\n\ndef cmd_check(message: Message, trigger: str) -> bool:\n    start_str = message.text.split(maxsplit=1)[0]\n    cmd = start_str.replace(trigger, \"\", 1)",
    "prefix": "from pyrogram import filters as _filters\nfrom pyrogram.types import Message\nfrom app import Config\nfrom app.core.client.conversation import Conversation\n\n\n# Overall BOT filters\n\nconvo_filter = _filters.create(\n    lambda _, __, message: (message.chat.id in Conversation.CONVO_DICT.keys())\n    and (not message.reactions)\n)\n\n\ndef cmd_check(message: Message, trigger: str) -> bool:\n    start_str = message.text.split(maxsplit=1)[0]\n    cmd = start_str.replace(trigger, \"\", 1)",
    "suffix": ""
  },
  {
    "name": "Enthusiasm23/primkit:src/primkit/utils/LoggerSetup.py@741",
    "canonical_solution": "    log_file_mode = log_file_mode if log_file_mode is not None else LOG_FILE_MODE",
    "prompt": "import logging\nimport logging.handlers\nfrom ..config import LOG_LEVEL, LOG_FILE, LOG_FORMAT, \\\n    LOG_FILE_MODE, MAX_LOG_SIZE, BACKUP_COUNT, LOG_STREAM\n\n\ndef setup_logging(\n    level=None,\n    log_file=None,\n    format=None,\n    log_file_mode=None,\n    max_log_size=None,\n    backup_count=None,\n    stream=None\n):\n    \"\"\"\n    Configure logging for the application.\n\n    :param level: The logging level, e.g., 'DEBUG', 'INFO', 'WARNING'. Defaults to value from config.py but can be overridden by user input.\n    :param log_file: Path to the log file. If specified, logs will be written to the file. Defaults to value from config.py but can be overridden by user input.\n    :param format: The format for the logging messages. Defaults to value from config.py but can be overridden by user input.\n    :param log_file_mode: The mode for writing to the log file, e.g., 'a' for append mode. Defaults to value from config.py but can be overridden by user input.\n    :param max_log_size: The maximum size of the log file in bytes. When exceeded, the log will rotate. Defaults to value from config.py but can be overridden by user input.\n    :param backup_count: The number of backup log files to keep. Defaults to value from config.py but can be overridden by user input.\n    :param stream: Whether to output logs to the console. Defaults to value from config.py but can be overridden by user input.\n\n    The function uses the default configuration or configuration provided by the user. Logging can be directed to a file, console, or both based on parameters.\n    \"\"\"\n\n    # Use the default configuration or user-provided configuration\n    if level is not None:\n        if isinstance(level, int):\n            log_level = level\n        else:\n            log_level = getattr(logging, level.upper(), logging.INFO)\n    else:\n        if isinstance(LOG_LEVEL, int):\n            log_level = LOG_LEVEL\n        else:\n            log_level = getattr(logging, LOG_LEVEL.upper(), logging.INFO)\n    log_file = log_file if log_file is not None else LOG_FILE\n    format = format if format is not None else LOG_FORMAT",
    "prefix": "import logging\nimport logging.handlers\nfrom ..config import LOG_LEVEL, LOG_FILE, LOG_FORMAT, \\\n    LOG_FILE_MODE, MAX_LOG_SIZE, BACKUP_COUNT, LOG_STREAM\n\n\ndef setup_logging(\n    level=None,\n    log_file=None,\n    format=None,\n    log_file_mode=None,\n    max_log_size=None,\n    backup_count=None,\n    stream=None\n):\n    \"\"\"\n    Configure logging for the application.\n\n    :param level: The logging level, e.g., 'DEBUG', 'INFO', 'WARNING'. Defaults to value from config.py but can be overridden by user input.\n    :param log_file: Path to the log file. If specified, logs will be written to the file. Defaults to value from config.py but can be overridden by user input.\n    :param format: The format for the logging messages. Defaults to value from config.py but can be overridden by user input.\n    :param log_file_mode: The mode for writing to the log file, e.g., 'a' for append mode. Defaults to value from config.py but can be overridden by user input.\n    :param max_log_size: The maximum size of the log file in bytes. When exceeded, the log will rotate. Defaults to value from config.py but can be overridden by user input.\n    :param backup_count: The number of backup log files to keep. Defaults to value from config.py but can be overridden by user input.\n    :param stream: Whether to output logs to the console. Defaults to value from config.py but can be overridden by user input.\n\n    The function uses the default configuration or configuration provided by the user. Logging can be directed to a file, console, or both based on parameters.\n    \"\"\"\n\n    # Use the default configuration or user-provided configuration\n    if level is not None:\n        if isinstance(level, int):\n            log_level = level\n        else:\n            log_level = getattr(logging, level.upper(), logging.INFO)\n    else:\n        if isinstance(LOG_LEVEL, int):\n            log_level = LOG_LEVEL\n        else:\n            log_level = getattr(logging, LOG_LEVEL.upper(), logging.INFO)\n    log_file = log_file if log_file is not None else LOG_FILE\n    format = format if format is not None else LOG_FORMAT",
    "suffix": ""
  },
  {
    "name": "Wangyuhao06/2022-adhoc:src/env.py@1479",
    "canonical_solution": "        self.geo_area = random_waypoint(self.node_max, dimensions=(MOV_AREA, MOV_AREA), velocity=(10, 15), wt_max=1.0)",
    "prompt": "import random\nimport numpy as np\nfrom math import log2, log10\nfrom queue import Queue\nfrom pymobility.models.mobility import random_waypoint\nfrom src.node import Node\nfrom src.packet import Packet\nfrom src.parameter import *\nfrom src.transtask import Trans_task\n\n\n\n\nclass Environment():\n     #\u521d\u59cb\u5316\u73af\u5883\n    def __init__(self):\n        #\u521d\u59cb\u6570\u636e-\u6700\u5927\u8282\u70b9\u6570\n        self.node_max=NODE_MAX\n        self.node_space_size=NODE_MAX\n        self.node_moving_area=MOV_AREA\n        #\u521d\u59cb\u5316\u4e8c\u7ef4\u5e73\u9762",
    "prefix": "import random\nimport numpy as np\nfrom math import log2, log10\nfrom queue import Queue\nfrom pymobility.models.mobility import random_waypoint\nfrom src.node import Node\nfrom src.packet import Packet\nfrom src.parameter import *\nfrom src.transtask import Trans_task\n\n\n\n\nclass Environment():\n     #\u521d\u59cb\u5316\u73af\u5883\n    def __init__(self):\n        #\u521d\u59cb\u6570\u636e-\u6700\u5927\u8282\u70b9\u6570\n        self.node_max=NODE_MAX\n        self.node_space_size=NODE_MAX\n        self.node_moving_area=MOV_AREA\n        #\u521d\u59cb\u5316\u4e8c\u7ef4\u5e73\u9762",
    "suffix": ""
  },
  {
    "name": "karthicksivakumarp/gui_read_csv:main.py@800",
    "canonical_solution": "gui.UI(root, main_read_csv, analyze_data, report_gen)\r",
    "prompt": "from read_from_csv import read_csv_file\r\nfrom data_analysis import analyze_data\r\nfrom report_generation import generate_report\r\nfrom tkinter import Tk\r\nfrom user_interface import gui\r\n# Import necessary modules\r\n\r\n# Initialize CSV reader instance\r\nread_csv = read_csv_file.read_csv_data()\r\n\r\n# Obtain the function/method for reading multiple CSV files\r\n# Note: \"read_mult_csv_file\" is a function or method defined in the \"read_csv_file\" module\r\nmain_read_csv = read_csv.read_mult_csv_file\r\n\r\n# Initialize data analyzer instance\r\nanalyze_data = analyze_data.analyze_csv_data()\r\n\r\n# Initialize report generator instance\r\nreport_gen = generate_report.generate_report()\r\n\r\n# Create the main Tkinter window\r\nroot = Tk()\r\nroot.title('Csv DataAnalyzer')  # Set the title of the Tkinter window\r\nroot.geometry(\"800x600\")  # Set the initial dimensions of the Tkinter window\r\n\r\n# Create the user interface (GUI) using the UI class from the \"user_interface\" module\r\n# Pass the necessary components (main_read_csv, analyze_data, report_gen) to the GUI\r",
    "prefix": "from read_from_csv import read_csv_file\r\nfrom data_analysis import analyze_data\r\nfrom report_generation import generate_report\r\nfrom tkinter import Tk\r\nfrom user_interface import gui\r\n# Import necessary modules\r\n\r\n# Initialize CSV reader instance\r\nread_csv = read_csv_file.read_csv_data()\r\n\r\n# Obtain the function/method for reading multiple CSV files\r\n# Note: \"read_mult_csv_file\" is a function or method defined in the \"read_csv_file\" module\r\nmain_read_csv = read_csv.read_mult_csv_file\r\n\r\n# Initialize data analyzer instance\r\nanalyze_data = analyze_data.analyze_csv_data()\r\n\r\n# Initialize report generator instance\r\nreport_gen = generate_report.generate_report()\r\n\r\n# Create the main Tkinter window\r\nroot = Tk()\r\nroot.title('Csv DataAnalyzer')  # Set the title of the Tkinter window\r\nroot.geometry(\"800x600\")  # Set the initial dimensions of the Tkinter window\r\n\r\n# Create the user interface (GUI) using the UI class from the \"user_interface\" module\r\n# Pass the necessary components (main_read_csv, analyze_data, report_gen) to the GUI\r",
    "suffix": ""
  },
  {
    "name": "Slenderman00/Ask-Surf:AskSurf/cli.py@795",
    "canonical_solution": "        edit_settings()",
    "prompt": "import os\nimport requests\nimport argparse\nimport tqdm\nimport time\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom halo import Halo\nfrom .settings import load_settings, settings_exist, edit_settings\n\nsettings = {}\nown_dir = Path(__file__).parent.absolute()\nquestion_pipe = own_dir / \"question_pipe\"\nresponse_pipe = own_dir / \"response_pipe\"\n\n\ndef conditional_decorator(dec, condition):\n    def decorator(func):\n        if not condition:\n            # Return the function unchanged, not decorated.\n            return func\n        return dec(func)\n\n    return decorator\n\n\ndef parse_message(message):\n    # replace the tags with the correct color codes\n    message = message.replace(\"[RED]\", \"\\033[31m\")\n    message = message.replace(\"[YELLOW]\", \"\\033[33m\")\n    message = message.replace(\"[ORANGE]\", \"\\033[33m\")\n    message = message.replace(\"[GREEN]\", \"\\033[32m\")\n    message = message.replace(\"[PURPLE]\", \"\\033[35m\")\n    message = message.replace(\"[BLUE]\", \"\\033[34m\")\n    message = message.replace(\"[NORMAL]\", \"\\033[0m\")\n\n    # replace all end tags with the normal color code\n    message = message.replace(\"[/RED]\", \"\\033[0m\")\n    message = message.replace(\"[/YELLOW]\", \"\\033[0m\")\n    message = message.replace(\"[/ORANGE]\", \"\\033[0m\")\n    message = message.replace(\"[/GREEN]\", \"\\033[0m\")\n    message = message.replace(\"[/PURPLE]\", \"\\033[0m\")\n    message = message.replace(\"[/BLUE]\", \"\\033[0m\")\n    message = message.replace(\"[/NORMAL]\", \"\\033[0m\")\n\n    return message\n\n\ndef init():\n    if not model_exists():\n        print(\"Please select a model\")\n        download_model(select_model())\n\n    if not settings_exist():\n        print(\"Please make sure the settings are correct\")\n        settings = load_settings()\n        exit(1)\n\n\ndef main():\n    \"\"\"Main entry point for the application\"\"\"\n    init()\n\n    # parse the arguments\n    parser = argparse.ArgumentParser(description=\"AskSurf CLI\")\n    parser.add_argument(\n        \"question\",\n        nargs=argparse.REMAINDER,\n        help=\"The question to ask Dolphin\",\n    )\n    parser.add_argument(\n        \"--model\",\n        \"-m\",\n        action=\"store_true\",\n        help=\"The model to use\",\n    )\n    parser.add_argument(\n        \"--delete\",\n        \"-d\",\n        action=\"store_true\",\n        help=\"Delete the model\",\n    )\n    parser.add_argument(\n        \"--kill\",\n        \"-k\",\n        action=\"store_true\",\n        help=\"Kill the Dolphin service\",\n    )\n    parser.add_argument(\n        \"--settings\",\n        \"-s\",\n        action=\"store_true\",\n        help=\"Edit the settings\",\n    )\n    args = parser.parse_args()\n\n    if args.model:\n        download_model(select_model())\n        return\n\n    if args.delete:\n        delete_model()\n        return\n\n    if args.kill:\n        os.system(\"pkill -f dolphin_service.py\")\n        return\n\n    if args.settings:",
    "prefix": "import os\nimport requests\nimport argparse\nimport tqdm\nimport time\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom halo import Halo\nfrom .settings import load_settings, settings_exist, edit_settings\n\nsettings = {}\nown_dir = Path(__file__).parent.absolute()\nquestion_pipe = own_dir / \"question_pipe\"\nresponse_pipe = own_dir / \"response_pipe\"\n\n\ndef conditional_decorator(dec, condition):\n    def decorator(func):\n        if not condition:\n            # Return the function unchanged, not decorated.\n            return func\n        return dec(func)\n\n    return decorator\n\n\ndef parse_message(message):\n    # replace the tags with the correct color codes\n    message = message.replace(\"[RED]\", \"\\033[31m\")\n    message = message.replace(\"[YELLOW]\", \"\\033[33m\")\n    message = message.replace(\"[ORANGE]\", \"\\033[33m\")\n    message = message.replace(\"[GREEN]\", \"\\033[32m\")\n    message = message.replace(\"[PURPLE]\", \"\\033[35m\")\n    message = message.replace(\"[BLUE]\", \"\\033[34m\")\n    message = message.replace(\"[NORMAL]\", \"\\033[0m\")\n\n    # replace all end tags with the normal color code\n    message = message.replace(\"[/RED]\", \"\\033[0m\")\n    message = message.replace(\"[/YELLOW]\", \"\\033[0m\")\n    message = message.replace(\"[/ORANGE]\", \"\\033[0m\")\n    message = message.replace(\"[/GREEN]\", \"\\033[0m\")\n    message = message.replace(\"[/PURPLE]\", \"\\033[0m\")\n    message = message.replace(\"[/BLUE]\", \"\\033[0m\")\n    message = message.replace(\"[/NORMAL]\", \"\\033[0m\")\n\n    return message\n\n\ndef init():\n    if not model_exists():\n        print(\"Please select a model\")\n        download_model(select_model())\n\n    if not settings_exist():\n        print(\"Please make sure the settings are correct\")\n        settings = load_settings()\n        exit(1)\n\n\ndef main():\n    \"\"\"Main entry point for the application\"\"\"\n    init()\n\n    # parse the arguments\n    parser = argparse.ArgumentParser(description=\"AskSurf CLI\")\n    parser.add_argument(\n        \"question\",\n        nargs=argparse.REMAINDER,\n        help=\"The question to ask Dolphin\",\n    )\n    parser.add_argument(\n        \"--model\",\n        \"-m\",\n        action=\"store_true\",\n        help=\"The model to use\",\n    )\n    parser.add_argument(\n        \"--delete\",\n        \"-d\",\n        action=\"store_true\",\n        help=\"Delete the model\",\n    )\n    parser.add_argument(\n        \"--kill\",\n        \"-k\",\n        action=\"store_true\",\n        help=\"Kill the Dolphin service\",\n    )\n    parser.add_argument(\n        \"--settings\",\n        \"-s\",\n        action=\"store_true\",\n        help=\"Edit the settings\",\n    )\n    args = parser.parse_args()\n\n    if args.model:\n        download_model(select_model())\n        return\n\n    if args.delete:\n        delete_model()\n        return\n\n    if args.kill:\n        os.system(\"pkill -f dolphin_service.py\")\n        return\n\n    if args.settings:",
    "suffix": ""
  },
  {
    "name": "davidsvy/fractal_video:src/prepare_data/diving48.py@685",
    "canonical_solution": "    dataset_stats(root=root, ext=ext)",
    "prompt": "import json\nimport os\nimport shutil\nfrom ..utils.data import dataset_stats\nfrom ..utils.other import run_bash\n\n\n\ndef move_files(path_split, dir_src, dir_tgt, ext):\n    with open(path_split, 'r') as file:\n        lut = json.load(file)\n\n        for item in lut:\n            filename = f'{item[\"vid_name\"]}.{ext}'\n            path_src = os.path.join(dir_src, filename)\n            label = str(item['label'])\n            dir_label = os.path.join(dir_tgt, label)\n            path_tgt = os.path.join(dir_label, filename)\n\n            os.makedirs(dir_label, exist_ok=True)\n            shutil.move(path_src, path_tgt)\n\n\ndef diving48(root):\n    \"\"\"\n    train -> 15943 files\n    val -> 2096 files\n    \"\"\"\n    url_data = 'http://www.svcl.ucsd.edu/projects/resound/Diving48_rgb.tar.gz'\n    url_split_train = 'http://www.svcl.ucsd.edu/projects/resound/Diving48_train.json'\n    url_split_val = 'http://www.svcl.ucsd.edu/projects/resound/Diving48_test.json'\n\n    path_data = os.path.join(root, os.path.basename(url_data))\n    path_split_train = os.path.join(root, os.path.basename(url_split_train))\n    path_split_val = os.path.join(root, os.path.basename(url_split_val))\n\n    dir_src = os.path.join(root, 'rgb')\n    dir_train = os.path.join(root, 'train')\n    dir_val = os.path.join(root, 'val')\n    ext = 'mp4'\n\n    os.makedirs(dir_train, exist_ok=True)\n    os.makedirs(dir_val, exist_ok=True)\n\n    print('\\nDownloading DIVING48...')\n    run_bash(f'wget {url_split_train} -P {root}')\n    run_bash(f'wget {url_split_val} -P {root}')\n    run_bash(f'wget {url_data} -P {root}')\n\n    print('Extracting DIVING48...')\n    run_bash(f'tar -xf {path_data} -C {root}')\n    os.remove(path_data)\n\n    move_files(\n        path_split=path_split_train, dir_src=dir_src,\n        dir_tgt=dir_train, ext=ext\n    )\n\n    move_files(\n        path_split=path_split_val, dir_src=dir_src,\n        dir_tgt=dir_val, ext=ext\n    )\n\n    shutil.rmtree(dir_src)\n    os.remove(path_split_train)\n    os.remove(path_split_val)\n",
    "prefix": "import json\nimport os\nimport shutil\nfrom ..utils.data import dataset_stats\nfrom ..utils.other import run_bash\n\n\n\ndef move_files(path_split, dir_src, dir_tgt, ext):\n    with open(path_split, 'r') as file:\n        lut = json.load(file)\n\n        for item in lut:\n            filename = f'{item[\"vid_name\"]}.{ext}'\n            path_src = os.path.join(dir_src, filename)\n            label = str(item['label'])\n            dir_label = os.path.join(dir_tgt, label)\n            path_tgt = os.path.join(dir_label, filename)\n\n            os.makedirs(dir_label, exist_ok=True)\n            shutil.move(path_src, path_tgt)\n\n\ndef diving48(root):\n    \"\"\"\n    train -> 15943 files\n    val -> 2096 files\n    \"\"\"\n    url_data = 'http://www.svcl.ucsd.edu/projects/resound/Diving48_rgb.tar.gz'\n    url_split_train = 'http://www.svcl.ucsd.edu/projects/resound/Diving48_train.json'\n    url_split_val = 'http://www.svcl.ucsd.edu/projects/resound/Diving48_test.json'\n\n    path_data = os.path.join(root, os.path.basename(url_data))\n    path_split_train = os.path.join(root, os.path.basename(url_split_train))\n    path_split_val = os.path.join(root, os.path.basename(url_split_val))\n\n    dir_src = os.path.join(root, 'rgb')\n    dir_train = os.path.join(root, 'train')\n    dir_val = os.path.join(root, 'val')\n    ext = 'mp4'\n\n    os.makedirs(dir_train, exist_ok=True)\n    os.makedirs(dir_val, exist_ok=True)\n\n    print('\\nDownloading DIVING48...')\n    run_bash(f'wget {url_split_train} -P {root}')\n    run_bash(f'wget {url_split_val} -P {root}')\n    run_bash(f'wget {url_data} -P {root}')\n\n    print('Extracting DIVING48...')\n    run_bash(f'tar -xf {path_data} -C {root}')\n    os.remove(path_data)\n\n    move_files(\n        path_split=path_split_train, dir_src=dir_src,\n        dir_tgt=dir_train, ext=ext\n    )\n\n    move_files(\n        path_split=path_split_val, dir_src=dir_src,\n        dir_tgt=dir_val, ext=ext\n    )\n\n    shutil.rmtree(dir_src)\n    os.remove(path_split_train)\n    os.remove(path_split_val)\n",
    "suffix": ""
  },
  {
    "name": "OpenBrickProtocolFoundation/client:main.py@754",
    "canonical_solution": "                        tetrion.enqueue_event(Event(key=Key.LEFT, type=EventType.PRESSED, frame=frame))",
    "prompt": "import pygame\nfrom tetrion import Event\nfrom tetrion import EventType\nfrom tetrion import Key\nfrom tetrion import Tetrion\n\n\n\ndef main() -> None:\n    frame = 0\n\n    with Tetrion() as tetrion:\n        pygame.init()\n\n        RECT_SIZE = 30\n        size = (RECT_SIZE * tetrion.width, (RECT_SIZE + 2) * tetrion.height)\n        screen = pygame.display.set_mode(size)\n\n        COLORS = [(0, 0, 0),\n                  (0, 240, 240),\n                  (0, 0, 240),\n                  (240, 160, 0),\n                  (240, 240, 0),\n                  (0, 240, 0),\n                  (160, 0, 240),\n                  (240, 0, 0)]\n\n        done = False\n\n        clock = pygame.time.Clock()\n\n        while not done:\n            for event in pygame.event.get():\n                if event.type == pygame.QUIT:\n                    done = True\n                elif event.type == pygame.KEYDOWN:\n                    if event.key == pygame.K_ESCAPE:\n                        done = True\n                    elif event.key == pygame.K_a:",
    "prefix": "import pygame\nfrom tetrion import Event\nfrom tetrion import EventType\nfrom tetrion import Key\nfrom tetrion import Tetrion\n\n\n\ndef main() -> None:\n    frame = 0\n\n    with Tetrion() as tetrion:\n        pygame.init()\n\n        RECT_SIZE = 30\n        size = (RECT_SIZE * tetrion.width, (RECT_SIZE + 2) * tetrion.height)\n        screen = pygame.display.set_mode(size)\n\n        COLORS = [(0, 0, 0),\n                  (0, 240, 240),\n                  (0, 0, 240),\n                  (240, 160, 0),\n                  (240, 240, 0),\n                  (0, 240, 0),\n                  (160, 0, 240),\n                  (240, 0, 0)]\n\n        done = False\n\n        clock = pygame.time.Clock()\n\n        while not done:\n            for event in pygame.event.get():\n                if event.type == pygame.QUIT:\n                    done = True\n                elif event.type == pygame.KEYDOWN:\n                    if event.key == pygame.K_ESCAPE:\n                        done = True\n                    elif event.key == pygame.K_a:",
    "suffix": ""
  },
  {
    "name": "Birch-san/natten-fwd-ad:script/demo.py@775",
    "canonical_solution": "natten_block = NattenBlock(d_model, d_head=d_head, kernel_size=kernel_size).to(device=device, dtype=dtype)",
    "prompt": "import torch\nimport torch.autograd.forward_ad as fwAD\nfrom torch import inference_mode, enable_grad\nfrom torch.backends.cuda import sdp_kernel\nfrom src.natten_block import NattenBlock\nfrom src.hood_attn_block import NeighbourhoodAttnBlock\n\ndevice=torch.device('cuda')\ndtype=torch.bfloat16\nseed=42\nd_model=128\nd_head=64\nkernel_size=13\ntorch.manual_seed(seed)",
    "prefix": "import torch\nimport torch.autograd.forward_ad as fwAD\nfrom torch import inference_mode, enable_grad\nfrom torch.backends.cuda import sdp_kernel\nfrom src.natten_block import NattenBlock\nfrom src.hood_attn_block import NeighbourhoodAttnBlock\n\ndevice=torch.device('cuda')\ndtype=torch.bfloat16\nseed=42\nd_model=128\nd_head=64\nkernel_size=13\ntorch.manual_seed(seed)",
    "suffix": ""
  },
  {
    "name": "ysyBrenda/Transformer-For-Geochemical-Anomaly-Detection:anomaly_detection.py@1019",
    "canonical_solution": "    model = Transformer(",
    "prompt": "import torch\nimport argparse\nimport dill as pickle\nimport numpy as np\nimport calculate_anomalyscore\n    import torch.utils.data as Data\n    import time\nfrom tqdm import tqdm\nfrom transformer.Models import Transformer\nfrom transformer.Translator import Translator\n'''\ngeochemical anomaly detection\n1\uff0creconstruct geochemical data with trained model.\n2\uff0cthen, identify geochemical anomaly\nAuthor: ysyBrenda\n'''\n\n\n\ndef load_model(opt, device):\n\n    checkpoint = torch.load(opt.model, map_location=device)\n    model_opt = checkpoint['settings']\n",
    "prefix": "import torch\nimport argparse\nimport dill as pickle\nimport numpy as np\nimport calculate_anomalyscore\n    import torch.utils.data as Data\n    import time\nfrom tqdm import tqdm\nfrom transformer.Models import Transformer\nfrom transformer.Translator import Translator\n'''\ngeochemical anomaly detection\n1\uff0creconstruct geochemical data with trained model.\n2\uff0cthen, identify geochemical anomaly\nAuthor: ysyBrenda\n'''\n\n\n\ndef load_model(opt, device):\n\n    checkpoint = torch.load(opt.model, map_location=device)\n    model_opt = checkpoint['settings']\n",
    "suffix": ""
  },
  {
    "name": "camenduru/MotionCtrl-hf:lvdm/modules/attention.py@1032",
    "canonical_solution": "        context_dim = default(context_dim, query_dim)",
    "prompt": "import math\nimport torch\nimport torch.nn.functional as F\n    import xformers\n    import xformers.ops\nfrom functools import partial\nfrom inspect import isfunction\nfrom einops import rearrange, repeat\nfrom torch import einsum, nn\nfrom lvdm.basics import conv_nd, normalization, zero_module\nfrom lvdm.common import checkpoint, default, exists, init_, max_neg_value, uniq\n\n\ntry:\n    XFORMERS_IS_AVAILBLE = True\nexcept:\n    XFORMERS_IS_AVAILBLE = False\n\n\nclass RelativePosition(nn.Module):\n    \"\"\" https://github.com/evelinehong/Transformer_Relative_Position_PyTorch/blob/master/relative_position.py \"\"\"\n\n    def __init__(self, num_units, max_relative_position):\n        super().__init__()\n        self.num_units = num_units\n        self.max_relative_position = max_relative_position\n        self.embeddings_table = nn.Parameter(torch.Tensor(max_relative_position * 2 + 1, num_units))\n        nn.init.xavier_uniform_(self.embeddings_table)\n\n    def forward(self, length_q, length_k):\n        device = self.embeddings_table.device\n        range_vec_q = torch.arange(length_q, device=device)\n        range_vec_k = torch.arange(length_k, device=device)\n        distance_mat = range_vec_k[None, :] - range_vec_q[:, None]\n        distance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)\n        final_mat = distance_mat_clipped + self.max_relative_position\n        # final_mat = th.LongTensor(final_mat).to(self.embeddings_table.device)\n        # final_mat = th.tensor(final_mat, device=self.embeddings_table.device, dtype=torch.long)\n        final_mat = final_mat.long()\n        embeddings = self.embeddings_table[final_mat]\n        return embeddings\n\n\nclass CrossAttention(nn.Module):\n\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0., \n                 relative_position=False, temporal_length=None):\n        super().__init__()\n        inner_dim = dim_head * heads",
    "prefix": "import math\nimport torch\nimport torch.nn.functional as F\n    import xformers\n    import xformers.ops\nfrom functools import partial\nfrom inspect import isfunction\nfrom einops import rearrange, repeat\nfrom torch import einsum, nn\nfrom lvdm.basics import conv_nd, normalization, zero_module\nfrom lvdm.common import checkpoint, default, exists, init_, max_neg_value, uniq\n\n\ntry:\n    XFORMERS_IS_AVAILBLE = True\nexcept:\n    XFORMERS_IS_AVAILBLE = False\n\n\nclass RelativePosition(nn.Module):\n    \"\"\" https://github.com/evelinehong/Transformer_Relative_Position_PyTorch/blob/master/relative_position.py \"\"\"\n\n    def __init__(self, num_units, max_relative_position):\n        super().__init__()\n        self.num_units = num_units\n        self.max_relative_position = max_relative_position\n        self.embeddings_table = nn.Parameter(torch.Tensor(max_relative_position * 2 + 1, num_units))\n        nn.init.xavier_uniform_(self.embeddings_table)\n\n    def forward(self, length_q, length_k):\n        device = self.embeddings_table.device\n        range_vec_q = torch.arange(length_q, device=device)\n        range_vec_k = torch.arange(length_k, device=device)\n        distance_mat = range_vec_k[None, :] - range_vec_q[:, None]\n        distance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)\n        final_mat = distance_mat_clipped + self.max_relative_position\n        # final_mat = th.LongTensor(final_mat).to(self.embeddings_table.device)\n        # final_mat = th.tensor(final_mat, device=self.embeddings_table.device, dtype=torch.long)\n        final_mat = final_mat.long()\n        embeddings = self.embeddings_table[final_mat]\n        return embeddings\n\n\nclass CrossAttention(nn.Module):\n\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0., \n                 relative_position=False, temporal_length=None):\n        super().__init__()\n        inner_dim = dim_head * heads",
    "suffix": ""
  },
  {
    "name": "vita-epfl/social-transmotion:evaluate_jrdb.py@1456",
    "canonical_solution": "        in_joints, in_masks, out_joints, out_masks, padding_mask = batch_process_coords(joints, masks, padding_mask, config, modality_selection)",
    "prompt": "import argparse\nimport torch\nimport random\nimport numpy as np\nfrom progress.bar import Bar\nfrom torch.utils.data import DataLoader\nfrom dataset_jrdb import batch_process_coords, create_dataset, collate_batch\nfrom model_jrdb import create_model\nfrom utils.utils import create_logger\n\n\ndef inference(model, config, input_joints, padding_mask, out_len=14):\n    model.eval()\n    \n    with torch.no_grad():\n        pred_joints = model(input_joints, padding_mask)\n\n    output_joints = pred_joints[:,-out_len:]\n\n    return output_joints\n\n\ndef evaluate_ade_fde(model, modality_selection, dataloader, bs, config, logger, return_all=False, bar_prefix=\"\", per_joint=False, show_avg=False):\n    in_F, out_F = config['TRAIN']['input_track_size'], config['TRAIN']['output_track_size']\n    bar = Bar(f\"EVAL ADE_FDE\", fill=\"#\", max=len(dataloader))\n\n    batch_size = bs\n    batch_id = 0\n    ade = 0\n    fde = 0\n    ade_batch = 0 \n    fde_batch = 0\n    for i, batch in enumerate(dataloader):\n        joints, masks, padding_mask = batch\n        padding_mask = padding_mask.to(config[\"DEVICE\"])\n   ",
    "prefix": "import argparse\nimport torch\nimport random\nimport numpy as np\nfrom progress.bar import Bar\nfrom torch.utils.data import DataLoader\nfrom dataset_jrdb import batch_process_coords, create_dataset, collate_batch\nfrom model_jrdb import create_model\nfrom utils.utils import create_logger\n\n\ndef inference(model, config, input_joints, padding_mask, out_len=14):\n    model.eval()\n    \n    with torch.no_grad():\n        pred_joints = model(input_joints, padding_mask)\n\n    output_joints = pred_joints[:,-out_len:]\n\n    return output_joints\n\n\ndef evaluate_ade_fde(model, modality_selection, dataloader, bs, config, logger, return_all=False, bar_prefix=\"\", per_joint=False, show_avg=False):\n    in_F, out_F = config['TRAIN']['input_track_size'], config['TRAIN']['output_track_size']\n    bar = Bar(f\"EVAL ADE_FDE\", fill=\"#\", max=len(dataloader))\n\n    batch_size = bs\n    batch_id = 0\n    ade = 0\n    fde = 0\n    ade_batch = 0 \n    fde_batch = 0\n    for i, batch in enumerate(dataloader):\n        joints, masks, padding_mask = batch\n        padding_mask = padding_mask.to(config[\"DEVICE\"])\n   ",
    "suffix": ""
  },
  {
    "name": "facebookresearch/ca_body:ca_body/nn/shadow.py@1068",
    "canonical_solution": "        self.apply(weights_initializer(self.lrelu_slope))",
    "prompt": "import logging\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport ca_body.nn.layers as la\nfrom typing import Optional, Dict\nfrom ca_body.nn.blocks import tile2d, weights_initializer\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# \n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n\n\n# TODO: use shared utils here?\n\nlogger = logging.getLogger(__name__)\n\n\nclass ShadowUNet(nn.Module):\n    def __init__(\n        self,\n        uv_size,\n        ao_mean,\n        shadow_size,\n        lrelu_slope=0.2,\n        beta=1.0,\n        n_dims=64,\n        interp_mode=\"bilinear\",\n        biases=True,\n        trainable_mean=False,\n    ):\n        super().__init__()\n\n        # this is the size of the output\n        self.uv_size = uv_size\n        self.shadow_size = shadow_size\n\n        ao_mean = F.interpolate(\n            th.as_tensor(ao_mean)[np.newaxis],\n            size=(self.shadow_size, self.shadow_size),\n        )[0]\n        if not trainable_mean:\n            # TODO:\n            self.register_buffer(\"ao_mean\", ao_mean)\n        else:\n            self.register_parameter(\"ao_mean\", th.nn.Parameter(ao_mean))\n\n        self.depth = 3\n        self.lrelu_slope = lrelu_slope\n        self.interp_mode = interp_mode\n        self.align_corners = None\n        if interp_mode == \"bilinear\":\n            self.align_corners = False\n\n        # the base number of dimensions for the shadow maps\n        n_dims = n_dims\n\n        # TODO: generate this?\n        self.n_enc_dims = [\n            (1, n_dims),\n            (n_dims, n_dims),\n            (n_dims, n_dims),\n            (n_dims, n_dims),\n        ]\n\n        self.sizes = [shadow_size // (2**i) for i in range(len(self.n_enc_dims))]\n\n        logger.debug(f\"sizes: {self.sizes}\")\n\n        self.enc_layers = nn.ModuleList()\n        for i, size in enumerate(self.sizes):\n            n_in, n_out = self.n_enc_dims[i]\n            logger.debug(f\"EncoderLayers({i}): {n_in}, {n_out}, {size}\")\n            self.enc_layers.append(\n                nn.Sequential(\n                    la.Conv2dWNUB(\n                        n_in,\n                        n_out,\n                        kernel_size=3,\n                        height=size,\n                        width=size,\n                        stride=1,\n                        padding=1,\n                    ),\n                    nn.LeakyReLU(self.lrelu_slope, inplace=True),\n                )\n            )\n\n        self.n_dec_dims = [\n            (n_dims, n_dims),\n            (n_dims * 2, n_dims),\n            (n_dims * 2, n_dims),\n            (n_dims * 2, n_dims),\n        ]\n        self.dec_layers = nn.ModuleList()\n        for i in range(len(self.sizes)):\n            size = self.sizes[-i - 1]\n            n_in, n_out = self.n_dec_dims[i]\n            logger.debug(f\"DecoderLayer({i}): {n_in}, {n_out}, {size}\")\n\n            self.dec_layers.append(\n                nn.Sequential(\n                    la.Conv2dWNUB(\n                        n_in,\n                        n_out,\n                        kernel_size=3,\n                        height=size,\n                        width=size,\n                        stride=1,\n                        padding=1,\n                    ),\n                    nn.LeakyReLU(self.lrelu_slope, inplace=True),\n                )\n            )\n",
    "prefix": "import logging\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport ca_body.nn.layers as la\nfrom typing import Optional, Dict\nfrom ca_body.nn.blocks import tile2d, weights_initializer\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# \n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n\n\n# TODO: use shared utils here?\n\nlogger = logging.getLogger(__name__)\n\n\nclass ShadowUNet(nn.Module):\n    def __init__(\n        self,\n        uv_size,\n        ao_mean,\n        shadow_size,\n        lrelu_slope=0.2,\n        beta=1.0,\n        n_dims=64,\n        interp_mode=\"bilinear\",\n        biases=True,\n        trainable_mean=False,\n    ):\n        super().__init__()\n\n        # this is the size of the output\n        self.uv_size = uv_size\n        self.shadow_size = shadow_size\n\n        ao_mean = F.interpolate(\n            th.as_tensor(ao_mean)[np.newaxis],\n            size=(self.shadow_size, self.shadow_size),\n        )[0]\n        if not trainable_mean:\n            # TODO:\n            self.register_buffer(\"ao_mean\", ao_mean)\n        else:\n            self.register_parameter(\"ao_mean\", th.nn.Parameter(ao_mean))\n\n        self.depth = 3\n        self.lrelu_slope = lrelu_slope\n        self.interp_mode = interp_mode\n        self.align_corners = None\n        if interp_mode == \"bilinear\":\n            self.align_corners = False\n\n        # the base number of dimensions for the shadow maps\n        n_dims = n_dims\n\n        # TODO: generate this?\n        self.n_enc_dims = [\n            (1, n_dims),\n            (n_dims, n_dims),\n            (n_dims, n_dims),\n            (n_dims, n_dims),\n        ]\n\n        self.sizes = [shadow_size // (2**i) for i in range(len(self.n_enc_dims))]\n\n        logger.debug(f\"sizes: {self.sizes}\")\n\n        self.enc_layers = nn.ModuleList()\n        for i, size in enumerate(self.sizes):\n            n_in, n_out = self.n_enc_dims[i]\n            logger.debug(f\"EncoderLayers({i}): {n_in}, {n_out}, {size}\")\n            self.enc_layers.append(\n                nn.Sequential(\n                    la.Conv2dWNUB(\n                        n_in,\n                        n_out,\n                        kernel_size=3,\n                        height=size,\n                        width=size,\n                        stride=1,\n                        padding=1,\n                    ),\n                    nn.LeakyReLU(self.lrelu_slope, inplace=True),\n                )\n            )\n\n        self.n_dec_dims = [\n            (n_dims, n_dims),\n            (n_dims * 2, n_dims),\n            (n_dims * 2, n_dims),\n            (n_dims * 2, n_dims),\n        ]\n        self.dec_layers = nn.ModuleList()\n        for i in range(len(self.sizes)):\n            size = self.sizes[-i - 1]\n            n_in, n_out = self.n_dec_dims[i]\n            logger.debug(f\"DecoderLayer({i}): {n_in}, {n_out}, {size}\")\n\n            self.dec_layers.append(\n                nn.Sequential(\n                    la.Conv2dWNUB(\n                        n_in,\n                        n_out,\n                        kernel_size=3,\n                        height=size,\n                        width=size,\n                        stride=1,\n                        padding=1,\n                    ),\n                    nn.LeakyReLU(self.lrelu_slope, inplace=True),\n                )\n            )\n",
    "suffix": ""
  },
  {
    "name": "0x00wolf/hkrsAI:src/logger.py@1302",
    "canonical_solution": "    def log(self, conversation: Conversation):",
    "prompt": "import os\nimport re\nimport json\nfrom typing import Type\nfrom src.pathfinder import PathFinder\nfrom src.conversation import Conversation\n\n\nclass Logger:\n    def __init__(self, paths: PathFinder, log_level: int, log_format: str):\n        \"\"\"Logs conversations and saves data at the user's request\"\"\"\n        self.level: int = log_level\n        self.format: str = log_format\n        self.paths: Paths = paths\n        self.number: int = 0\n        self.file: str = ''\n        self.savefile: str = ''\n        self.save_number: int = 0\n        self.new_log()\n\n    @property\n    def level(self):\n        return self._level\n\n    @level.setter\n    def level(self, new_value: int):\n        if 1 != new_value != 2:\n            raise TypeError\n        else:\n            self._level = new_value\n\n    @property\n    def format(self):\n        return self._format\n\n    @format.setter\n    def format(self, new_value: str):\n        if new_value == 'txt' or new_value == 'json':\n            self._format = new_value\n        else:\n            self._format = new_value\n\n    def new_log(self):\n        self.number = self._next_number()\n        self.file = self._new_file()\n        \n    def _next_number(self):\n        \"\"\"Fetch the next log number from config.json and updates it\"\"\"\n        config_data = self._load(self.paths.config)\n        self.number = log_num = config_data['log_number']\n        config_data['log_number'] = self.number + 1\n        self._dump(config_data, self.paths.config)\n        return self.number\n    \n    def _new_file(self):\n        \"\"\"Generates a new logfile relative the current log number\"\"\"\n        while True:  # to prevent inadvertently overwriting logs if the value is changed in config.json\n            self.file = f'{self.paths.logs}/log{self.number}.{self.format}'\n            try:\n                with open(self.file, 'x'):\n                    print(f'[*] logfile generated ~ {self.file}')\n                return self.file\n            except FileExistsError:\n                self.number += 1\n",
    "prefix": "import os\nimport re\nimport json\nfrom typing import Type\nfrom src.pathfinder import PathFinder\nfrom src.conversation import Conversation\n\n\nclass Logger:\n    def __init__(self, paths: PathFinder, log_level: int, log_format: str):\n        \"\"\"Logs conversations and saves data at the user's request\"\"\"\n        self.level: int = log_level\n        self.format: str = log_format\n        self.paths: Paths = paths\n        self.number: int = 0\n        self.file: str = ''\n        self.savefile: str = ''\n        self.save_number: int = 0\n        self.new_log()\n\n    @property\n    def level(self):\n        return self._level\n\n    @level.setter\n    def level(self, new_value: int):\n        if 1 != new_value != 2:\n            raise TypeError\n        else:\n            self._level = new_value\n\n    @property\n    def format(self):\n        return self._format\n\n    @format.setter\n    def format(self, new_value: str):\n        if new_value == 'txt' or new_value == 'json':\n            self._format = new_value\n        else:\n            self._format = new_value\n\n    def new_log(self):\n        self.number = self._next_number()\n        self.file = self._new_file()\n        \n    def _next_number(self):\n        \"\"\"Fetch the next log number from config.json and updates it\"\"\"\n        config_data = self._load(self.paths.config)\n        self.number = log_num = config_data['log_number']\n        config_data['log_number'] = self.number + 1\n        self._dump(config_data, self.paths.config)\n        return self.number\n    \n    def _new_file(self):\n        \"\"\"Generates a new logfile relative the current log number\"\"\"\n        while True:  # to prevent inadvertently overwriting logs if the value is changed in config.json\n            self.file = f'{self.paths.logs}/log{self.number}.{self.format}'\n            try:\n                with open(self.file, 'x'):\n                    print(f'[*] logfile generated ~ {self.file}')\n                return self.file\n            except FileExistsError:\n                self.number += 1\n",
    "suffix": ""
  },
  {
    "name": "ccurme/chesster:chesster/app/board_manager.py@974",
    "canonical_solution": "                        \"board\": serialize_board_state_with_last_move(",
    "prompt": "import os\nimport urllib\nimport chess\nfrom typing import Iterator\nfrom fastapi import WebSocket, WebSocketDisconnect\nfrom langserve import RemoteRunnable\nfrom chesster.app.utils import (\n    display_board,\n    get_engine_score,\n    serialize_board_state_with_last_move,\n)\n\n\n\n\nLANGSERVE_HOST = os.getenv(\"LANGSERVE_HOST\", \"localhost\")\nLANGSERVE_SECRET = os.getenv(\"LANGSERVE_SECRET\", \"secret\")\nCHAT_HISTORY_LENGTH = 50  # Number of most recent (human, ai) exchanges to retain.\n\n\nclass BoardManager:\n    def __init__(self):\n        self.active_websockets: list[WebSocket] = []\n        self.last_updated_image = None\n        self.board = chess.Board()\n        self.player_side = chess.WHITE\n        self.interesting_move_iterator = None\n        self.chat_history = []\n        self.remote_runnable = RemoteRunnable(\n            f\"http://{LANGSERVE_HOST}:8001/chesster\", headers={\"x-token\": LANGSERVE_SECRET}\n        )\n\n    async def set_board(self, board: chess.Board) -> None:\n        \"\"\"Set board.\"\"\"\n        self.board = board\n        await self.update_board(self.board)\n\n    async def set_player_side(self, player_side: chess.Color) -> None:\n        \"\"\"Set player side.\"\"\"\n        self.player_side = player_side\n        await self.update_board(self.board)\n\n    async def set_interesting_move_iterator(self) -> None:\n        \"\"\"Calculate interesting moves in board's move stack.\"\"\"\n        self.interesting_move_iterator = self._interesting_move_iterator()\n\n    async def make_move(self, move: chess.Move) -> None:\n        \"\"\"Parse move and update board.\"\"\"\n        self.board.push(move)\n        await self.update_board(self.board)\n\n    async def _interesting_move_iterator(\n        self, centipawn_threshold: int = 100\n    ) -> Iterator[chess.Board]:\n        \"\"\"Make iterator over interesting moves according to Chess engine.\"\"\"\n        new_board = chess.Board()\n        centipawns = 0\n        for move in self.board.move_stack:\n            new_board.push(move)\n            new_centipawns = get_engine_score(new_board, self.player_side)\n            if new_centipawns is None:\n                continue\n            delta = new_centipawns - centipawns\n            if new_board.turn != self.player_side:  # player just moved\n                if abs(delta) > centipawn_threshold:\n                    await self.update_board(new_board)\n                    yield {",
    "prefix": "import os\nimport urllib\nimport chess\nfrom typing import Iterator\nfrom fastapi import WebSocket, WebSocketDisconnect\nfrom langserve import RemoteRunnable\nfrom chesster.app.utils import (\n    display_board,\n    get_engine_score,\n    serialize_board_state_with_last_move,\n)\n\n\n\n\nLANGSERVE_HOST = os.getenv(\"LANGSERVE_HOST\", \"localhost\")\nLANGSERVE_SECRET = os.getenv(\"LANGSERVE_SECRET\", \"secret\")\nCHAT_HISTORY_LENGTH = 50  # Number of most recent (human, ai) exchanges to retain.\n\n\nclass BoardManager:\n    def __init__(self):\n        self.active_websockets: list[WebSocket] = []\n        self.last_updated_image = None\n        self.board = chess.Board()\n        self.player_side = chess.WHITE\n        self.interesting_move_iterator = None\n        self.chat_history = []\n        self.remote_runnable = RemoteRunnable(\n            f\"http://{LANGSERVE_HOST}:8001/chesster\", headers={\"x-token\": LANGSERVE_SECRET}\n        )\n\n    async def set_board(self, board: chess.Board) -> None:\n        \"\"\"Set board.\"\"\"\n        self.board = board\n        await self.update_board(self.board)\n\n    async def set_player_side(self, player_side: chess.Color) -> None:\n        \"\"\"Set player side.\"\"\"\n        self.player_side = player_side\n        await self.update_board(self.board)\n\n    async def set_interesting_move_iterator(self) -> None:\n        \"\"\"Calculate interesting moves in board's move stack.\"\"\"\n        self.interesting_move_iterator = self._interesting_move_iterator()\n\n    async def make_move(self, move: chess.Move) -> None:\n        \"\"\"Parse move and update board.\"\"\"\n        self.board.push(move)\n        await self.update_board(self.board)\n\n    async def _interesting_move_iterator(\n        self, centipawn_threshold: int = 100\n    ) -> Iterator[chess.Board]:\n        \"\"\"Make iterator over interesting moves according to Chess engine.\"\"\"\n        new_board = chess.Board()\n        centipawns = 0\n        for move in self.board.move_stack:\n            new_board.push(move)\n            new_centipawns = get_engine_score(new_board, self.player_side)\n            if new_centipawns is None:\n                continue\n            delta = new_centipawns - centipawns\n            if new_board.turn != self.player_side:  # player just moved\n                if abs(delta) > centipawn_threshold:\n                    await self.update_board(new_board)\n                    yield {",
    "suffix": ""
  },
  {
    "name": "zkarpinski/codeinsight-sdk-python:tests/test_client.py@1265",
    "canonical_solution": "            with pytest.raises(CodeInsightError):",
    "prompt": "import pytest\nimport logging\nimport requests_mock\nfrom codeinsight_sdk import CodeInsightClient\nfrom codeinsight_sdk.exceptions import CodeInsightError\n\n\n\n\nlogger = logging.getLogger(__name__)\n\n## CHANGE ME ##\nTEST_URL = \"https://api.revenera.com\"\nTEST_API_TOKEN = \"your_api_token\"\n\nclass TestCodeInsightClient:\n    @pytest.fixture\n    def client(self):\n        return CodeInsightClient(TEST_URL, TEST_API_TOKEN)\n    \n    def test_client(self, client):\n        assert client.base_url == TEST_URL\n\n    def test_endpoint_not_found(self, client):\n        with requests_mock.Mocker() as m:\n            m.get(f\"{TEST_URL}/codeinsight/api/projects\", status_code=404)\n            with pytest.raises(Exception):\n                client.projects.all()\n\nclass TestProjectEndpoints:\n    @pytest.fixture\n    def client(self):\n        return CodeInsightClient(TEST_URL, TEST_API_TOKEN)\n    \n    def test_create_project(self, client):\n        project_name = \"Test\"\n        with requests_mock.Mocker() as m:\n            m.post(f\"{TEST_URL}/codeinsight/api/projects\", text='{\"data\": {\"id\":1}}')\n            project_id = client.projects.create(project_name)\n        assert project_id == 1\n   \n    def test_get_all_projects(self, client):\n        with requests_mock.Mocker() as m:\n            m.get(f\"{TEST_URL}/codeinsight/api/projects\", text='{\"data\": [{\"id\":1, \"name\":\"Test\"}, {\"id\":2, \"name\":\"Test 2\"}]}')\n            projects = client.projects.all()\n        assert len(projects) > 0\n\n    def test_get_project_id(self, client):\n        project_name = \"Test\"\n        with requests_mock.Mocker() as m:\n            m.get(f\"{TEST_URL}/codeinsight/api/project/id\", text='{ \"Content: \": 1 }') # Yes, the key is called 'Content: ' ...\n            project_id = client.projects.get_id(project_name)\n        assert project_id == 1\n\n    def test_get_project_id_invalid(self,client):\n        project_name = \"Invalid_Project\"\n        fake_response_json = \"\"\"{ \"Arguments: \" : [\"\",\"\"],\n            \"Key: \": \" InvalidProjectNameParm\",\n            \"Error: \": \"The project name entered was not found\" }\n        \"\"\"\n        with requests_mock.Mocker() as m:\n            # Note, the key names end with a colon and space '...: ' \n            m.get(f\"{TEST_URL}/codeinsight/api/project/id\", text=fake_response_json, status_code=400)",
    "prefix": "import pytest\nimport logging\nimport requests_mock\nfrom codeinsight_sdk import CodeInsightClient\nfrom codeinsight_sdk.exceptions import CodeInsightError\n\n\n\n\nlogger = logging.getLogger(__name__)\n\n## CHANGE ME ##\nTEST_URL = \"https://api.revenera.com\"\nTEST_API_TOKEN = \"your_api_token\"\n\nclass TestCodeInsightClient:\n    @pytest.fixture\n    def client(self):\n        return CodeInsightClient(TEST_URL, TEST_API_TOKEN)\n    \n    def test_client(self, client):\n        assert client.base_url == TEST_URL\n\n    def test_endpoint_not_found(self, client):\n        with requests_mock.Mocker() as m:\n            m.get(f\"{TEST_URL}/codeinsight/api/projects\", status_code=404)\n            with pytest.raises(Exception):\n                client.projects.all()\n\nclass TestProjectEndpoints:\n    @pytest.fixture\n    def client(self):\n        return CodeInsightClient(TEST_URL, TEST_API_TOKEN)\n    \n    def test_create_project(self, client):\n        project_name = \"Test\"\n        with requests_mock.Mocker() as m:\n            m.post(f\"{TEST_URL}/codeinsight/api/projects\", text='{\"data\": {\"id\":1}}')\n            project_id = client.projects.create(project_name)\n        assert project_id == 1\n   \n    def test_get_all_projects(self, client):\n        with requests_mock.Mocker() as m:\n            m.get(f\"{TEST_URL}/codeinsight/api/projects\", text='{\"data\": [{\"id\":1, \"name\":\"Test\"}, {\"id\":2, \"name\":\"Test 2\"}]}')\n            projects = client.projects.all()\n        assert len(projects) > 0\n\n    def test_get_project_id(self, client):\n        project_name = \"Test\"\n        with requests_mock.Mocker() as m:\n            m.get(f\"{TEST_URL}/codeinsight/api/project/id\", text='{ \"Content: \": 1 }') # Yes, the key is called 'Content: ' ...\n            project_id = client.projects.get_id(project_name)\n        assert project_id == 1\n\n    def test_get_project_id_invalid(self,client):\n        project_name = \"Invalid_Project\"\n        fake_response_json = \"\"\"{ \"Arguments: \" : [\"\",\"\"],\n            \"Key: \": \" InvalidProjectNameParm\",\n            \"Error: \": \"The project name entered was not found\" }\n        \"\"\"\n        with requests_mock.Mocker() as m:\n            # Note, the key names end with a colon and space '...: ' \n            m.get(f\"{TEST_URL}/codeinsight/api/project/id\", text=fake_response_json, status_code=400)",
    "suffix": ""
  },
  {
    "name": "chebupelka8/Engine:scripts/loop.py@951",
    "canonical_solution": "        pygame.display.set_icon(Image(\"Engine/assets/icon.png\").image)\r",
    "prompt": "import pygame, sys\r\nfrom pygame.locals import *\r\nfrom .math import Vec2\r\nfrom .image import Image\r\n\r\n\r\nclass WindowLoop:\r\n\r\n    def __init__(self, __size: Vec2, fps: int = 144) -> None:\r\n        pygame.init()\r\n\r\n        self.__display = pygame.display.set_mode((__size.x, __size.y))\r\n        pygame.display.set_caption(\"Engine: v0.1\")\r",
    "prefix": "import pygame, sys\r\nfrom pygame.locals import *\r\nfrom .math import Vec2\r\nfrom .image import Image\r\n\r\n\r\nclass WindowLoop:\r\n\r\n    def __init__(self, __size: Vec2, fps: int = 144) -> None:\r\n        pygame.init()\r\n\r\n        self.__display = pygame.display.set_mode((__size.x, __size.y))\r\n        pygame.display.set_caption(\"Engine: v0.1\")\r",
    "suffix": ""
  },
  {
    "name": "lxbme/TSPLifesaver:TSPLifesaver/tools.py@1502",
    "canonical_solution": "            opt = SimulatedAnnealing(route, temperature=temperature,",
    "prompt": "from typing import Iterable, MutableSequence, Type\nfrom random import shuffle\nfrom copy import deepcopy\nfrom TSPLifesaver.abc import AbstractRoute, AbstractPoint\nfrom TSPLifesaver.structure import BasicRoute, PointWithEuclideanDistance\nfrom TSPLifesaver.optimizer import SimulatedAnnealing\n\n\n\ndef route_from_sequence(sequence: Iterable[MutableSequence], route: AbstractRoute = BasicRoute([]),\n                        point_class: Type[AbstractPoint] = PointWithEuclideanDistance,\n                        name_offset: int = 1, ) -> AbstractRoute:\n    \"\"\"\n    :param route: Instances of the AbstractRoute class or its subclasses, defaults to empty instance of BasicRoute\n    :param name_offset: Index of the name\n    :param sequence: Sequence containing coordinates\n    :param point_class: AbstractPoint or its subclasses ,defaults to PointWithEuclideanDistance\n    :return: a new route\n    \"\"\"\n    index = name_offset\n\n    for pos in sequence:\n        try:\n            point = point_class(pos, name=f\"{index}\")\n        except:\n            point = point_class(pos)\n\n        route.append(point)\n        index += 1\n\n    return route\n\n\ndef simulated_annealing(route: AbstractRoute, epoch: int = 100, temperature: float = 10000,\n                        cooling_rate: float = 0.03, min_temperature: float = 1,\n                        log: bool = False) -> AbstractRoute:\n    \"\"\"\n    :param route: Instances of the AbstractRoute class or its subclasses\n    :param epoch: Number of epochs to simulate, defaults to 100\n    :param temperature: Temperature of the annealing, defaults to 10000\n    :param cooling_rate: Cooling rate of the annealing, defaults to 0.03\n    :param min_temperature: Minimum temperature of the annealing, defaults to 1\n    :param log: Whether to print the log of the annealing, defaults to False\n    :return: optimized route\n    \"\"\"\n    if len(route):\n        best_route = deepcopy(route)\n        for i in range(epoch):\n            if log:\n                print(f\"Running epoch {i} of {epoch}\")\n            shuffle(route)",
    "prefix": "from typing import Iterable, MutableSequence, Type\nfrom random import shuffle\nfrom copy import deepcopy\nfrom TSPLifesaver.abc import AbstractRoute, AbstractPoint\nfrom TSPLifesaver.structure import BasicRoute, PointWithEuclideanDistance\nfrom TSPLifesaver.optimizer import SimulatedAnnealing\n\n\n\ndef route_from_sequence(sequence: Iterable[MutableSequence], route: AbstractRoute = BasicRoute([]),\n                        point_class: Type[AbstractPoint] = PointWithEuclideanDistance,\n                        name_offset: int = 1, ) -> AbstractRoute:\n    \"\"\"\n    :param route: Instances of the AbstractRoute class or its subclasses, defaults to empty instance of BasicRoute\n    :param name_offset: Index of the name\n    :param sequence: Sequence containing coordinates\n    :param point_class: AbstractPoint or its subclasses ,defaults to PointWithEuclideanDistance\n    :return: a new route\n    \"\"\"\n    index = name_offset\n\n    for pos in sequence:\n        try:\n            point = point_class(pos, name=f\"{index}\")\n        except:\n            point = point_class(pos)\n\n        route.append(point)\n        index += 1\n\n    return route\n\n\ndef simulated_annealing(route: AbstractRoute, epoch: int = 100, temperature: float = 10000,\n                        cooling_rate: float = 0.03, min_temperature: float = 1,\n                        log: bool = False) -> AbstractRoute:\n    \"\"\"\n    :param route: Instances of the AbstractRoute class or its subclasses\n    :param epoch: Number of epochs to simulate, defaults to 100\n    :param temperature: Temperature of the annealing, defaults to 10000\n    :param cooling_rate: Cooling rate of the annealing, defaults to 0.03\n    :param min_temperature: Minimum temperature of the annealing, defaults to 1\n    :param log: Whether to print the log of the annealing, defaults to False\n    :return: optimized route\n    \"\"\"\n    if len(route):\n        best_route = deepcopy(route)\n        for i in range(epoch):\n            if log:\n                print(f\"Running epoch {i} of {epoch}\")\n            shuffle(route)",
    "suffix": ""
  },
  {
    "name": "DLYuanGod/TinyGPT-V:minigpt4/datasets/datasets/coco_caption.py@1158",
    "canonical_solution": "class NoCapsEvalDataset(CaptionEvalDataset):",
    "prompt": "import os\nimport json\nimport torch\nimport numpy as np\nimport time\nfrom PIL import Image\nfrom PIL import ImageFile\nfrom tqdm import tqdm\nfrom minigpt4.datasets.datasets.caption_datasets import COCOCaptionDataset, CaptionEvalDataset\n\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n\nCOCOCapDataset = COCOCaptionDataset\n\n\n\n\n\nclass COCOCapEvalDataset(CaptionEvalDataset):\n    def __init__(self, vis_processor, text_processor, vis_root, ann_paths):\n        \"\"\"\n        vis_root (string): Root directory of images (e.g. coco/images/)\n        ann_root (string): directory to store the annotation file\n        split (string): val or test\n        \"\"\"\n        super().__init__(vis_processor, text_processor, vis_root, ann_paths)\n\n    def __getitem__(self, index):\n        ann = self.annotation[index]\n\n        image_path = os.path.join(self.vis_root, ann[\"image\"])\n        image = Image.open(image_path).convert(\"RGB\")\n\n        image = self.vis_processor(image)\n\n        img_id = ann[\"image\"].split(\"/\")[-1].strip(\".jpg\").split(\"_\")[-1]\n\n        return {\n            \"image\": image,\n            \"image_id\": img_id,\n            \"instance_id\": ann[\"instance_id\"],\n        }\n\n",
    "prefix": "import os\nimport json\nimport torch\nimport numpy as np\nimport time\nfrom PIL import Image\nfrom PIL import ImageFile\nfrom tqdm import tqdm\nfrom minigpt4.datasets.datasets.caption_datasets import COCOCaptionDataset, CaptionEvalDataset\n\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n\nCOCOCapDataset = COCOCaptionDataset\n\n\n\n\n\nclass COCOCapEvalDataset(CaptionEvalDataset):\n    def __init__(self, vis_processor, text_processor, vis_root, ann_paths):\n        \"\"\"\n        vis_root (string): Root directory of images (e.g. coco/images/)\n        ann_root (string): directory to store the annotation file\n        split (string): val or test\n        \"\"\"\n        super().__init__(vis_processor, text_processor, vis_root, ann_paths)\n\n    def __getitem__(self, index):\n        ann = self.annotation[index]\n\n        image_path = os.path.join(self.vis_root, ann[\"image\"])\n        image = Image.open(image_path).convert(\"RGB\")\n\n        image = self.vis_processor(image)\n\n        img_id = ann[\"image\"].split(\"/\")[-1].strip(\".jpg\").split(\"_\")[-1]\n\n        return {\n            \"image\": image,\n            \"image_id\": img_id,\n            \"instance_id\": ann[\"instance_id\"],\n        }\n\n",
    "suffix": ""
  },
  {
    "name": "jianchang512/vocal-separate:start.py@1592",
    "canonical_solution": "            threading.Thread(target=tool.openweb, args=(cfg.web_address,)).start()",
    "prompt": "import logging\nimport threading\nimport sys\nimport os\nimport subprocess\nfrom flask import Flask, request, render_template, jsonify, send_from_directory\nfrom gevent.pywsgi import WSGIServer, WSGIHandler,LoggingLogAdapter\nfrom logging.handlers import RotatingFileHandler\nfrom vocal import cfg, tool\nfrom vocal.cfg import ROOT_DIR\nfrom spleeter.separator import Separator\n\nclass CustomRequestHandler(WSGIHandler):\n    def log_request(self):\n        pass\n\n# \u7981\u7528 Werkzeug \u9ed8\u8ba4\u7684\u65e5\u5fd7\u5904\u7406\u5668\nlog = logging.getLogger('werkzeug')\nlog.handlers[:] = []\nlog.setLevel(logging.WARNING)\n\napp = Flask(__name__, static_folder=os.path.join(ROOT_DIR, 'static'), static_url_path='/static',\n            template_folder=os.path.join(ROOT_DIR, 'templates'))\nroot_log = logging.getLogger()  # Flask\u7684\u6839\u65e5\u5fd7\u8bb0\u5f55\u5668\nroot_log.handlers = []\nroot_log.setLevel(logging.WARNING)\n\n# \u914d\u7f6e\u65e5\u5fd7\napp.logger.setLevel(logging.WARNING)  # \u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b\u4e3a INFO\n# \u521b\u5efa RotatingFileHandler \u5bf9\u8c61\uff0c\u8bbe\u7f6e\u5199\u5165\u7684\u6587\u4ef6\u8def\u5f84\u548c\u5927\u5c0f\u9650\u5236\nfile_handler = RotatingFileHandler(os.path.join(ROOT_DIR, 'vocal.log'), maxBytes=1024 * 1024, backupCount=5)\n# \u521b\u5efa\u65e5\u5fd7\u7684\u683c\u5f0f\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n# \u8bbe\u7f6e\u6587\u4ef6\u5904\u7406\u5668\u7684\u7ea7\u522b\u548c\u683c\u5f0f\nfile_handler.setLevel(logging.WARNING)\nfile_handler.setFormatter(formatter)\n# \u5c06\u6587\u4ef6\u5904\u7406\u5668\u6dfb\u52a0\u5230\u65e5\u5fd7\u8bb0\u5f55\u5668\u4e2d\napp.logger.addHandler(file_handler)\n\n\n@app.route('/static/<path:filename>')\ndef static_files(filename):\n    return send_from_directory(app.config['STATIC_FOLDER'], filename)\n\n@app.route('/')\ndef index():\n    return render_template(\"index.html\",cuda=cfg.cuda, language=cfg.LANG,root_dir=ROOT_DIR.replace('\\\\', '/'))\n\n\n# \u4e0a\u4f20\u97f3\u9891\n@app.route('/upload', methods=['POST'])\ndef upload():\n    try:\n        # \u83b7\u53d6\u4e0a\u4f20\u7684\u6587\u4ef6\n        audio_file = request.files['audio']\n        # \u5982\u679c\u662fmp4\n        noextname, ext = os.path.splitext(audio_file.filename)\n        ext = ext.lower()\n        # \u5982\u679c\u662f\u89c6\u9891\uff0c\u5148\u5206\u79bb\n        wav_file = os.path.join(cfg.TMP_DIR, f'{noextname}.wav')\n        if os.path.exists(wav_file) and os.path.getsize(wav_file) > 0:\n            return jsonify({'code': 0, 'msg': cfg.transobj['lang1'], \"data\": os.path.basename(wav_file)})\n        msg=\"\"\n        if ext in ['.mp4', '.mov', '.avi', '.mkv', '.mpeg', '.mp3', '.flac']:\n            video_file = os.path.join(cfg.TMP_DIR, f'{noextname}{ext}')\n            audio_file.save(video_file)\n            params = [\n                \"-i\",\n                video_file,\n            ]\n            if ext not in ['.mp3', '.flac']:\n                params.append('-vn')\n            params.append(wav_file)\n            rs = tool.runffmpeg(params)\n            if rs != 'ok':\n                return jsonify({\"code\": 1, \"msg\": rs})\n            msg=\",\"+cfg.transobj['lang9']\n        elif ext == '.wav':\n            audio_file.save(wav_file)\n        else:\n            return jsonify({\"code\": 1, \"msg\": f\"{cfg.transobj['lang3']} {ext}\"})\n        \n        # \u8fd4\u56de\u6210\u529f\u7684\u54cd\u5e94\n        return jsonify({'code': 0, 'msg': cfg.transobj['lang1']+msg, \"data\": os.path.basename(wav_file)})\n    except Exception as e:\n        app.logger.error(f'[upload]error: {e}')\n        return jsonify({'code': 2, 'msg': cfg.transobj['lang2']})\n\n\n# \u6839\u636e\u6587\u672c\u8fd4\u56detts\u7ed3\u679c\uff0c\u8fd4\u56de name=\u6587\u4ef6\u540d\u5b57\uff0cfilename=\u6587\u4ef6\u7edd\u5bf9\u8def\u5f84\n# \u8bf7\u6c42\u7aef\u6839\u636e\u9700\u8981\u81ea\u884c\u9009\u62e9\u4f7f\u7528\u54ea\u4e2a\n# params\n# wav_name:tmp\u4e0b\u7684wav\u6587\u4ef6\n# model \u6a21\u578b\u540d\u79f0\n@app.route('/process', methods=['GET', 'POST'])\ndef process():\n    # \u539f\u59cb\u5b57\u7b26\u4e32\n    wav_name = request.form.get(\"wav_name\").strip()\n    model = request.form.get(\"model\")\n    wav_file = os.path.join(cfg.TMP_DIR, wav_name)\n    noextname = wav_name[:-4]\n    if not os.path.exists(wav_file):\n        return jsonify({\"code\": 1, \"msg\": f\"{wav_file} {cfg.langlist['lang5']}\"})\n    if not os.path.exists(os.path.join(cfg.MODEL_DIR, model, 'model.meta')):\n        return jsonify({\"code\": 1, \"msg\": f\"{model} {cfg.transobj['lang4']}\"})\n    try:\n        p=subprocess.run(['ffprobe','-v','error','-show_entries',\"format=duration\",'-of', \"default=noprint_wrappers=1:nokey=1\", wav_file], capture_output=True)      \n        if p.returncode==0:\n            sec=float(p.stdout)  \n    except:\n        sec=1800\n    print(f'{sec=}')\n    separator = Separator(f'spleeter:{model}', multiprocess=False)\n    dirname = os.path.join(cfg.FILES_DIR, noextname)\n    try:\n        separator.separate_to_file(wav_file, destination=dirname, filename_format=\"{instrument}.{codec}\", duration=sec)\n    except Exception as e:\n        return jsonify({\"code\": 1, \"msg\": str(e)})\n    status={\n        \"accompaniment\":\"\u4f34\u594f\",\n        \"bass\":\"\u4f4e\u97f3\",\n        \"drums\":\"\u9f13\",\n        \"piano\":\"\u7434\",\n        \"vocals\":\"\u4eba\u58f0\",\n        \"other\":\"\u5176\u4ed6\"\n    }\n    data = []\n    urllist = []\n    for it in os.listdir(dirname):\n        if it.endswith('.wav'):\n            data.append( status[it[:-4]] if cfg.LANG=='zh' else it[:-4])\n            urllist.append(f'http://{cfg.web_address}/static/files/{noextname}/{it}')\n\n    return jsonify({\"code\": 0, \"msg\": cfg.transobj['lang6'], \"data\": data, \"urllist\": urllist,\"dirname\":dirname})\n\n\n@app.route('/checkupdate', methods=['GET', 'POST'])\ndef checkupdate():\n    return jsonify({'code': 0, \"msg\": cfg.updatetips})\n\n\nif __name__ == '__main__':\n    http_server = None\n    try:\n        threading.Thread(target=tool.checkupdate).start()        \n        try:\n            host = cfg.web_address.split(':')\n            http_server = WSGIServer((host[0], int(host[1])), app ,handler_class=CustomRequestHandler)",
    "prefix": "import logging\nimport threading\nimport sys\nimport os\nimport subprocess\nfrom flask import Flask, request, render_template, jsonify, send_from_directory\nfrom gevent.pywsgi import WSGIServer, WSGIHandler,LoggingLogAdapter\nfrom logging.handlers import RotatingFileHandler\nfrom vocal import cfg, tool\nfrom vocal.cfg import ROOT_DIR\nfrom spleeter.separator import Separator\n\nclass CustomRequestHandler(WSGIHandler):\n    def log_request(self):\n        pass\n\n# \u7981\u7528 Werkzeug \u9ed8\u8ba4\u7684\u65e5\u5fd7\u5904\u7406\u5668\nlog = logging.getLogger('werkzeug')\nlog.handlers[:] = []\nlog.setLevel(logging.WARNING)\n\napp = Flask(__name__, static_folder=os.path.join(ROOT_DIR, 'static'), static_url_path='/static',\n            template_folder=os.path.join(ROOT_DIR, 'templates'))\nroot_log = logging.getLogger()  # Flask\u7684\u6839\u65e5\u5fd7\u8bb0\u5f55\u5668\nroot_log.handlers = []\nroot_log.setLevel(logging.WARNING)\n\n# \u914d\u7f6e\u65e5\u5fd7\napp.logger.setLevel(logging.WARNING)  # \u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b\u4e3a INFO\n# \u521b\u5efa RotatingFileHandler \u5bf9\u8c61\uff0c\u8bbe\u7f6e\u5199\u5165\u7684\u6587\u4ef6\u8def\u5f84\u548c\u5927\u5c0f\u9650\u5236\nfile_handler = RotatingFileHandler(os.path.join(ROOT_DIR, 'vocal.log'), maxBytes=1024 * 1024, backupCount=5)\n# \u521b\u5efa\u65e5\u5fd7\u7684\u683c\u5f0f\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n# \u8bbe\u7f6e\u6587\u4ef6\u5904\u7406\u5668\u7684\u7ea7\u522b\u548c\u683c\u5f0f\nfile_handler.setLevel(logging.WARNING)\nfile_handler.setFormatter(formatter)\n# \u5c06\u6587\u4ef6\u5904\u7406\u5668\u6dfb\u52a0\u5230\u65e5\u5fd7\u8bb0\u5f55\u5668\u4e2d\napp.logger.addHandler(file_handler)\n\n\n@app.route('/static/<path:filename>')\ndef static_files(filename):\n    return send_from_directory(app.config['STATIC_FOLDER'], filename)\n\n@app.route('/')\ndef index():\n    return render_template(\"index.html\",cuda=cfg.cuda, language=cfg.LANG,root_dir=ROOT_DIR.replace('\\\\', '/'))\n\n\n# \u4e0a\u4f20\u97f3\u9891\n@app.route('/upload', methods=['POST'])\ndef upload():\n    try:\n        # \u83b7\u53d6\u4e0a\u4f20\u7684\u6587\u4ef6\n        audio_file = request.files['audio']\n        # \u5982\u679c\u662fmp4\n        noextname, ext = os.path.splitext(audio_file.filename)\n        ext = ext.lower()\n        # \u5982\u679c\u662f\u89c6\u9891\uff0c\u5148\u5206\u79bb\n        wav_file = os.path.join(cfg.TMP_DIR, f'{noextname}.wav')\n        if os.path.exists(wav_file) and os.path.getsize(wav_file) > 0:\n            return jsonify({'code': 0, 'msg': cfg.transobj['lang1'], \"data\": os.path.basename(wav_file)})\n        msg=\"\"\n        if ext in ['.mp4', '.mov', '.avi', '.mkv', '.mpeg', '.mp3', '.flac']:\n            video_file = os.path.join(cfg.TMP_DIR, f'{noextname}{ext}')\n            audio_file.save(video_file)\n            params = [\n                \"-i\",\n                video_file,\n            ]\n            if ext not in ['.mp3', '.flac']:\n                params.append('-vn')\n            params.append(wav_file)\n            rs = tool.runffmpeg(params)\n            if rs != 'ok':\n                return jsonify({\"code\": 1, \"msg\": rs})\n            msg=\",\"+cfg.transobj['lang9']\n        elif ext == '.wav':\n            audio_file.save(wav_file)\n        else:\n            return jsonify({\"code\": 1, \"msg\": f\"{cfg.transobj['lang3']} {ext}\"})\n        \n        # \u8fd4\u56de\u6210\u529f\u7684\u54cd\u5e94\n        return jsonify({'code': 0, 'msg': cfg.transobj['lang1']+msg, \"data\": os.path.basename(wav_file)})\n    except Exception as e:\n        app.logger.error(f'[upload]error: {e}')\n        return jsonify({'code': 2, 'msg': cfg.transobj['lang2']})\n\n\n# \u6839\u636e\u6587\u672c\u8fd4\u56detts\u7ed3\u679c\uff0c\u8fd4\u56de name=\u6587\u4ef6\u540d\u5b57\uff0cfilename=\u6587\u4ef6\u7edd\u5bf9\u8def\u5f84\n# \u8bf7\u6c42\u7aef\u6839\u636e\u9700\u8981\u81ea\u884c\u9009\u62e9\u4f7f\u7528\u54ea\u4e2a\n# params\n# wav_name:tmp\u4e0b\u7684wav\u6587\u4ef6\n# model \u6a21\u578b\u540d\u79f0\n@app.route('/process', methods=['GET', 'POST'])\ndef process():\n    # \u539f\u59cb\u5b57\u7b26\u4e32\n    wav_name = request.form.get(\"wav_name\").strip()\n    model = request.form.get(\"model\")\n    wav_file = os.path.join(cfg.TMP_DIR, wav_name)\n    noextname = wav_name[:-4]\n    if not os.path.exists(wav_file):\n        return jsonify({\"code\": 1, \"msg\": f\"{wav_file} {cfg.langlist['lang5']}\"})\n    if not os.path.exists(os.path.join(cfg.MODEL_DIR, model, 'model.meta')):\n        return jsonify({\"code\": 1, \"msg\": f\"{model} {cfg.transobj['lang4']}\"})\n    try:\n        p=subprocess.run(['ffprobe','-v','error','-show_entries',\"format=duration\",'-of', \"default=noprint_wrappers=1:nokey=1\", wav_file], capture_output=True)      \n        if p.returncode==0:\n            sec=float(p.stdout)  \n    except:\n        sec=1800\n    print(f'{sec=}')\n    separator = Separator(f'spleeter:{model}', multiprocess=False)\n    dirname = os.path.join(cfg.FILES_DIR, noextname)\n    try:\n        separator.separate_to_file(wav_file, destination=dirname, filename_format=\"{instrument}.{codec}\", duration=sec)\n    except Exception as e:\n        return jsonify({\"code\": 1, \"msg\": str(e)})\n    status={\n        \"accompaniment\":\"\u4f34\u594f\",\n        \"bass\":\"\u4f4e\u97f3\",\n        \"drums\":\"\u9f13\",\n        \"piano\":\"\u7434\",\n        \"vocals\":\"\u4eba\u58f0\",\n        \"other\":\"\u5176\u4ed6\"\n    }\n    data = []\n    urllist = []\n    for it in os.listdir(dirname):\n        if it.endswith('.wav'):\n            data.append( status[it[:-4]] if cfg.LANG=='zh' else it[:-4])\n            urllist.append(f'http://{cfg.web_address}/static/files/{noextname}/{it}')\n\n    return jsonify({\"code\": 0, \"msg\": cfg.transobj['lang6'], \"data\": data, \"urllist\": urllist,\"dirname\":dirname})\n\n\n@app.route('/checkupdate', methods=['GET', 'POST'])\ndef checkupdate():\n    return jsonify({'code': 0, \"msg\": cfg.updatetips})\n\n\nif __name__ == '__main__':\n    http_server = None\n    try:\n        threading.Thread(target=tool.checkupdate).start()        \n        try:\n            host = cfg.web_address.split(':')\n            http_server = WSGIServer((host[0], int(host[1])), app ,handler_class=CustomRequestHandler)",
    "suffix": ""
  },
  {
    "name": "jiawei-ren/dreamgaussian4d:diffusers/src/diffusers/pipelines/stable_diffusion_xl/pipeline_output.py@1133",
    "canonical_solution": "    class FlaxStableDiffusionXLPipelineOutput(BaseOutput):",
    "prompt": "from dataclasses import dataclass\nfrom typing import List, Union\nfrom ...utils import BaseOutput, is_flax_available\nimport numpy as np\nimport PIL.Image\n    import flax\n\n\n\n\n@dataclass\nclass StableDiffusionXLPipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for Stable Diffusion pipelines.\n\n    Args:\n        images (`List[PIL.Image.Image]` or `np.ndarray`)\n            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n    \"\"\"\n\n    images: Union[List[PIL.Image.Image], np.ndarray]\n\n\nif is_flax_available():\n\n    @flax.struct.dataclass",
    "prefix": "from dataclasses import dataclass\nfrom typing import List, Union\nfrom ...utils import BaseOutput, is_flax_available\nimport numpy as np\nimport PIL.Image\n    import flax\n\n\n\n\n@dataclass\nclass StableDiffusionXLPipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for Stable Diffusion pipelines.\n\n    Args:\n        images (`List[PIL.Image.Image]` or `np.ndarray`)\n            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n    \"\"\"\n\n    images: Union[List[PIL.Image.Image], np.ndarray]\n\n\nif is_flax_available():\n\n    @flax.struct.dataclass",
    "suffix": ""
  },
  {
    "name": "Meituan-AutoML/MobileVLM:mobilevlm/model/mobilevlm.py@939",
    "canonical_solution": "        self.mm_projector = build_vision_projector(self.config)",
    "prompt": "import torch\nimport torch.nn as nn\nfrom abc import ABC, abstractmethod\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\nfrom mobilevlm.model.vision_encoder import build_vision_tower\nfrom mobilevlm.model.vision_projector import build_vision_projector\nfrom mobilevlm.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, \\\n    DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n    from mobilevlm.model.mobilellama import MobileLlamaForCausalLM\n\n\nclass MobileVLMMetaModel:\n\n    def __init__(self, config):\n        super(MobileVLMMetaModel, self).__init__(config)\n        if hasattr(config, \"mm_vision_tower\"):  \n            self.vision_tower = build_vision_tower(config, delay_load=False)\n            self.mm_projector = build_vision_projector(config)\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, 'vision_tower', None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None):\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n        self.config.mm_vision_tower = model_args.vision_tower\n        self.config.use_mm_proj = True\n        self.config.mm_projector_type = getattr(model_args, 'mm_projector_type', 'linear')\n        self.config.mm_vision_select_layer = mm_vision_select_layer\n        self.config.mm_vision_select_feature = mm_vision_select_feature\n        # Build VisionTower\n        vision_tower = build_vision_tower(model_args)\n        if fsdp is not None and len(fsdp) > 0:\n            self.vision_tower = [vision_tower]\n        else:\n            self.vision_tower = vision_tower\n        self.config.mm_hidden_size = vision_tower.hidden_size\n        # Build Vision-Projector",
    "prefix": "import torch\nimport torch.nn as nn\nfrom abc import ABC, abstractmethod\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\nfrom mobilevlm.model.vision_encoder import build_vision_tower\nfrom mobilevlm.model.vision_projector import build_vision_projector\nfrom mobilevlm.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, \\\n    DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n    from mobilevlm.model.mobilellama import MobileLlamaForCausalLM\n\n\nclass MobileVLMMetaModel:\n\n    def __init__(self, config):\n        super(MobileVLMMetaModel, self).__init__(config)\n        if hasattr(config, \"mm_vision_tower\"):  \n            self.vision_tower = build_vision_tower(config, delay_load=False)\n            self.mm_projector = build_vision_projector(config)\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, 'vision_tower', None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None):\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n        self.config.mm_vision_tower = model_args.vision_tower\n        self.config.use_mm_proj = True\n        self.config.mm_projector_type = getattr(model_args, 'mm_projector_type', 'linear')\n        self.config.mm_vision_select_layer = mm_vision_select_layer\n        self.config.mm_vision_select_feature = mm_vision_select_feature\n        # Build VisionTower\n        vision_tower = build_vision_tower(model_args)\n        if fsdp is not None and len(fsdp) > 0:\n            self.vision_tower = [vision_tower]\n        else:\n            self.vision_tower = vision_tower\n        self.config.mm_hidden_size = vision_tower.hidden_size\n        # Build Vision-Projector",
    "suffix": ""
  },
  {
    "name": "kinggongzilla/ai-clone-whatsapp:utils/config_utils.py@1559",
    "canonical_solution": "    assert train_config.peft_method in names, f\"Peft config not found: {train_config.peft_method}\"",
    "prompt": "import inspect\nimport torch.distributed as dist\nfrom dataclasses import asdict\nfrom torch.utils.data import DistributedSampler\nfrom peft import (\n    LoraConfig,\n    AdaptionPromptConfig,\n    PrefixTuningConfig,\n)\nfrom transformers import default_data_collator\nfrom transformers.data import DataCollatorForSeq2Seq\nfrom configs import datasets, lora_config, llama_adapter_config, prefix_config, train_config\nfrom data.sampler import LengthBasedBatchSampler, DistributedLengthBasedBatchSampler\nfrom utils.dataset_utils import DATASET_PREPROC\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\n\n\n\n\ndef update_config(config, **kwargs):\n    if isinstance(config, (tuple, list)):\n        for c in config:\n            update_config(c, **kwargs)\n    else:\n        for k, v in kwargs.items():\n            if hasattr(config, k):\n                setattr(config, k, v)\n            elif \".\" in k:\n                # allow --some_config.some_param=True\n                config_name, param_name = k.split(\".\")\n                if type(config).__name__ == config_name:\n                    if hasattr(config, param_name):\n                        setattr(config, param_name, v)\n                    else:\n                        # In case of specialized config we can warm user\n                        print(f\"Warning: {config_name} does not accept parameter: {k}\")\n            elif isinstance(config, train_config):\n                print(f\"Warning: unknown parameter {k}\")\n\n\ndef generate_peft_config(train_config, kwargs):\n    configs = (lora_config, llama_adapter_config, prefix_config)\n    peft_configs = (LoraConfig, AdaptionPromptConfig, PrefixTuningConfig)\n    names = tuple(c.__name__.rstrip(\"_config\") for c in configs)\n",
    "prefix": "import inspect\nimport torch.distributed as dist\nfrom dataclasses import asdict\nfrom torch.utils.data import DistributedSampler\nfrom peft import (\n    LoraConfig,\n    AdaptionPromptConfig,\n    PrefixTuningConfig,\n)\nfrom transformers import default_data_collator\nfrom transformers.data import DataCollatorForSeq2Seq\nfrom configs import datasets, lora_config, llama_adapter_config, prefix_config, train_config\nfrom data.sampler import LengthBasedBatchSampler, DistributedLengthBasedBatchSampler\nfrom utils.dataset_utils import DATASET_PREPROC\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\n\n\n\n\ndef update_config(config, **kwargs):\n    if isinstance(config, (tuple, list)):\n        for c in config:\n            update_config(c, **kwargs)\n    else:\n        for k, v in kwargs.items():\n            if hasattr(config, k):\n                setattr(config, k, v)\n            elif \".\" in k:\n                # allow --some_config.some_param=True\n                config_name, param_name = k.split(\".\")\n                if type(config).__name__ == config_name:\n                    if hasattr(config, param_name):\n                        setattr(config, param_name, v)\n                    else:\n                        # In case of specialized config we can warm user\n                        print(f\"Warning: {config_name} does not accept parameter: {k}\")\n            elif isinstance(config, train_config):\n                print(f\"Warning: unknown parameter {k}\")\n\n\ndef generate_peft_config(train_config, kwargs):\n    configs = (lora_config, llama_adapter_config, prefix_config)\n    peft_configs = (LoraConfig, AdaptionPromptConfig, PrefixTuningConfig)\n    names = tuple(c.__name__.rstrip(\"_config\") for c in configs)\n",
    "suffix": ""
  },
  {
    "name": "FoundationVision/UniRef:projects/UniRef/uniref/models/deformable_detr/matcher.py@1206",
    "canonical_solution": "                cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(bz_boxes),  box_cxcywh_to_xyxy(bz_gtboxs))",
    "prompt": "import torch\nimport torch.nn.functional as F\nimport torchvision.ops as ops\nfrom scipy.optimize import linear_sum_assignment\nfrom torch import nn\nfrom ...util.box_ops import box_cxcywh_to_xyxy, generalized_box_iou\n# ------------------------------------------------------------------------\n# Deformable DETR\n# Copyright (c) 2020 SenseTime. All Rights Reserved.\n# Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n# ------------------------------------------------------------------------\n# Modified from DETR (https://github.com/facebookresearch/detr)\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n# ------------------------------------------------------------------------\n\n\"\"\"\nModules to compute the matching cost and solve the corresponding LSAP.\n\"\"\"\n\n\nclass HungarianMatcher(nn.Module):\n    \"\"\"This class computes an assignment between the targets and the predictions of the network\n\n    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n    while the others are un-matched (and thus treated as non-objects).\n    \"\"\"\n\n    def __init__(self,\n                 cost_class: float = 1,\n                 cost_bbox: float = 1,\n                 cost_giou: float = 1):\n        \"\"\"Creates the matcher\n\n        Params:\n            cost_class: This is the relative weight of the classification error in the matching cost\n            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n        \"\"\"\n        super().__init__()\n        self.cost_class = cost_class\n        self.cost_bbox = cost_bbox\n        self.cost_giou = cost_giou\n        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n        \n    def forward_ota(self, outputs, targets):\n        \"\"\" simOTA for detr\n        \"\"\"\n        with torch.no_grad():\n            bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n            out_prob = outputs[\"pred_logits\"].sigmoid()\n            out_bbox = outputs[\"pred_boxes\"]  # \u8df3\u8fc7frame \u7ef4\u5ea6\n            indices = []\n            matched_ids = []\n            for batch_idx in range(bs):\n                bz_boxes = out_bbox[batch_idx] #[300,4]\n                bz_out_prob = out_prob[batch_idx] \n                bz_tgt_ids = targets[batch_idx][\"labels\"]\n                num_insts = len(bz_tgt_ids)\n                bz_gtboxs = targets[batch_idx]['boxes'].reshape(num_insts,4) #[num_gt, 4]\n                fg_mask, is_in_boxes_and_center  = \\\n                    self.get_in_boxes_info(bz_boxes,bz_gtboxs,expanded_strides=32)\n                pair_wise_ious = ops.box_iou(box_cxcywh_to_xyxy(bz_boxes), box_cxcywh_to_xyxy(bz_gtboxs))\n                # pair_wise_ious_loss = -torch.log(pair_wise_ious + 1e-8)\n\n                # Compute the classification cost.\n                alpha = 0.25\n                gamma = 2.0\n                neg_cost_class = (1 - alpha) * (bz_out_prob ** gamma) * (-(1 - bz_out_prob + 1e-8).log())\n                pos_cost_class = alpha * ((1 - bz_out_prob) ** gamma) * (-(bz_out_prob + 1e-8).log())\n                cost_class = pos_cost_class[:, bz_tgt_ids] - neg_cost_class[:, bz_tgt_ids]",
    "prefix": "import torch\nimport torch.nn.functional as F\nimport torchvision.ops as ops\nfrom scipy.optimize import linear_sum_assignment\nfrom torch import nn\nfrom ...util.box_ops import box_cxcywh_to_xyxy, generalized_box_iou\n# ------------------------------------------------------------------------\n# Deformable DETR\n# Copyright (c) 2020 SenseTime. All Rights Reserved.\n# Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n# ------------------------------------------------------------------------\n# Modified from DETR (https://github.com/facebookresearch/detr)\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n# ------------------------------------------------------------------------\n\n\"\"\"\nModules to compute the matching cost and solve the corresponding LSAP.\n\"\"\"\n\n\nclass HungarianMatcher(nn.Module):\n    \"\"\"This class computes an assignment between the targets and the predictions of the network\n\n    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n    while the others are un-matched (and thus treated as non-objects).\n    \"\"\"\n\n    def __init__(self,\n                 cost_class: float = 1,\n                 cost_bbox: float = 1,\n                 cost_giou: float = 1):\n        \"\"\"Creates the matcher\n\n        Params:\n            cost_class: This is the relative weight of the classification error in the matching cost\n            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n        \"\"\"\n        super().__init__()\n        self.cost_class = cost_class\n        self.cost_bbox = cost_bbox\n        self.cost_giou = cost_giou\n        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n        \n    def forward_ota(self, outputs, targets):\n        \"\"\" simOTA for detr\n        \"\"\"\n        with torch.no_grad():\n            bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n            out_prob = outputs[\"pred_logits\"].sigmoid()\n            out_bbox = outputs[\"pred_boxes\"]  # \u8df3\u8fc7frame \u7ef4\u5ea6\n            indices = []\n            matched_ids = []\n            for batch_idx in range(bs):\n                bz_boxes = out_bbox[batch_idx] #[300,4]\n                bz_out_prob = out_prob[batch_idx] \n                bz_tgt_ids = targets[batch_idx][\"labels\"]\n                num_insts = len(bz_tgt_ids)\n                bz_gtboxs = targets[batch_idx]['boxes'].reshape(num_insts,4) #[num_gt, 4]\n                fg_mask, is_in_boxes_and_center  = \\\n                    self.get_in_boxes_info(bz_boxes,bz_gtboxs,expanded_strides=32)\n                pair_wise_ious = ops.box_iou(box_cxcywh_to_xyxy(bz_boxes), box_cxcywh_to_xyxy(bz_gtboxs))\n                # pair_wise_ious_loss = -torch.log(pair_wise_ious + 1e-8)\n\n                # Compute the classification cost.\n                alpha = 0.25\n                gamma = 2.0\n                neg_cost_class = (1 - alpha) * (bz_out_prob ** gamma) * (-(1 - bz_out_prob + 1e-8).log())\n                pos_cost_class = alpha * ((1 - bz_out_prob) ** gamma) * (-(bz_out_prob + 1e-8).log())\n                cost_class = pos_cost_class[:, bz_tgt_ids] - neg_cost_class[:, bz_tgt_ids]",
    "suffix": ""
  },
  {
    "name": "xhuangcv/humannorm:threestudio/models/materials/diffuse_with_point_light_material.py@648",
    "canonical_solution": "    class Config(BaseMaterial.Config):",
    "prompt": "import random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport threestudio\nfrom dataclasses import dataclass, field\nfrom threestudio.models.materials.base import BaseMaterial\nfrom threestudio.utils.ops import dot, get_activation\nfrom threestudio.utils.typing import *\n\n\n\n\n@threestudio.register(\"diffuse-with-point-light-material\")\nclass DiffuseWithPointLightMaterial(BaseMaterial):\n    @dataclass",
    "prefix": "import random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport threestudio\nfrom dataclasses import dataclass, field\nfrom threestudio.models.materials.base import BaseMaterial\nfrom threestudio.utils.ops import dot, get_activation\nfrom threestudio.utils.typing import *\n\n\n\n\n@threestudio.register(\"diffuse-with-point-light-material\")\nclass DiffuseWithPointLightMaterial(BaseMaterial):\n    @dataclass",
    "suffix": ""
  },
  {
    "name": "jianchang512/stt:start.py@1399",
    "canonical_solution": "            endTime = tool.ms_to_time_string(ms=end)",
    "prompt": "import logging\nimport re\nimport threading\nimport sys\nimport torch\nimport os\nfrom flask import Flask, request, render_template, jsonify, send_from_directory\nfrom gevent.pywsgi import WSGIServer, WSGIHandler, LoggingLogAdapter\nfrom logging.handlers import RotatingFileHandler\nfrom stslib import cfg, tool\nfrom stslib.cfg import ROOT_DIR\nfrom faster_whisper import WhisperModel\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass CustomRequestHandler(WSGIHandler):\n    def log_request(self):\n        pass\n\n\n# \u914d\u7f6e\u65e5\u5fd7\n# \u7981\u7528 Werkzeug \u9ed8\u8ba4\u7684\u65e5\u5fd7\u5904\u7406\u5668\nlog = logging.getLogger('werkzeug')\nlog.handlers[:] = []\nlog.setLevel(logging.WARNING)\napp = Flask(__name__, static_folder=os.path.join(ROOT_DIR, 'static'), static_url_path='/static',\n            template_folder=os.path.join(ROOT_DIR, 'templates'))\nroot_log = logging.getLogger()  # Flask\u7684\u6839\u65e5\u5fd7\u8bb0\u5f55\u5668\nroot_log.handlers = []\nroot_log.setLevel(logging.WARNING)\n\n# \u914d\u7f6e\u65e5\u5fd7\napp.logger.setLevel(logging.WARNING)  # \u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b\u4e3a INFO\n# \u521b\u5efa RotatingFileHandler \u5bf9\u8c61\uff0c\u8bbe\u7f6e\u5199\u5165\u7684\u6587\u4ef6\u8def\u5f84\u548c\u5927\u5c0f\u9650\u5236\nfile_handler = RotatingFileHandler(os.path.join(ROOT_DIR, 'sts.log'), maxBytes=1024 * 1024, backupCount=5)\n# \u521b\u5efa\u65e5\u5fd7\u7684\u683c\u5f0f\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n# \u8bbe\u7f6e\u6587\u4ef6\u5904\u7406\u5668\u7684\u7ea7\u522b\u548c\u683c\u5f0f\nfile_handler.setLevel(logging.WARNING)\nfile_handler.setFormatter(formatter)\n# \u5c06\u6587\u4ef6\u5904\u7406\u5668\u6dfb\u52a0\u5230\u65e5\u5fd7\u8bb0\u5f55\u5668\u4e2d\napp.logger.addHandler(file_handler)\n\n\n@app.route('/static/<path:filename>')\ndef static_files(filename):\n    return send_from_directory(app.config['STATIC_FOLDER'], filename)\n\n\n@app.route('/')\ndef index():\n    return render_template(\"index.html\",\n                           cuda=cfg.cuda,\n                           lang_code=cfg.lang_code,\n                           language=cfg.LANG,\n                           root_dir=ROOT_DIR.replace('\\\\', '/'))\n\n\n# \u4e0a\u4f20\u97f3\u9891\n@app.route('/upload', methods=['POST'])\ndef upload():\n    try:\n        # \u83b7\u53d6\u4e0a\u4f20\u7684\u6587\u4ef6\n        audio_file = request.files['audio']\n        # \u5982\u679c\u662fmp4\n        noextname, ext = os.path.splitext(audio_file.filename)\n        ext = ext.lower()\n        # \u5982\u679c\u662f\u89c6\u9891\uff0c\u5148\u5206\u79bb\n        wav_file = os.path.join(cfg.TMP_DIR, f'{noextname}.wav')\n        if os.path.exists(wav_file) and os.path.getsize(wav_file) > 0:\n            return jsonify({'code': 0, 'msg': cfg.transobj['lang1'], \"data\": os.path.basename(wav_file)})\n        msg = \"\"\n        if ext in ['.mp4', '.mov', '.avi', '.mkv', '.mpeg', '.mp3', '.flac']:\n            video_file = os.path.join(cfg.TMP_DIR, f'{noextname}{ext}')\n            audio_file.save(video_file)\n            params = [\n                \"-i\",\n                video_file,\n            ]\n            if ext not in ['.mp3', '.flac']:\n                params.append('-vn')\n            params.append(wav_file)\n            rs = tool.runffmpeg(params)\n            if rs != 'ok':\n                return jsonify({\"code\": 1, \"msg\": rs})\n            msg = \",\" + cfg.transobj['lang9']\n        elif ext == '.wav':\n            audio_file.save(wav_file)\n        else:\n            return jsonify({\"code\": 1, \"msg\": f\"{cfg.transobj['lang3']} {ext}\"})\n\n        # \u8fd4\u56de\u6210\u529f\u7684\u54cd\u5e94\n        return jsonify({'code': 0, 'msg': cfg.transobj['lang1'] + msg, \"data\": os.path.basename(wav_file)})\n    except Exception as e:\n        app.logger.error(f'[upload]error: {e}')\n        return jsonify({'code': 2, 'msg': cfg.transobj['lang2']})\n\n\n# \u6839\u636e\u6587\u672c\u8fd4\u56detts\u7ed3\u679c\uff0c\u8fd4\u56de name=\u6587\u4ef6\u540d\u5b57\uff0cfilename=\u6587\u4ef6\u7edd\u5bf9\u8def\u5f84\n# \u8bf7\u6c42\u7aef\u6839\u636e\u9700\u8981\u81ea\u884c\u9009\u62e9\u4f7f\u7528\u54ea\u4e2a\n# params\n# wav_name:tmp\u4e0b\u7684wav\u6587\u4ef6\n# model \u6a21\u578b\u540d\u79f0\n@app.route('/process', methods=['GET', 'POST'])\ndef process():\n    # \u539f\u59cb\u5b57\u7b26\u4e32\n    wav_name = request.form.get(\"wav_name\").strip()\n    model = request.form.get(\"model\")\n    # \u8bed\u8a00\n    language = request.form.get(\"language\")\n    # \u8fd4\u56de\u683c\u5f0f json txt srt\n    data_type = request.form.get(\"data_type\")\n    wav_file = os.path.join(cfg.TMP_DIR, wav_name)\n    if not os.path.exists(wav_file):\n        return jsonify({\"code\": 1, \"msg\": f\"{wav_file} {cfg.langlist['lang5']}\"})\n    if not os.path.exists(os.path.join(cfg.MODEL_DIR, f'models--Systran--faster-whisper-{model}/snapshots/')):\n        return jsonify({\"code\": 1, \"msg\": f\"{model} {cfg.transobj['lang4']}\"})\n\n    try:\n        model = WhisperModel(model, device=device, compute_type=\"int8\", download_root=cfg.ROOT_DIR + \"/models\")\n        #model = whisper.load_model(model, download_root=cfg.ROOT_DIR + \"/models\")\n        segments,_ = model.transcribe(wav_file, beam_size=5,  vad_filter=True,\n    vad_parameters=dict(min_silence_duration_ms=500),language=language)\n        #segments = transcribe\n        raw_subtitles = []\n        #sidx=0\n        for segment in segments:\n            start = int(segment.start * 1000)\n            end = int(segment.end * 1000)\n            startTime = tool.ms_to_time_string(ms=start)",
    "prefix": "import logging\nimport re\nimport threading\nimport sys\nimport torch\nimport os\nfrom flask import Flask, request, render_template, jsonify, send_from_directory\nfrom gevent.pywsgi import WSGIServer, WSGIHandler, LoggingLogAdapter\nfrom logging.handlers import RotatingFileHandler\nfrom stslib import cfg, tool\nfrom stslib.cfg import ROOT_DIR\nfrom faster_whisper import WhisperModel\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass CustomRequestHandler(WSGIHandler):\n    def log_request(self):\n        pass\n\n\n# \u914d\u7f6e\u65e5\u5fd7\n# \u7981\u7528 Werkzeug \u9ed8\u8ba4\u7684\u65e5\u5fd7\u5904\u7406\u5668\nlog = logging.getLogger('werkzeug')\nlog.handlers[:] = []\nlog.setLevel(logging.WARNING)\napp = Flask(__name__, static_folder=os.path.join(ROOT_DIR, 'static'), static_url_path='/static',\n            template_folder=os.path.join(ROOT_DIR, 'templates'))\nroot_log = logging.getLogger()  # Flask\u7684\u6839\u65e5\u5fd7\u8bb0\u5f55\u5668\nroot_log.handlers = []\nroot_log.setLevel(logging.WARNING)\n\n# \u914d\u7f6e\u65e5\u5fd7\napp.logger.setLevel(logging.WARNING)  # \u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b\u4e3a INFO\n# \u521b\u5efa RotatingFileHandler \u5bf9\u8c61\uff0c\u8bbe\u7f6e\u5199\u5165\u7684\u6587\u4ef6\u8def\u5f84\u548c\u5927\u5c0f\u9650\u5236\nfile_handler = RotatingFileHandler(os.path.join(ROOT_DIR, 'sts.log'), maxBytes=1024 * 1024, backupCount=5)\n# \u521b\u5efa\u65e5\u5fd7\u7684\u683c\u5f0f\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n# \u8bbe\u7f6e\u6587\u4ef6\u5904\u7406\u5668\u7684\u7ea7\u522b\u548c\u683c\u5f0f\nfile_handler.setLevel(logging.WARNING)\nfile_handler.setFormatter(formatter)\n# \u5c06\u6587\u4ef6\u5904\u7406\u5668\u6dfb\u52a0\u5230\u65e5\u5fd7\u8bb0\u5f55\u5668\u4e2d\napp.logger.addHandler(file_handler)\n\n\n@app.route('/static/<path:filename>')\ndef static_files(filename):\n    return send_from_directory(app.config['STATIC_FOLDER'], filename)\n\n\n@app.route('/')\ndef index():\n    return render_template(\"index.html\",\n                           cuda=cfg.cuda,\n                           lang_code=cfg.lang_code,\n                           language=cfg.LANG,\n                           root_dir=ROOT_DIR.replace('\\\\', '/'))\n\n\n# \u4e0a\u4f20\u97f3\u9891\n@app.route('/upload', methods=['POST'])\ndef upload():\n    try:\n        # \u83b7\u53d6\u4e0a\u4f20\u7684\u6587\u4ef6\n        audio_file = request.files['audio']\n        # \u5982\u679c\u662fmp4\n        noextname, ext = os.path.splitext(audio_file.filename)\n        ext = ext.lower()\n        # \u5982\u679c\u662f\u89c6\u9891\uff0c\u5148\u5206\u79bb\n        wav_file = os.path.join(cfg.TMP_DIR, f'{noextname}.wav')\n        if os.path.exists(wav_file) and os.path.getsize(wav_file) > 0:\n            return jsonify({'code': 0, 'msg': cfg.transobj['lang1'], \"data\": os.path.basename(wav_file)})\n        msg = \"\"\n        if ext in ['.mp4', '.mov', '.avi', '.mkv', '.mpeg', '.mp3', '.flac']:\n            video_file = os.path.join(cfg.TMP_DIR, f'{noextname}{ext}')\n            audio_file.save(video_file)\n            params = [\n                \"-i\",\n                video_file,\n            ]\n            if ext not in ['.mp3', '.flac']:\n                params.append('-vn')\n            params.append(wav_file)\n            rs = tool.runffmpeg(params)\n            if rs != 'ok':\n                return jsonify({\"code\": 1, \"msg\": rs})\n            msg = \",\" + cfg.transobj['lang9']\n        elif ext == '.wav':\n            audio_file.save(wav_file)\n        else:\n            return jsonify({\"code\": 1, \"msg\": f\"{cfg.transobj['lang3']} {ext}\"})\n\n        # \u8fd4\u56de\u6210\u529f\u7684\u54cd\u5e94\n        return jsonify({'code': 0, 'msg': cfg.transobj['lang1'] + msg, \"data\": os.path.basename(wav_file)})\n    except Exception as e:\n        app.logger.error(f'[upload]error: {e}')\n        return jsonify({'code': 2, 'msg': cfg.transobj['lang2']})\n\n\n# \u6839\u636e\u6587\u672c\u8fd4\u56detts\u7ed3\u679c\uff0c\u8fd4\u56de name=\u6587\u4ef6\u540d\u5b57\uff0cfilename=\u6587\u4ef6\u7edd\u5bf9\u8def\u5f84\n# \u8bf7\u6c42\u7aef\u6839\u636e\u9700\u8981\u81ea\u884c\u9009\u62e9\u4f7f\u7528\u54ea\u4e2a\n# params\n# wav_name:tmp\u4e0b\u7684wav\u6587\u4ef6\n# model \u6a21\u578b\u540d\u79f0\n@app.route('/process', methods=['GET', 'POST'])\ndef process():\n    # \u539f\u59cb\u5b57\u7b26\u4e32\n    wav_name = request.form.get(\"wav_name\").strip()\n    model = request.form.get(\"model\")\n    # \u8bed\u8a00\n    language = request.form.get(\"language\")\n    # \u8fd4\u56de\u683c\u5f0f json txt srt\n    data_type = request.form.get(\"data_type\")\n    wav_file = os.path.join(cfg.TMP_DIR, wav_name)\n    if not os.path.exists(wav_file):\n        return jsonify({\"code\": 1, \"msg\": f\"{wav_file} {cfg.langlist['lang5']}\"})\n    if not os.path.exists(os.path.join(cfg.MODEL_DIR, f'models--Systran--faster-whisper-{model}/snapshots/')):\n        return jsonify({\"code\": 1, \"msg\": f\"{model} {cfg.transobj['lang4']}\"})\n\n    try:\n        model = WhisperModel(model, device=device, compute_type=\"int8\", download_root=cfg.ROOT_DIR + \"/models\")\n        #model = whisper.load_model(model, download_root=cfg.ROOT_DIR + \"/models\")\n        segments,_ = model.transcribe(wav_file, beam_size=5,  vad_filter=True,\n    vad_parameters=dict(min_silence_duration_ms=500),language=language)\n        #segments = transcribe\n        raw_subtitles = []\n        #sidx=0\n        for segment in segments:\n            start = int(segment.start * 1000)\n            end = int(segment.end * 1000)\n            startTime = tool.ms_to_time_string(ms=start)",
    "suffix": ""
  },
  {
    "name": "ffmemes/ff-backend:src/tgbot/service.py@764",
    "canonical_solution": "    language_code: Language,",
    "prompt": "from typing import Any\nfrom datetime import datetime\nfrom sqlalchemy import select, nulls_first, text\nfrom sqlalchemy.dialects.postgresql import insert\nfrom src.database import (\n    language,\n    meme,\n    meme_source,\n    user,\n    user_tg,\n    user_language,\n    meme_raw_telegram,\n    execute, fetch_one, fetch_all,\n)\nfrom src.storage.constants import Language\n\n\n\n\nasync def save_tg_user(\n    id: int,\n    **kwargs,\n) -> None:\n    insert_statement = (\n        insert(user_tg)\n        .values({\"id\": id, **kwargs})\n        .on_conflict_do_update(\n            index_elements=(user_tg.c.id,),\n            set_={\"updated_at\": datetime.utcnow()},\n            # do we need to update more fields if a user already exists?\n        )\n    )\n\n    await execute(insert_statement)\n    # do not return the same data\n\n\nasync def save_user(\n    id: int,\n    **kwargs,\n) -> None:\n    insert_statement = (\n        insert(user)\n        .values({\"id\": id, **kwargs})\n        .on_conflict_do_update(\n            index_elements=(user.c.id,),\n            set_={\n                \"last_active_at\": datetime.utcnow(),\n                \"blocked_bot_at\": None,\n            },\n        )\n        .returning(user)\n    )\n\n    return await fetch_one(insert_statement)\n\n\nasync def get_user_by_id(\n    id: int,\n) -> dict[str, Any] | None:\n    select_statement = select(user).where(user.c.id == id)\n    return await fetch_one(select_statement)\n\n\nasync def get_meme_source_by_id(\n    id: int,\n) -> dict[str, Any] | None:\n    select_statement = select(meme_source).where(meme_source.c.id == id)\n    return await fetch_one(select_statement)\n\n\nasync def get_or_create_meme_source(\n    url: str,\n    **kwargs,\n) -> dict[str, Any] | None:\n    insert_statement = (\n        insert(meme_source)\n        .values({\"url\": url, **kwargs})\n        .on_conflict_do_update(\n            index_elements=(meme_source.c.url,),\n            set_={\"updated_at\": datetime.utcnow()},\n        )\n        .returning(meme_source)\n    )\n\n    return await fetch_one(insert_statement)\n\n\nasync def update_meme_source(\n    id: int,\n    **kwargs,\n) -> dict[str, Any] | None:\n    update_statement = (\n        meme_source.update()\n        .where(meme_source.c.id == id)\n        .values({\"updated_at\": datetime.utcnow(), **kwargs})\n        .returning(meme_source)\n    )\n\n    return await fetch_one(update_statement)\n\n\nasync def add_user_language(\n    user_id: int,\n    language_code: Language,\n) -> None:\n    insert_language_query = (\n        insert(user_language)\n        .values({\"user_id\": user_id, \"language_code\": language_code})\n        .on_conflict_do_nothing(\n            index_elements=(user_language.c.user_id, user_language.c.language_code)\n        )\n    )\n\n    await execute(insert_language_query)\n\n\nasync def del_user_language(\n    user_id: int,",
    "prefix": "from typing import Any\nfrom datetime import datetime\nfrom sqlalchemy import select, nulls_first, text\nfrom sqlalchemy.dialects.postgresql import insert\nfrom src.database import (\n    language,\n    meme,\n    meme_source,\n    user,\n    user_tg,\n    user_language,\n    meme_raw_telegram,\n    execute, fetch_one, fetch_all,\n)\nfrom src.storage.constants import Language\n\n\n\n\nasync def save_tg_user(\n    id: int,\n    **kwargs,\n) -> None:\n    insert_statement = (\n        insert(user_tg)\n        .values({\"id\": id, **kwargs})\n        .on_conflict_do_update(\n            index_elements=(user_tg.c.id,),\n            set_={\"updated_at\": datetime.utcnow()},\n            # do we need to update more fields if a user already exists?\n        )\n    )\n\n    await execute(insert_statement)\n    # do not return the same data\n\n\nasync def save_user(\n    id: int,\n    **kwargs,\n) -> None:\n    insert_statement = (\n        insert(user)\n        .values({\"id\": id, **kwargs})\n        .on_conflict_do_update(\n            index_elements=(user.c.id,),\n            set_={\n                \"last_active_at\": datetime.utcnow(),\n                \"blocked_bot_at\": None,\n            },\n        )\n        .returning(user)\n    )\n\n    return await fetch_one(insert_statement)\n\n\nasync def get_user_by_id(\n    id: int,\n) -> dict[str, Any] | None:\n    select_statement = select(user).where(user.c.id == id)\n    return await fetch_one(select_statement)\n\n\nasync def get_meme_source_by_id(\n    id: int,\n) -> dict[str, Any] | None:\n    select_statement = select(meme_source).where(meme_source.c.id == id)\n    return await fetch_one(select_statement)\n\n\nasync def get_or_create_meme_source(\n    url: str,\n    **kwargs,\n) -> dict[str, Any] | None:\n    insert_statement = (\n        insert(meme_source)\n        .values({\"url\": url, **kwargs})\n        .on_conflict_do_update(\n            index_elements=(meme_source.c.url,),\n            set_={\"updated_at\": datetime.utcnow()},\n        )\n        .returning(meme_source)\n    )\n\n    return await fetch_one(insert_statement)\n\n\nasync def update_meme_source(\n    id: int,\n    **kwargs,\n) -> dict[str, Any] | None:\n    update_statement = (\n        meme_source.update()\n        .where(meme_source.c.id == id)\n        .values({\"updated_at\": datetime.utcnow(), **kwargs})\n        .returning(meme_source)\n    )\n\n    return await fetch_one(update_statement)\n\n\nasync def add_user_language(\n    user_id: int,\n    language_code: Language,\n) -> None:\n    insert_language_query = (\n        insert(user_language)\n        .values({\"user_id\": user_id, \"language_code\": language_code})\n        .on_conflict_do_nothing(\n            index_elements=(user_language.c.user_id, user_language.c.language_code)\n        )\n    )\n\n    await execute(insert_language_query)\n\n\nasync def del_user_language(\n    user_id: int,",
    "suffix": ""
  },
  {
    "name": "Con6924/SPM:src/evaluation/clip_evaluator.py@1179",
    "canonical_solution": "        base_cfg: GenerationConfig = GenerationConfig(),",
    "prompt": "import json\nimport os\nimport random\nfrom argparse import ArgumentParser\nfrom prettytable import PrettyTable\nfrom tqdm import tqdm\nfrom src.configs.generation_config import GenerationConfig\nfrom ..misc.clip_templates import anchor_templates, imagenet_templates\nfrom .eval_util import clip_eval_by_image\nfrom .evaluator import Evaluator, GenerationDataset\n\n\n\n\nclass ClipTemplateDataset(GenerationDataset):\n    def __init__(\n        self,\n        concepts: list[str],\n        save_folder: str = \"benchmark/generated_imgs/\",",
    "prefix": "import json\nimport os\nimport random\nfrom argparse import ArgumentParser\nfrom prettytable import PrettyTable\nfrom tqdm import tqdm\nfrom src.configs.generation_config import GenerationConfig\nfrom ..misc.clip_templates import anchor_templates, imagenet_templates\nfrom .eval_util import clip_eval_by_image\nfrom .evaluator import Evaluator, GenerationDataset\n\n\n\n\nclass ClipTemplateDataset(GenerationDataset):\n    def __init__(\n        self,\n        concepts: list[str],\n        save_folder: str = \"benchmark/generated_imgs/\",",
    "suffix": ""
  },
  {
    "name": "dakpinaroglu/Frame2seq:frame2seq/utils/score.py@1488",
    "canonical_solution": "                torch.tensor([residue_constants.AA_TO_ID[aa]",
    "prompt": "import os\nimport torch\nfrom tqdm import tqdm\nfrom frame2seq.utils import residue_constants\nfrom frame2seq.utils.util import get_neg_pll, read_fasta_file\nfrom frame2seq.utils.pdb2input import get_inference_inputs\nfrom frame2seq.utils.pred2output import output_csv, output_indiv_csv\n\n\n\ndef score(self, pdb_file, chain_id, fasta_file, save_indiv_neg_pll):\n    temperature = 1.0\n    seq_mask, aatype, X = get_inference_inputs(pdb_file, chain_id)\n    seq_mask = seq_mask.to(self.device)\n    aatype = aatype.to(self.device)\n    X = X.to(self.device)\n    str_form = [residue_constants.ID_TO_AA[int(i)] for i in aatype[0]]\n    input_aatype_onehot = residue_constants.sequence_to_onehot(\n        sequence=str_form,\n        mapping=residue_constants.AA_TO_ID,\n    )\n    input_aatype_onehot = torch.from_numpy(input_aatype_onehot).float()\n    input_aatype_onehot = input_aatype_onehot.unsqueeze(0)\n    input_aatype_onehot = input_aatype_onehot.to(self.device)\n    input_aatype_onehot = torch.zeros_like(input_aatype_onehot)\n    input_aatype_onehot[:, :,\n                        20] = 1  # all positions are masked (set to unknown)\n    scores, preds = {}, []\n    with torch.no_grad():\n        pred_seq1 = self.models[0].forward(X, seq_mask, input_aatype_onehot)\n        pred_seq2 = self.models[1].forward(X, seq_mask, input_aatype_onehot)\n        pred_seq3 = self.models[2].forward(X, seq_mask, input_aatype_onehot)\n        pred_seq = (pred_seq1 + pred_seq2 + pred_seq3) / 3  # ensemble\n        pred_seq = pred_seq / temperature\n        pred_seq = torch.nn.functional.softmax(pred_seq, dim=-1)\n        pred_seq = pred_seq[seq_mask]\n        if fasta_file is not None:\n            input_seqs = read_fasta_file(fasta_file)\n            input_seqs = [",
    "prefix": "import os\nimport torch\nfrom tqdm import tqdm\nfrom frame2seq.utils import residue_constants\nfrom frame2seq.utils.util import get_neg_pll, read_fasta_file\nfrom frame2seq.utils.pdb2input import get_inference_inputs\nfrom frame2seq.utils.pred2output import output_csv, output_indiv_csv\n\n\n\ndef score(self, pdb_file, chain_id, fasta_file, save_indiv_neg_pll):\n    temperature = 1.0\n    seq_mask, aatype, X = get_inference_inputs(pdb_file, chain_id)\n    seq_mask = seq_mask.to(self.device)\n    aatype = aatype.to(self.device)\n    X = X.to(self.device)\n    str_form = [residue_constants.ID_TO_AA[int(i)] for i in aatype[0]]\n    input_aatype_onehot = residue_constants.sequence_to_onehot(\n        sequence=str_form,\n        mapping=residue_constants.AA_TO_ID,\n    )\n    input_aatype_onehot = torch.from_numpy(input_aatype_onehot).float()\n    input_aatype_onehot = input_aatype_onehot.unsqueeze(0)\n    input_aatype_onehot = input_aatype_onehot.to(self.device)\n    input_aatype_onehot = torch.zeros_like(input_aatype_onehot)\n    input_aatype_onehot[:, :,\n                        20] = 1  # all positions are masked (set to unknown)\n    scores, preds = {}, []\n    with torch.no_grad():\n        pred_seq1 = self.models[0].forward(X, seq_mask, input_aatype_onehot)\n        pred_seq2 = self.models[1].forward(X, seq_mask, input_aatype_onehot)\n        pred_seq3 = self.models[2].forward(X, seq_mask, input_aatype_onehot)\n        pred_seq = (pred_seq1 + pred_seq2 + pred_seq3) / 3  # ensemble\n        pred_seq = pred_seq / temperature\n        pred_seq = torch.nn.functional.softmax(pred_seq, dim=-1)\n        pred_seq = pred_seq[seq_mask]\n        if fasta_file is not None:\n            input_seqs = read_fasta_file(fasta_file)\n            input_seqs = [",
    "suffix": ""
  },
  {
    "name": "davep/oshit:oshit/app/oshit.py@1386",
    "canonical_solution": "        configuration = load_configuration()",
    "prompt": "from textual.app import App\nfrom .data import load_configuration, save_configuration\nfrom .screens import Main\n\"\"\"The main application class.\"\"\"\n\n##############################################################################\n# Textual imports.\n\n##############################################################################\n# Local imports.\n\n\n##############################################################################\nclass OSHit(App[None]):\n    \"\"\"The Orange Site Hit application.\"\"\"\n\n    ENABLE_COMMAND_PALETTE = False\n\n    def __init__(self) -> None:\n        \"\"\"Initialise the application.\"\"\"\n        super().__init__()\n        self.dark = load_configuration().dark_mode\n\n    def on_mount(self) -> None:\n        \"\"\"Get things going once the app is up and running.\"\"\"\n        self.push_screen(Main())\n\n    def _watch_dark(self) -> None:\n        \"\"\"Save the light/dark mode configuration choice.\"\"\"",
    "prefix": "from textual.app import App\nfrom .data import load_configuration, save_configuration\nfrom .screens import Main\n\"\"\"The main application class.\"\"\"\n\n##############################################################################\n# Textual imports.\n\n##############################################################################\n# Local imports.\n\n\n##############################################################################\nclass OSHit(App[None]):\n    \"\"\"The Orange Site Hit application.\"\"\"\n\n    ENABLE_COMMAND_PALETTE = False\n\n    def __init__(self) -> None:\n        \"\"\"Initialise the application.\"\"\"\n        super().__init__()\n        self.dark = load_configuration().dark_mode\n\n    def on_mount(self) -> None:\n        \"\"\"Get things going once the app is up and running.\"\"\"\n        self.push_screen(Main())\n\n    def _watch_dark(self) -> None:\n        \"\"\"Save the light/dark mode configuration choice.\"\"\"",
    "suffix": ""
  },
  {
    "name": "Maximilian-Winter/llama-cpp-agent:src/llama_cpp_agent/agent_memory/memory_tools.py@1284",
    "canonical_solution": "    def run(self, retrieval_memory_manager: RetrievalMemoryManager):",
    "prompt": "from pydantic import BaseModel, Field\nfrom ..function_calling import LlamaCppFunctionTool\nfrom .core_memory_manager import CoreMemoryManager\nfrom .retrieval_memory_manager import RetrievalMemoryManager, RetrievalMemory\n\n\n\nclass AddCoreMemory(BaseModel):\n    \"\"\"\n    Add a new entry to the core memory.\n    \"\"\"\n    key: str = Field(..., description=\"The key identifier for the core memory entry.\")\n    field: str = Field(..., description=\"A secondary key or field within the core memory entry.\")\n    value: str = Field(..., description=\"The value or data to be stored in the specified core memory entry.\")\n\n    def run(self, core_memory_manager: CoreMemoryManager):\n        return core_memory_manager.add_to_core_memory(self.key, self.field, self.value)\n\n\n# Replace Core Memory Model\nclass ReplaceCoreMemory(BaseModel):\n    \"\"\"\n    Replace an entry in the core memory.\n    \"\"\"\n    key: str = Field(..., description=\"The key identifier for the core memory entry.\")\n    field: str = Field(..., description=\"The specific field within the core memory entry to be replaced.\")\n    new_value: str = Field(...,\n                           description=\"The new value to replace the existing data in the specified core memory field.\")\n\n    def run(self, core_memory_manager: CoreMemoryManager):\n        return core_memory_manager.replace_in_core_memory(self.key, self.field, self.value)\n\n\nclass RemoveCoreMemory(BaseModel):\n    \"\"\"\n    Remove an entry in the core memory.\n    \"\"\"\n    key: str = Field(..., description=\"The key identifier for the core memory entry to be removed.\")\n    field: str = Field(..., description=\"The specific field within the core memory entry to be removed.\")\n\n    def run(self, core_memory_manager: CoreMemoryManager):\n        return core_memory_manager.remove_from_core_memory(self.key, self.field)\n\n\nclass RetrieveMemories(BaseModel):\n    \"\"\"\n    Retrieve memories from the retrieval memory based on a query.\n    \"\"\"\n    query: str = Field(..., description=\"The query to be used to retrieve memories from the retrieval memory.\")\n\n    def run(self, retrieval_memory_manager: RetrievalMemoryManager):\n        return retrieval_memory_manager.retrieve_memories(self.query)\n\n\nclass AddRetrievalMemory(BaseModel):\n    \"\"\"\n    Add memory to the retrieval memory.\n    \"\"\"\n    memory: str = Field(..., description=\"The memory to be added to the retrieval memory.\")\n    importance: float = Field(..., description=\"The importance of the memory to be added to the retrieval memory.\")\n",
    "prefix": "from pydantic import BaseModel, Field\nfrom ..function_calling import LlamaCppFunctionTool\nfrom .core_memory_manager import CoreMemoryManager\nfrom .retrieval_memory_manager import RetrievalMemoryManager, RetrievalMemory\n\n\n\nclass AddCoreMemory(BaseModel):\n    \"\"\"\n    Add a new entry to the core memory.\n    \"\"\"\n    key: str = Field(..., description=\"The key identifier for the core memory entry.\")\n    field: str = Field(..., description=\"A secondary key or field within the core memory entry.\")\n    value: str = Field(..., description=\"The value or data to be stored in the specified core memory entry.\")\n\n    def run(self, core_memory_manager: CoreMemoryManager):\n        return core_memory_manager.add_to_core_memory(self.key, self.field, self.value)\n\n\n# Replace Core Memory Model\nclass ReplaceCoreMemory(BaseModel):\n    \"\"\"\n    Replace an entry in the core memory.\n    \"\"\"\n    key: str = Field(..., description=\"The key identifier for the core memory entry.\")\n    field: str = Field(..., description=\"The specific field within the core memory entry to be replaced.\")\n    new_value: str = Field(...,\n                           description=\"The new value to replace the existing data in the specified core memory field.\")\n\n    def run(self, core_memory_manager: CoreMemoryManager):\n        return core_memory_manager.replace_in_core_memory(self.key, self.field, self.value)\n\n\nclass RemoveCoreMemory(BaseModel):\n    \"\"\"\n    Remove an entry in the core memory.\n    \"\"\"\n    key: str = Field(..., description=\"The key identifier for the core memory entry to be removed.\")\n    field: str = Field(..., description=\"The specific field within the core memory entry to be removed.\")\n\n    def run(self, core_memory_manager: CoreMemoryManager):\n        return core_memory_manager.remove_from_core_memory(self.key, self.field)\n\n\nclass RetrieveMemories(BaseModel):\n    \"\"\"\n    Retrieve memories from the retrieval memory based on a query.\n    \"\"\"\n    query: str = Field(..., description=\"The query to be used to retrieve memories from the retrieval memory.\")\n\n    def run(self, retrieval_memory_manager: RetrievalMemoryManager):\n        return retrieval_memory_manager.retrieve_memories(self.query)\n\n\nclass AddRetrievalMemory(BaseModel):\n    \"\"\"\n    Add memory to the retrieval memory.\n    \"\"\"\n    memory: str = Field(..., description=\"The memory to be added to the retrieval memory.\")\n    importance: float = Field(..., description=\"The importance of the memory to be added to the retrieval memory.\")\n",
    "suffix": ""
  },
  {
    "name": "tedivm/paracelsus:paracelsus/cli.py@1281",
    "canonical_solution": "    \"mermaid\": Mermaid,",
    "prompt": "import importlib\nimport re\nimport sys\nimport typer\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import List\nfrom typing_extensions import Annotated\nfrom .transformers.dot import Dot\nfrom .transformers.mermaid import Mermaid\n    from . import _version\n\n\n\napp = typer.Typer()\n\ntransformers = {\n    \"mmd\": Mermaid,",
    "prefix": "import importlib\nimport re\nimport sys\nimport typer\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import List\nfrom typing_extensions import Annotated\nfrom .transformers.dot import Dot\nfrom .transformers.mermaid import Mermaid\n    from . import _version\n\n\n\napp = typer.Typer()\n\ntransformers = {\n    \"mmd\": Mermaid,",
    "suffix": ""
  },
  {
    "name": "winniesi/tg-gemini-bot:api/handle.py@1066",
    "canonical_solution": "        send_message(update.from_id, response_text)",
    "prompt": "from .auth import is_authorized\nfrom .context import ChatManager, ImageChatManger\nfrom .telegram import Update, send_message\n\"\"\"\nAll the chat that comes through the Telegram bot gets passed to the\nhandle_message function. This function checks out if the user has the\ngreen light to chat with the bot. Once that's sorted, it figures out if\nthe user sent words or an image and deals with it accordingly.\n\nFor text messages, it fires up the ChatManager class that keeps track of\nthe back-and-forth with that user.\n\nAs for images, in Gemini pro, they're context-free, so you can handle\nthem pretty straight-up without much fuss.\n\"\"\"\n\n\nchat_manager = ChatManager()\n\n\ndef handle_message(update_data):\n    update = Update(update_data)\n    authorized = is_authorized(update.from_id, update.user_name)\n    if not authorized:\n        send_message(update.from_id, \"\ud83d\ude2b You are not allowed to use this bot.\")\n        return\n    if update.type == \"text\":\n        chat = chat_manager.get_chat(update.from_id)\n        anwser = chat.send_message(update.text)\n        extra_text = (\n            \"\\n\\nType /new to kick off a new chat.\" if chat.history_length >= 2 else \"\"\n        )\n        response_text = f\"{anwser}{extra_text}\"",
    "prefix": "from .auth import is_authorized\nfrom .context import ChatManager, ImageChatManger\nfrom .telegram import Update, send_message\n\"\"\"\nAll the chat that comes through the Telegram bot gets passed to the\nhandle_message function. This function checks out if the user has the\ngreen light to chat with the bot. Once that's sorted, it figures out if\nthe user sent words or an image and deals with it accordingly.\n\nFor text messages, it fires up the ChatManager class that keeps track of\nthe back-and-forth with that user.\n\nAs for images, in Gemini pro, they're context-free, so you can handle\nthem pretty straight-up without much fuss.\n\"\"\"\n\n\nchat_manager = ChatManager()\n\n\ndef handle_message(update_data):\n    update = Update(update_data)\n    authorized = is_authorized(update.from_id, update.user_name)\n    if not authorized:\n        send_message(update.from_id, \"\ud83d\ude2b You are not allowed to use this bot.\")\n        return\n    if update.type == \"text\":\n        chat = chat_manager.get_chat(update.from_id)\n        anwser = chat.send_message(update.text)\n        extra_text = (\n            \"\\n\\nType /new to kick off a new chat.\" if chat.history_length >= 2 else \"\"\n        )\n        response_text = f\"{anwser}{extra_text}\"",
    "suffix": ""
  },
  {
    "name": "usail-hkust/LLMTSCS:run_open_llm_wait_time_forecast.py@1225",
    "canonical_solution": "    except error.flowFileException as e:",
    "prompt": "from utils.utils import oneline_wrapper\nfrom multiprocessing import Process\nfrom utils import error\nimport os\nimport time\nimport argparse\nimport uvicorn\n\"\"\"\nRun the Fixed-Time model\nOn JiNan and HangZhou real data\n\"\"\"\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--memo\", type=str, default='LLMTLCSWaitTimeForecast')\n    parser.add_argument(\"--model\", type=str, default=\"LLMTLCSWaitTimeForecast\")\n    parser.add_argument(\"--llm_model\", type=str, default=\"llama_2_70b_chat_hf\")\n    parser.add_argument(\"--proj_name\", type=str, default=\"chatgpt-TSCS\")\n    parser.add_argument(\"--eightphase\", action=\"store_true\", default=False)\n    parser.add_argument(\"--multi_process\", action=\"store_true\", default=True)\n    parser.add_argument(\"--workers\", type=int, default=1)\n    parser.add_argument(\"--llm_api_thread_num\", type=int, default=1)\n    parser.add_argument(\"--with_external_api\", type=bool, default=False)\n    parser.add_argument(\"--dataset\", type=str, default=\"hangzhou\")\n    parser.add_argument(\"--traffic_file\", type=str, default=\"anon_4_4_hangzhou_real.json\")\n\n    return parser.parse_args()\n\n\ndef main(in_args):\n    traffic_file_list = []\n\n    if in_args.dataset == 'jinan':\n        count = 3600\n        road_net = \"3_4\"\n        traffic_file_list = [\"anon_3_4_jinan_real.json\", \"anon_3_4_jinan_real_2000.json\", \"anon_3_4_jinan_real_2500.json\"]\n        template = \"Jinan\"\n    elif in_args.dataset == 'hangzhou':\n        count = 3600\n        road_net = \"4_4\"\n        traffic_file_list = [\"anon_4_4_hangzhou_real.json\", \"anon_4_4_hangzhou_real_5816.json\"]\n        template = \"Hangzhou\"\n    elif in_args.dataset == 'newyork_16x3':\n        count = 3600\n        road_net = \"16_3\"\n        traffic_file_list = [\"anon_16_3_newyork_real.json\"]\n        template = \"NewYork\"\n    elif in_args.dataset == 'newyork_28x7':\n        count = 3600\n        road_net = \"28_7\"\n        traffic_file_list = [\"anon_28_7_newyork_real_double.json\", \"anon_28_7_newyork_real_triple.json\"]\n        template = \"NewYork\"\n\n    # flow_file error\n    try:\n        if in_args.traffic_file not in traffic_file_list:\n            raise error.flowFileException('Flow file does not exist.')",
    "prefix": "from utils.utils import oneline_wrapper\nfrom multiprocessing import Process\nfrom utils import error\nimport os\nimport time\nimport argparse\nimport uvicorn\n\"\"\"\nRun the Fixed-Time model\nOn JiNan and HangZhou real data\n\"\"\"\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--memo\", type=str, default='LLMTLCSWaitTimeForecast')\n    parser.add_argument(\"--model\", type=str, default=\"LLMTLCSWaitTimeForecast\")\n    parser.add_argument(\"--llm_model\", type=str, default=\"llama_2_70b_chat_hf\")\n    parser.add_argument(\"--proj_name\", type=str, default=\"chatgpt-TSCS\")\n    parser.add_argument(\"--eightphase\", action=\"store_true\", default=False)\n    parser.add_argument(\"--multi_process\", action=\"store_true\", default=True)\n    parser.add_argument(\"--workers\", type=int, default=1)\n    parser.add_argument(\"--llm_api_thread_num\", type=int, default=1)\n    parser.add_argument(\"--with_external_api\", type=bool, default=False)\n    parser.add_argument(\"--dataset\", type=str, default=\"hangzhou\")\n    parser.add_argument(\"--traffic_file\", type=str, default=\"anon_4_4_hangzhou_real.json\")\n\n    return parser.parse_args()\n\n\ndef main(in_args):\n    traffic_file_list = []\n\n    if in_args.dataset == 'jinan':\n        count = 3600\n        road_net = \"3_4\"\n        traffic_file_list = [\"anon_3_4_jinan_real.json\", \"anon_3_4_jinan_real_2000.json\", \"anon_3_4_jinan_real_2500.json\"]\n        template = \"Jinan\"\n    elif in_args.dataset == 'hangzhou':\n        count = 3600\n        road_net = \"4_4\"\n        traffic_file_list = [\"anon_4_4_hangzhou_real.json\", \"anon_4_4_hangzhou_real_5816.json\"]\n        template = \"Hangzhou\"\n    elif in_args.dataset == 'newyork_16x3':\n        count = 3600\n        road_net = \"16_3\"\n        traffic_file_list = [\"anon_16_3_newyork_real.json\"]\n        template = \"NewYork\"\n    elif in_args.dataset == 'newyork_28x7':\n        count = 3600\n        road_net = \"28_7\"\n        traffic_file_list = [\"anon_28_7_newyork_real_double.json\", \"anon_28_7_newyork_real_triple.json\"]\n        template = \"NewYork\"\n\n    # flow_file error\n    try:\n        if in_args.traffic_file not in traffic_file_list:\n            raise error.flowFileException('Flow file does not exist.')",
    "suffix": ""
  },
  {
    "name": "ohadmata/shmessy:src/shmessy/types/unix_timestamp.py@821",
    "canonical_solution": "    def fix(self, column: Series, inferred_field: InferredField) -> Series:",
    "prompt": "import logging\nimport math\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Optional\nfrom numpy import ndarray\nfrom pandas import Series, to_datetime\nfrom ..schema import InferredField, ValidatorTypes\nfrom .base import BaseType\n\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass TimestampResolution(str, Enum):\n    SECONDS = \"s\"\n    MILLISECONDS = \"ms\"\n    NANOSECONDS = \"ns\"\n\n\nclass UnixTimestampType(BaseType):\n    weight = 4\n    validator_types = (ValidatorTypes.NUMERIC,)\n    min_valid_year: int = 1980\n    max_valid_year: int = 2100\n\n    @staticmethod\n    def _unix_timestamp_resolution(value: float) -> TimestampResolution:\n        number_of_digits = len(str(int(value)))\n        if number_of_digits == 10:\n            return TimestampResolution.SECONDS\n        if number_of_digits == 13:\n            return TimestampResolution.MILLISECONDS\n        if number_of_digits == 16:\n            return TimestampResolution.NANOSECONDS\n\n    @staticmethod\n    def _fix_input_resolution(\n        value: float, selected_resolution: TimestampResolution\n    ) -> float:\n        if selected_resolution == TimestampResolution.SECONDS:\n            return value\n        if selected_resolution == TimestampResolution.MILLISECONDS:\n            return value / 1000\n        if selected_resolution == TimestampResolution.NANOSECONDS:\n            return value / 1000 / 1000\n\n    def validate(self, data: ndarray) -> Optional[InferredField]:\n        if not self.is_validator_type_valid(dtype=data.dtype):\n            return None\n\n        try:\n            selected_resolution = self._unix_timestamp_resolution(float(data[0]))\n            if not selected_resolution:\n                return None\n            for value in data:\n                if not math.isnan(value):\n                    parsed_value = datetime.utcfromtimestamp(\n                        self._fix_input_resolution(value, selected_resolution)\n                    )\n                    if (\n                        parsed_value.year < self.min_valid_year\n                        or parsed_value.year > self.max_valid_year\n                    ):\n                        return None\n\n            return InferredField(\n                inferred_type=self.name, inferred_pattern=selected_resolution\n            )\n        except ValueError:\n            return None\n",
    "prefix": "import logging\nimport math\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Optional\nfrom numpy import ndarray\nfrom pandas import Series, to_datetime\nfrom ..schema import InferredField, ValidatorTypes\nfrom .base import BaseType\n\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass TimestampResolution(str, Enum):\n    SECONDS = \"s\"\n    MILLISECONDS = \"ms\"\n    NANOSECONDS = \"ns\"\n\n\nclass UnixTimestampType(BaseType):\n    weight = 4\n    validator_types = (ValidatorTypes.NUMERIC,)\n    min_valid_year: int = 1980\n    max_valid_year: int = 2100\n\n    @staticmethod\n    def _unix_timestamp_resolution(value: float) -> TimestampResolution:\n        number_of_digits = len(str(int(value)))\n        if number_of_digits == 10:\n            return TimestampResolution.SECONDS\n        if number_of_digits == 13:\n            return TimestampResolution.MILLISECONDS\n        if number_of_digits == 16:\n            return TimestampResolution.NANOSECONDS\n\n    @staticmethod\n    def _fix_input_resolution(\n        value: float, selected_resolution: TimestampResolution\n    ) -> float:\n        if selected_resolution == TimestampResolution.SECONDS:\n            return value\n        if selected_resolution == TimestampResolution.MILLISECONDS:\n            return value / 1000\n        if selected_resolution == TimestampResolution.NANOSECONDS:\n            return value / 1000 / 1000\n\n    def validate(self, data: ndarray) -> Optional[InferredField]:\n        if not self.is_validator_type_valid(dtype=data.dtype):\n            return None\n\n        try:\n            selected_resolution = self._unix_timestamp_resolution(float(data[0]))\n            if not selected_resolution:\n                return None\n            for value in data:\n                if not math.isnan(value):\n                    parsed_value = datetime.utcfromtimestamp(\n                        self._fix_input_resolution(value, selected_resolution)\n                    )\n                    if (\n                        parsed_value.year < self.min_valid_year\n                        or parsed_value.year > self.max_valid_year\n                    ):\n                        return None\n\n            return InferredField(\n                inferred_type=self.name, inferred_pattern=selected_resolution\n            )\n        except ValueError:\n            return None\n",
    "suffix": ""
  },
  {
    "name": "kokiez/solana-sniper:alreadyBought.py@1525",
    "canonical_solution": "    sendWebhook(f\"a|Token INFO SAVE {token_symbol}\", f\"Settings saved in 'previousSELLBUYINFO.json'.\")\r",
    "prompt": "import os, sys\r\nimport json\r\nfrom webhook import sendWebhook\r\nfrom birdeye import getSymbol\r\n\r\ndef write_token_to_file(token_address):\r\n    file_path = os.path.join(sys.path[0], 'data', 'alreadyBoughtTokens.json')\r\n\r\n    # Load the JSON file\r\n    with open(file_path, 'r') as file:\r\n        data = json.load(file)\r\n\r\n    # Check if the 'tokens' key exists in the JSON object\r\n    if 'tokens' in data:\r\n        # If it exists, append the new tokens to the existing list\r\n        data['tokens'].extend([token_address])\r\n    else:\r\n        # If it doesn't exist, create a new list with the new tokens\r\n        data['tokens'] = [token_address]\r\n\r\n    # Write the updated data back to the file\r\n    with open(file_path, 'w') as file:\r\n        json.dump(data, file, indent=4)\r\n\r\n    print(f\"Token address [{token_address}] saved in 'alreadyBoughtTokens.json'.\")\r\n\r\n# def check_if_already_bought(token_address):\r\ndef check_token_existence(token_to_check):\r\n    token_symbol, SOl_Symbol = getSymbol(token_to_check)\r\n\r\n    file_path = os.path.join(sys.path[0], 'data', 'alreadyBoughtTokens.json')\r\n\r\n    # Open the JSON file in read mode ('r')\r\n    with open(file_path, 'r') as file:\r\n        # Load the JSON data into a dictionary\r\n        token_data = json.load(file)\r\n\r\n    # Check if the token exists in the JSON data\r\n    if token_to_check in token_data['tokens']:\r\n        print(f\"[{token_to_check}] already exists in 'alreadyBoughtTokens.json'.\")\r\n        sendWebhook(f\"a|Token SAVE {token_symbol}\", f\"[{token_to_check}] already exists in 'alreadyBoughtTokens.json'.\")\r\n        return True\r\n    return False\r\n\r\n\r\ndef storeSettings(amm,\r\n                  desired_token_address,\r\n                  txB,\r\n                  execution_time,\r\n                  limit_order_sell_Bool,\r\n                  take_profit_ratio,\r\n                  trailing_stop_Bool,\r\n                  trailing_stop_ratio,\r\n                  Limit_and_Trailing_Stop_Bool,\r\n                  bought_token_price):\r\n    \r\n    token_symbol, SOl_Symbol = getSymbol(desired_token_address)\r\n\r\n    file_path = os.path.join(sys.path[0], 'data', 'previousSELLBUYINFO.json')\r\n\r\n    # Define the settings\r\n    settings = {\r\n         'amm': amm,\r\n            'txB': str(txB),\r\n            'execution_time': execution_time,\r\n            'limit_order_sell_Bool': limit_order_sell_Bool,\r\n            'take_profit_ratio': take_profit_ratio,\r\n            'trailing_stop_Bool': trailing_stop_Bool,\r\n            'trailing_stop_ratio': trailing_stop_ratio,\r\n            'Limit_and_Trailing_Stop_Bool': Limit_and_Trailing_Stop_Bool,\r\n            'bought_token_price': bought_token_price\r\n    }\r\n\r\n    # Load the JSON file\r\n    with open(file_path, 'r') as file:\r\n        data = json.load(file)\r\n\r\n    # Append the settings to the JSON object\r\n    data[desired_token_address] = settings\r\n\r\n    # Write the updated data back to the file\r\n    with open(file_path, 'w') as file:\r\n        json.dump(data, file, indent=4)\r\n\r\n    print(\"Settings saved in 'previousSELLBUYINFO.json'.\")\r",
    "prefix": "import os, sys\r\nimport json\r\nfrom webhook import sendWebhook\r\nfrom birdeye import getSymbol\r\n\r\ndef write_token_to_file(token_address):\r\n    file_path = os.path.join(sys.path[0], 'data', 'alreadyBoughtTokens.json')\r\n\r\n    # Load the JSON file\r\n    with open(file_path, 'r') as file:\r\n        data = json.load(file)\r\n\r\n    # Check if the 'tokens' key exists in the JSON object\r\n    if 'tokens' in data:\r\n        # If it exists, append the new tokens to the existing list\r\n        data['tokens'].extend([token_address])\r\n    else:\r\n        # If it doesn't exist, create a new list with the new tokens\r\n        data['tokens'] = [token_address]\r\n\r\n    # Write the updated data back to the file\r\n    with open(file_path, 'w') as file:\r\n        json.dump(data, file, indent=4)\r\n\r\n    print(f\"Token address [{token_address}] saved in 'alreadyBoughtTokens.json'.\")\r\n\r\n# def check_if_already_bought(token_address):\r\ndef check_token_existence(token_to_check):\r\n    token_symbol, SOl_Symbol = getSymbol(token_to_check)\r\n\r\n    file_path = os.path.join(sys.path[0], 'data', 'alreadyBoughtTokens.json')\r\n\r\n    # Open the JSON file in read mode ('r')\r\n    with open(file_path, 'r') as file:\r\n        # Load the JSON data into a dictionary\r\n        token_data = json.load(file)\r\n\r\n    # Check if the token exists in the JSON data\r\n    if token_to_check in token_data['tokens']:\r\n        print(f\"[{token_to_check}] already exists in 'alreadyBoughtTokens.json'.\")\r\n        sendWebhook(f\"a|Token SAVE {token_symbol}\", f\"[{token_to_check}] already exists in 'alreadyBoughtTokens.json'.\")\r\n        return True\r\n    return False\r\n\r\n\r\ndef storeSettings(amm,\r\n                  desired_token_address,\r\n                  txB,\r\n                  execution_time,\r\n                  limit_order_sell_Bool,\r\n                  take_profit_ratio,\r\n                  trailing_stop_Bool,\r\n                  trailing_stop_ratio,\r\n                  Limit_and_Trailing_Stop_Bool,\r\n                  bought_token_price):\r\n    \r\n    token_symbol, SOl_Symbol = getSymbol(desired_token_address)\r\n\r\n    file_path = os.path.join(sys.path[0], 'data', 'previousSELLBUYINFO.json')\r\n\r\n    # Define the settings\r\n    settings = {\r\n         'amm': amm,\r\n            'txB': str(txB),\r\n            'execution_time': execution_time,\r\n            'limit_order_sell_Bool': limit_order_sell_Bool,\r\n            'take_profit_ratio': take_profit_ratio,\r\n            'trailing_stop_Bool': trailing_stop_Bool,\r\n            'trailing_stop_ratio': trailing_stop_ratio,\r\n            'Limit_and_Trailing_Stop_Bool': Limit_and_Trailing_Stop_Bool,\r\n            'bought_token_price': bought_token_price\r\n    }\r\n\r\n    # Load the JSON file\r\n    with open(file_path, 'r') as file:\r\n        data = json.load(file)\r\n\r\n    # Append the settings to the JSON object\r\n    data[desired_token_address] = settings\r\n\r\n    # Write the updated data back to the file\r\n    with open(file_path, 'w') as file:\r\n        json.dump(data, file, indent=4)\r\n\r\n    print(\"Settings saved in 'previousSELLBUYINFO.json'.\")\r",
    "suffix": ""
  },
  {
    "name": "zy7y/dfs-generate:main.py@1070",
    "canonical_solution": "    return R.error(msg=\"\u8bf7\u786e\u8ba4\u4fe1\u606f\u662f\u5426\u586b\u5199\u6b63\u786e\")",
    "prompt": "from fastapi import FastAPI, Query\nfrom fastapi.requests import Request\nfrom fastapi.responses import FileResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom entity import CodeGen, Conf, DBConf, R, RList, Table\nfrom generate.main import generate_code\n    import uvicorn\n\n\napp = FastAPI(\n    title=\"dfs-generate\", description=\"FastAPI SQLModel \u9006\u5411\u751f\u6210\u4ee3\u7801\", docs_url=None\n)\n\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n\n@app.get(\"/\", include_in_schema=False)\ndef index():\n    return FileResponse(\"static/index.html\")\n\n\n@app.get(\"/tables\", response_model=RList[Table])\ndef query_by_table_name_limit(\n    request: Request, table_name: str = Query(\"\", alias=\"tableName\")\n):\n    total = []\n    try:\n        uri, metadata = Conf.get_last_uri_with_metadata()\n        request.app.state.uri = uri\n        request.app.state.metadata = metadata\n        for _, table in metadata.tables.items():\n            if table_name in table.name:\n                total.append(dict(table_name=table.name, table_comment=table.comment))\n        return RList.success(data=total)\n\n    except Exception as e:\n        print(e)\n    return RList.error(msg=\"\u8bf7\u5148\u53bb\u914d\u7f6e\u6570\u636e\u5e93\")\n\n\n@app.get(\"/codegen\", response_model=RList[CodeGen])\ndef get_codegen_by_table_name(\n    request: Request, table_name: str = Query(..., alias=\"tableName\")\n):\n    try:\n        table = request.app.state.metadata.tables[table_name]\n        uri = request.app.state.uri\n\n        return RList.success(data=generate_code(table, uri))\n\n    except Exception as e:\n        print(e)\n    return RList.error(msg=\"\u8868\u4e0d\u5b58\u5728 / \u8fde\u63a5\u5931\u8d25\")\n\n\n@app.post(\"/conf\")\ndef change_db(conf: DBConf):\n    try:\n        conf.get_metadata()\n        Conf.create(conf.get_db_uri())\n        return R.success(msg=\"\u8bbe\u7f6e\u6210\u529f\")\n    except Exception as e:\n        print(e)",
    "prefix": "from fastapi import FastAPI, Query\nfrom fastapi.requests import Request\nfrom fastapi.responses import FileResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom entity import CodeGen, Conf, DBConf, R, RList, Table\nfrom generate.main import generate_code\n    import uvicorn\n\n\napp = FastAPI(\n    title=\"dfs-generate\", description=\"FastAPI SQLModel \u9006\u5411\u751f\u6210\u4ee3\u7801\", docs_url=None\n)\n\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n\n@app.get(\"/\", include_in_schema=False)\ndef index():\n    return FileResponse(\"static/index.html\")\n\n\n@app.get(\"/tables\", response_model=RList[Table])\ndef query_by_table_name_limit(\n    request: Request, table_name: str = Query(\"\", alias=\"tableName\")\n):\n    total = []\n    try:\n        uri, metadata = Conf.get_last_uri_with_metadata()\n        request.app.state.uri = uri\n        request.app.state.metadata = metadata\n        for _, table in metadata.tables.items():\n            if table_name in table.name:\n                total.append(dict(table_name=table.name, table_comment=table.comment))\n        return RList.success(data=total)\n\n    except Exception as e:\n        print(e)\n    return RList.error(msg=\"\u8bf7\u5148\u53bb\u914d\u7f6e\u6570\u636e\u5e93\")\n\n\n@app.get(\"/codegen\", response_model=RList[CodeGen])\ndef get_codegen_by_table_name(\n    request: Request, table_name: str = Query(..., alias=\"tableName\")\n):\n    try:\n        table = request.app.state.metadata.tables[table_name]\n        uri = request.app.state.uri\n\n        return RList.success(data=generate_code(table, uri))\n\n    except Exception as e:\n        print(e)\n    return RList.error(msg=\"\u8868\u4e0d\u5b58\u5728 / \u8fde\u63a5\u5931\u8d25\")\n\n\n@app.post(\"/conf\")\ndef change_db(conf: DBConf):\n    try:\n        conf.get_metadata()\n        Conf.create(conf.get_db_uri())\n        return R.success(msg=\"\u8bbe\u7f6e\u6210\u529f\")\n    except Exception as e:\n        print(e)",
    "suffix": ""
  },
  {
    "name": "KyanChen/TTP:mmdet/models/losses/ddq_detr_aux_loss.py@1586",
    "canonical_solution": "        bbox_avg_factor = reduce_mean(",
    "prompt": "import torch\nimport torch.nn as nn\nfrom mmengine.structures import BaseDataElement\nfrom mmdet.models.utils import multi_apply\nfrom mmdet.registry import MODELS, TASK_UTILS\nfrom mmdet.utils import reduce_mean\n# Copyright (c) OpenMMLab. All rights reserved.\n\n\n\nclass DDQAuxLoss(nn.Module):\n    \"\"\"DDQ auxiliary branches loss for dense queries.\n\n    Args:\n        loss_cls (dict):\n            Configuration of classification loss function.\n        loss_bbox (dict):\n            Configuration of bbox regression loss function.\n        train_cfg (dict):\n            Configuration of gt targets assigner for each predicted bbox.\n    \"\"\"\n\n    def __init__(\n        self,\n        loss_cls=dict(\n            type='QualityFocalLoss',\n            use_sigmoid=True,\n            activated=True,  # use probability instead of logit as input\n            beta=2.0,\n            loss_weight=1.0),\n        loss_bbox=dict(type='GIoULoss', loss_weight=2.0),\n        train_cfg=dict(\n            assigner=dict(type='TopkHungarianAssigner', topk=8),\n            alpha=1,\n            beta=6),\n    ):\n        super(DDQAuxLoss, self).__init__()\n        self.train_cfg = train_cfg\n        self.loss_cls = MODELS.build(loss_cls)\n        self.loss_bbox = MODELS.build(loss_bbox)\n        self.assigner = TASK_UTILS.build(self.train_cfg['assigner'])\n\n        sampler_cfg = dict(type='PseudoSampler')\n        self.sampler = TASK_UTILS.build(sampler_cfg)\n\n    def loss_single(self, cls_score, bbox_pred, labels, label_weights,\n                    bbox_targets, alignment_metrics):\n        \"\"\"Calculate auxiliary branches loss for dense queries for one image.\n\n        Args:\n            cls_score (Tensor): Predicted normalized classification\n                scores for one image, has shape (num_dense_queries,\n                cls_out_channels).\n            bbox_pred (Tensor): Predicted unnormalized bbox coordinates\n                for one image, has shape (num_dense_queries, 4) with the\n                last dimension arranged as (x1, y1, x2, y2).\n            labels (Tensor): Labels for one image.\n            label_weights (Tensor): Label weights for one image.\n            bbox_targets (Tensor): Bbox targets for one image.\n            alignment_metrics (Tensor): Normalized alignment metrics for one\n                image.\n\n        Returns:\n            tuple: A tuple of loss components and loss weights.\n        \"\"\"\n        bbox_targets = bbox_targets.reshape(-1, 4)\n        labels = labels.reshape(-1)\n        alignment_metrics = alignment_metrics.reshape(-1)\n        label_weights = label_weights.reshape(-1)\n        targets = (labels, alignment_metrics)\n        cls_loss_func = self.loss_cls\n\n        loss_cls = cls_loss_func(\n            cls_score, targets, label_weights, avg_factor=1.0)\n\n        # FG cat_id: [0, num_classes -1], BG cat_id: num_classes\n        bg_class_ind = cls_score.size(-1)\n        pos_inds = ((labels >= 0)\n                    & (labels < bg_class_ind)).nonzero().squeeze(1)\n\n        if len(pos_inds) > 0:\n            pos_bbox_targets = bbox_targets[pos_inds]\n            pos_bbox_pred = bbox_pred[pos_inds]\n\n            pos_decode_bbox_pred = pos_bbox_pred\n            pos_decode_bbox_targets = pos_bbox_targets\n\n            # regression loss\n            pos_bbox_weight = alignment_metrics[pos_inds]\n\n            loss_bbox = self.loss_bbox(\n                pos_decode_bbox_pred,\n                pos_decode_bbox_targets,\n                weight=pos_bbox_weight,\n                avg_factor=1.0)\n        else:\n            loss_bbox = bbox_pred.sum() * 0\n            pos_bbox_weight = bbox_targets.new_tensor(0.)\n\n        return loss_cls, loss_bbox, alignment_metrics.sum(\n        ), pos_bbox_weight.sum()\n\n    def loss(self, cls_scores, bbox_preds, gt_bboxes, gt_labels, img_metas,\n             **kwargs):\n        \"\"\"Calculate auxiliary branches loss for dense queries.\n\n        Args:\n            cls_scores (Tensor): Predicted normalized classification\n                scores, has shape (bs, num_dense_queries,\n                cls_out_channels).\n            bbox_preds (Tensor): Predicted unnormalized bbox coordinates,\n                has shape (bs, num_dense_queries, 4) with the last\n                dimension arranged as (x1, y1, x2, y2).\n            gt_bboxes (list[Tensor]): List of unnormalized ground truth\n                bboxes for each image, each has shape (num_gt, 4) with the\n                last dimension arranged as (x1, y1, x2, y2).\n                NOTE: num_gt is dynamic for each image.\n            gt_labels (list[Tensor]): List of ground truth classification\n                index for each image, each has shape (num_gt,).\n                NOTE: num_gt is dynamic for each image.\n            img_metas (list[dict]): Meta information for one image,\n                e.g., image size, scaling factor, etc.\n\n        Returns:\n            dict: A dictionary of loss components.\n        \"\"\"\n        flatten_cls_scores = cls_scores\n        flatten_bbox_preds = bbox_preds\n\n        cls_reg_targets = self.get_targets(\n            flatten_cls_scores,\n            flatten_bbox_preds,\n            gt_bboxes,\n            img_metas,\n            gt_labels_list=gt_labels,\n        )\n        (labels_list, label_weights_list, bbox_targets_list,\n         alignment_metrics_list) = cls_reg_targets\n\n        losses_cls, losses_bbox, \\\n            cls_avg_factors, bbox_avg_factors = multi_apply(\n                self.loss_single,\n                flatten_cls_scores,\n                flatten_bbox_preds,\n                labels_list,\n                label_weights_list,\n                bbox_targets_list,\n                alignment_metrics_list,\n                )\n\n        cls_avg_factor = reduce_mean(sum(cls_avg_factors)).clamp_(min=1).item()\n        losses_cls = list(map(lambda x: x / cls_avg_factor, losses_cls))\n",
    "prefix": "import torch\nimport torch.nn as nn\nfrom mmengine.structures import BaseDataElement\nfrom mmdet.models.utils import multi_apply\nfrom mmdet.registry import MODELS, TASK_UTILS\nfrom mmdet.utils import reduce_mean\n# Copyright (c) OpenMMLab. All rights reserved.\n\n\n\nclass DDQAuxLoss(nn.Module):\n    \"\"\"DDQ auxiliary branches loss for dense queries.\n\n    Args:\n        loss_cls (dict):\n            Configuration of classification loss function.\n        loss_bbox (dict):\n            Configuration of bbox regression loss function.\n        train_cfg (dict):\n            Configuration of gt targets assigner for each predicted bbox.\n    \"\"\"\n\n    def __init__(\n        self,\n        loss_cls=dict(\n            type='QualityFocalLoss',\n            use_sigmoid=True,\n            activated=True,  # use probability instead of logit as input\n            beta=2.0,\n            loss_weight=1.0),\n        loss_bbox=dict(type='GIoULoss', loss_weight=2.0),\n        train_cfg=dict(\n            assigner=dict(type='TopkHungarianAssigner', topk=8),\n            alpha=1,\n            beta=6),\n    ):\n        super(DDQAuxLoss, self).__init__()\n        self.train_cfg = train_cfg\n        self.loss_cls = MODELS.build(loss_cls)\n        self.loss_bbox = MODELS.build(loss_bbox)\n        self.assigner = TASK_UTILS.build(self.train_cfg['assigner'])\n\n        sampler_cfg = dict(type='PseudoSampler')\n        self.sampler = TASK_UTILS.build(sampler_cfg)\n\n    def loss_single(self, cls_score, bbox_pred, labels, label_weights,\n                    bbox_targets, alignment_metrics):\n        \"\"\"Calculate auxiliary branches loss for dense queries for one image.\n\n        Args:\n            cls_score (Tensor): Predicted normalized classification\n                scores for one image, has shape (num_dense_queries,\n                cls_out_channels).\n            bbox_pred (Tensor): Predicted unnormalized bbox coordinates\n                for one image, has shape (num_dense_queries, 4) with the\n                last dimension arranged as (x1, y1, x2, y2).\n            labels (Tensor): Labels for one image.\n            label_weights (Tensor): Label weights for one image.\n            bbox_targets (Tensor): Bbox targets for one image.\n            alignment_metrics (Tensor): Normalized alignment metrics for one\n                image.\n\n        Returns:\n            tuple: A tuple of loss components and loss weights.\n        \"\"\"\n        bbox_targets = bbox_targets.reshape(-1, 4)\n        labels = labels.reshape(-1)\n        alignment_metrics = alignment_metrics.reshape(-1)\n        label_weights = label_weights.reshape(-1)\n        targets = (labels, alignment_metrics)\n        cls_loss_func = self.loss_cls\n\n        loss_cls = cls_loss_func(\n            cls_score, targets, label_weights, avg_factor=1.0)\n\n        # FG cat_id: [0, num_classes -1], BG cat_id: num_classes\n        bg_class_ind = cls_score.size(-1)\n        pos_inds = ((labels >= 0)\n                    & (labels < bg_class_ind)).nonzero().squeeze(1)\n\n        if len(pos_inds) > 0:\n            pos_bbox_targets = bbox_targets[pos_inds]\n            pos_bbox_pred = bbox_pred[pos_inds]\n\n            pos_decode_bbox_pred = pos_bbox_pred\n            pos_decode_bbox_targets = pos_bbox_targets\n\n            # regression loss\n            pos_bbox_weight = alignment_metrics[pos_inds]\n\n            loss_bbox = self.loss_bbox(\n                pos_decode_bbox_pred,\n                pos_decode_bbox_targets,\n                weight=pos_bbox_weight,\n                avg_factor=1.0)\n        else:\n            loss_bbox = bbox_pred.sum() * 0\n            pos_bbox_weight = bbox_targets.new_tensor(0.)\n\n        return loss_cls, loss_bbox, alignment_metrics.sum(\n        ), pos_bbox_weight.sum()\n\n    def loss(self, cls_scores, bbox_preds, gt_bboxes, gt_labels, img_metas,\n             **kwargs):\n        \"\"\"Calculate auxiliary branches loss for dense queries.\n\n        Args:\n            cls_scores (Tensor): Predicted normalized classification\n                scores, has shape (bs, num_dense_queries,\n                cls_out_channels).\n            bbox_preds (Tensor): Predicted unnormalized bbox coordinates,\n                has shape (bs, num_dense_queries, 4) with the last\n                dimension arranged as (x1, y1, x2, y2).\n            gt_bboxes (list[Tensor]): List of unnormalized ground truth\n                bboxes for each image, each has shape (num_gt, 4) with the\n                last dimension arranged as (x1, y1, x2, y2).\n                NOTE: num_gt is dynamic for each image.\n            gt_labels (list[Tensor]): List of ground truth classification\n                index for each image, each has shape (num_gt,).\n                NOTE: num_gt is dynamic for each image.\n            img_metas (list[dict]): Meta information for one image,\n                e.g., image size, scaling factor, etc.\n\n        Returns:\n            dict: A dictionary of loss components.\n        \"\"\"\n        flatten_cls_scores = cls_scores\n        flatten_bbox_preds = bbox_preds\n\n        cls_reg_targets = self.get_targets(\n            flatten_cls_scores,\n            flatten_bbox_preds,\n            gt_bboxes,\n            img_metas,\n            gt_labels_list=gt_labels,\n        )\n        (labels_list, label_weights_list, bbox_targets_list,\n         alignment_metrics_list) = cls_reg_targets\n\n        losses_cls, losses_bbox, \\\n            cls_avg_factors, bbox_avg_factors = multi_apply(\n                self.loss_single,\n                flatten_cls_scores,\n                flatten_bbox_preds,\n                labels_list,\n                label_weights_list,\n                bbox_targets_list,\n                alignment_metrics_list,\n                )\n\n        cls_avg_factor = reduce_mean(sum(cls_avg_factors)).clamp_(min=1).item()\n        losses_cls = list(map(lambda x: x / cls_avg_factor, losses_cls))\n",
    "suffix": ""
  },
  {
    "name": "m1stadev/apple-compress:apple_compress/compress.py@843",
    "canonical_solution": "            raise CompressionError('Failed to decompress data')",
    "prompt": "from ctypes import create_string_buffer\nfrom typing import Optional\nfrom loguru import logger\nfrom ._lib import _lib\nfrom .errors import CompressionError\nfrom .types import Algorithm\n\n\n\n\ndef compress(data: bytes, algorithm: Algorithm) -> bytes:\n    \"\"\"\n    Compresses the given data using the specified algorithm.\n\n    Args:\n        data (bytes): The data to be compressed.\n        algorithm (Algorithm): The compression algorithm to use.\n\n    Returns:\n        bytes: The compressed data.\n\n    Raises:\n        CompressionError: If the compression fails.\n    \"\"\"\n    logger.info(\n        f'Compressing data ({len(data)} bytes) with algorithm: {algorithm.name}'\n    )\n    logger.debug(f'Creating buffer of size: {len(data) + 256}')\n    dest_buf = create_string_buffer(len(data) + 256)\n\n    logger.debug('Calling compression_encode_buffer')\n    size = _lib.compression_encode_buffer(\n        dest_buf, len(data), data, len(data), None, algorithm.value\n    )\n    if size == 0:\n        raise CompressionError('Failed to compress data')\n\n    logger.success(\n        f'Compression with algorithm: {algorithm.name} successful, returning {size} bytes'\n    )\n    return dest_buf.raw[:size]\n\n\ndef decompress(\n    data: bytes, algorithm: Algorithm, decmp_size: Optional[int] = None\n) -> bytes:\n    \"\"\"\n    Decompresses the given data using the specified algorithm.\n\n    Args:\n        data (bytes): The compressed data to be decompressed.\n        algorithm (Algorithm): The compression algorithm to use for decompression.\n        decmp_size (Optional[int]): The expected size of the decompressed data. If not provided, it is twice the size of `data`. Providing this value is recommended on resource-limited systems.\n\n    Returns:\n        bytes: The decompressed data.\n\n    Raises:\n        CompressionError: If the decompression fails.\n    \"\"\"\n    logger.info(\n        f'Decompressing data ({len(data)} bytes) with algorithm: {algorithm.name}'\n    )\n\n    if decmp_size is None:\n        logger.debug(\n            f'No decompressed size provided, assuming data length * 2 ({len(data) * 2})'\n        )\n        decmp_size = len(data) * 2\n\n    while True:\n        logger.debug(f'Creating buffer of size: {decmp_size}')\n        dest_buf = create_string_buffer(decmp_size)\n\n        logger.debug('Calling compression_decode_buffer')\n        size = _lib.compression_decode_buffer(\n            dest_buf, decmp_size, data, len(data), None, algorithm.value\n        )\n        if size == 0:",
    "prefix": "from ctypes import create_string_buffer\nfrom typing import Optional\nfrom loguru import logger\nfrom ._lib import _lib\nfrom .errors import CompressionError\nfrom .types import Algorithm\n\n\n\n\ndef compress(data: bytes, algorithm: Algorithm) -> bytes:\n    \"\"\"\n    Compresses the given data using the specified algorithm.\n\n    Args:\n        data (bytes): The data to be compressed.\n        algorithm (Algorithm): The compression algorithm to use.\n\n    Returns:\n        bytes: The compressed data.\n\n    Raises:\n        CompressionError: If the compression fails.\n    \"\"\"\n    logger.info(\n        f'Compressing data ({len(data)} bytes) with algorithm: {algorithm.name}'\n    )\n    logger.debug(f'Creating buffer of size: {len(data) + 256}')\n    dest_buf = create_string_buffer(len(data) + 256)\n\n    logger.debug('Calling compression_encode_buffer')\n    size = _lib.compression_encode_buffer(\n        dest_buf, len(data), data, len(data), None, algorithm.value\n    )\n    if size == 0:\n        raise CompressionError('Failed to compress data')\n\n    logger.success(\n        f'Compression with algorithm: {algorithm.name} successful, returning {size} bytes'\n    )\n    return dest_buf.raw[:size]\n\n\ndef decompress(\n    data: bytes, algorithm: Algorithm, decmp_size: Optional[int] = None\n) -> bytes:\n    \"\"\"\n    Decompresses the given data using the specified algorithm.\n\n    Args:\n        data (bytes): The compressed data to be decompressed.\n        algorithm (Algorithm): The compression algorithm to use for decompression.\n        decmp_size (Optional[int]): The expected size of the decompressed data. If not provided, it is twice the size of `data`. Providing this value is recommended on resource-limited systems.\n\n    Returns:\n        bytes: The decompressed data.\n\n    Raises:\n        CompressionError: If the decompression fails.\n    \"\"\"\n    logger.info(\n        f'Decompressing data ({len(data)} bytes) with algorithm: {algorithm.name}'\n    )\n\n    if decmp_size is None:\n        logger.debug(\n            f'No decompressed size provided, assuming data length * 2 ({len(data) * 2})'\n        )\n        decmp_size = len(data) * 2\n\n    while True:\n        logger.debug(f'Creating buffer of size: {decmp_size}')\n        dest_buf = create_string_buffer(decmp_size)\n\n        logger.debug('Calling compression_decode_buffer')\n        size = _lib.compression_decode_buffer(\n            dest_buf, decmp_size, data, len(data), None, algorithm.value\n        )\n        if size == 0:",
    "suffix": ""
  },
  {
    "name": "dan-r/HomeAssistant-Ohme:custom_components/ohme/number.py@870",
    "canonical_solution": "        if session_in_progress(self.coordinator.data):",
    "prompt": "import asyncio\nfrom homeassistant.components.number import NumberEntity, NumberDeviceClass\nfrom homeassistant.helpers.entity import generate_entity_id\nfrom homeassistant.core import callback, HomeAssistant\nfrom .const import DOMAIN, DATA_CLIENT, DATA_COORDINATORS, COORDINATOR_CHARGESESSIONS, COORDINATOR_SCHEDULES\nfrom .utils import session_in_progress\nfrom __future__ import annotations\n\nasync def async_setup_entry(\n    hass: HomeAssistant,\n    config_entry: config_entries.ConfigEntry,\n    async_add_entities\n):\n    \"\"\"Setup switches and configure coordinator.\"\"\"\n    coordinators = hass.data[DOMAIN][DATA_COORDINATORS]\n\n    client = hass.data[DOMAIN][DATA_CLIENT]\n\n    numbers = [TargetPercentNumber(\n        coordinators[COORDINATOR_CHARGESESSIONS], coordinators[COORDINATOR_SCHEDULES], hass, client)]\n\n    async_add_entities(numbers, update_before_add=True)\n\n\nclass TargetPercentNumber(NumberEntity):\n    \"\"\"Target percentage sensor.\"\"\"\n    _attr_name = \"Target Percentage\"\n    _attr_device_class = NumberDeviceClass.BATTERY\n    _attr_suggested_display_precision = 0\n\n    def __init__(self, coordinator, coordinator_schedules, hass: HomeAssistant, client):\n        self.coordinator = coordinator\n        self.coordinator_schedules = coordinator_schedules\n\n        self._client = client\n\n        self._state = None\n        self._last_updated = None\n        self._attributes = {}\n\n        self.entity_id = generate_entity_id(\n            \"number.{}\", \"ohme_target_percent\", hass=hass)\n\n        self._attr_device_info = client.get_device_info()\n\n    async def async_added_to_hass(self) -> None:\n        \"\"\"When entity is added to hass.\"\"\"\n        await super().async_added_to_hass()\n        self.async_on_remove(\n            self.coordinator.async_add_listener(\n                self._handle_coordinator_update, None\n            )\n        )\n        self.async_on_remove(\n            self.coordinator_schedules.async_add_listener(\n                self._handle_coordinator_update, None\n            )\n        )\n        \n    @property\n    def unique_id(self):\n        \"\"\"The unique ID of the switch.\"\"\"\n        return self._client.get_unique_id(\"target_percent\")\n\n    async def async_set_native_value(self, value: float) -> None:\n        \"\"\"Update the current value.\"\"\"\n        # If session in progress, update this session, if not update the first schedule\n        if session_in_progress(self.coordinator.data):\n            await self._client.async_apply_session_rule(target_percent=int(value))\n            await asyncio.sleep(1)\n            await self.coordinator.async_refresh()\n        else:\n            await self._client.async_update_schedule(target_percent=int(value))\n            await asyncio.sleep(1)\n            await self.coordinator_schedules.async_refresh()\n\n    @property\n    def icon(self):\n        \"\"\"Icon of the sensor.\"\"\"\n        return \"mdi:battery-heart\"\n\n    @callback\n    def _handle_coordinator_update(self) -> None:\n        \"\"\"Get value from data returned from API by coordinator\"\"\"\n        # Set with the same logic as reading",
    "prefix": "import asyncio\nfrom homeassistant.components.number import NumberEntity, NumberDeviceClass\nfrom homeassistant.helpers.entity import generate_entity_id\nfrom homeassistant.core import callback, HomeAssistant\nfrom .const import DOMAIN, DATA_CLIENT, DATA_COORDINATORS, COORDINATOR_CHARGESESSIONS, COORDINATOR_SCHEDULES\nfrom .utils import session_in_progress\nfrom __future__ import annotations\n\nasync def async_setup_entry(\n    hass: HomeAssistant,\n    config_entry: config_entries.ConfigEntry,\n    async_add_entities\n):\n    \"\"\"Setup switches and configure coordinator.\"\"\"\n    coordinators = hass.data[DOMAIN][DATA_COORDINATORS]\n\n    client = hass.data[DOMAIN][DATA_CLIENT]\n\n    numbers = [TargetPercentNumber(\n        coordinators[COORDINATOR_CHARGESESSIONS], coordinators[COORDINATOR_SCHEDULES], hass, client)]\n\n    async_add_entities(numbers, update_before_add=True)\n\n\nclass TargetPercentNumber(NumberEntity):\n    \"\"\"Target percentage sensor.\"\"\"\n    _attr_name = \"Target Percentage\"\n    _attr_device_class = NumberDeviceClass.BATTERY\n    _attr_suggested_display_precision = 0\n\n    def __init__(self, coordinator, coordinator_schedules, hass: HomeAssistant, client):\n        self.coordinator = coordinator\n        self.coordinator_schedules = coordinator_schedules\n\n        self._client = client\n\n        self._state = None\n        self._last_updated = None\n        self._attributes = {}\n\n        self.entity_id = generate_entity_id(\n            \"number.{}\", \"ohme_target_percent\", hass=hass)\n\n        self._attr_device_info = client.get_device_info()\n\n    async def async_added_to_hass(self) -> None:\n        \"\"\"When entity is added to hass.\"\"\"\n        await super().async_added_to_hass()\n        self.async_on_remove(\n            self.coordinator.async_add_listener(\n                self._handle_coordinator_update, None\n            )\n        )\n        self.async_on_remove(\n            self.coordinator_schedules.async_add_listener(\n                self._handle_coordinator_update, None\n            )\n        )\n        \n    @property\n    def unique_id(self):\n        \"\"\"The unique ID of the switch.\"\"\"\n        return self._client.get_unique_id(\"target_percent\")\n\n    async def async_set_native_value(self, value: float) -> None:\n        \"\"\"Update the current value.\"\"\"\n        # If session in progress, update this session, if not update the first schedule\n        if session_in_progress(self.coordinator.data):\n            await self._client.async_apply_session_rule(target_percent=int(value))\n            await asyncio.sleep(1)\n            await self.coordinator.async_refresh()\n        else:\n            await self._client.async_update_schedule(target_percent=int(value))\n            await asyncio.sleep(1)\n            await self.coordinator_schedules.async_refresh()\n\n    @property\n    def icon(self):\n        \"\"\"Icon of the sensor.\"\"\"\n        return \"mdi:battery-heart\"\n\n    @callback\n    def _handle_coordinator_update(self) -> None:\n        \"\"\"Get value from data returned from API by coordinator\"\"\"\n        # Set with the same logic as reading",
    "suffix": ""
  },
  {
    "name": "Almas-Ali/SpyIP:spyip/backend.py@1530",
    "canonical_solution": "            return IPResponse(**res.json())",
    "prompt": "from typing import List, Union\nfrom .exceptions import (\n    TooManyRequests,\n    ConnectionTimeout,\n    StatusError,\n)\nfrom .models import (\n    IPResponse,\n    DNSResponse,\n)\nimport asyncio\nimport random\nimport string\nimport httpx\n\n\n\n\ndef get_random_string(length: int = 32) -> str:\n    \"\"\"Generate a random string of fixed length.\"\"\"\n    letters = string.ascii_lowercase + string.digits\n    return ''.join(random.sample(letters, length))\n\n\n# API endpoints for IP address lookup\ntrace_me_url = 'http://ip-api.com/json/'\ntrace_ip_url = 'http://ip-api.com/json/%(query)s'\ntrace_dns_url = f'http://{get_random_string(32)}.edns.ip-api.com/json/'\ntrace_ip_batch_url = 'http://ip-api.com/batch'\n\nheaders = {\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n    'Accept-Encoding': 'gzip, deflate',\n    'Accept-Language': 'en-US,en;q=0.5',\n    'Connection': 'keep-alive',\n    'Upgrade-Insecure-Requests': '1',\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n}\n\n\ndef trace_me(\n    timeout: int = 5,\n    lang: str = 'en',\n) -> Union[IPResponse, None]:\n    \"\"\"Trace your own IP address.\"\"\"\n    try:\n        res = httpx.get(\n            url=trace_me_url,\n            params={'fields': 66842623, 'lang': lang},\n            headers=headers,\n            timeout=timeout,\n        )\n        if res.status_code == 200:\n            return IPResponse(**res.json())\n        else:\n            raise StatusError(f'Invalid status code: {res.status_code}. Expected 200.')\n\n    # 408 Request Timeout\n    except httpx._exceptions.ConnectTimeout:\n        raise ConnectionTimeout(\n            'Connection timeout. The server timed out waiting for the request. According to the HTTP specification, the client is allowed to repeat the request again after some time.'\n        )\n    # 429 Too Many Requests\n    except httpx._exceptions.TooManyRedirects:\n        raise TooManyRequests(\n            'Too many requests. Our endpoints are limited to 45 HTTP requests per minute from an IP address. If you go over this limit your requests will be throttled (HTTP 429) until your rate limit window is reset.'\n        )\n\n\ndef trace_ip(\n    query: str,\n    timeout: int = 5,\n    lang: str = 'en',\n) -> IPResponse:\n    \"\"\"Trace IP address\"\"\"\n    try:\n        res = httpx.get(\n            url=trace_ip_url % {'query': query},\n            params={'fields': 66842623, 'lang': lang},\n            headers=headers,\n            timeout=timeout,\n        )\n        if res.status_code == 200:",
    "prefix": "from typing import List, Union\nfrom .exceptions import (\n    TooManyRequests,\n    ConnectionTimeout,\n    StatusError,\n)\nfrom .models import (\n    IPResponse,\n    DNSResponse,\n)\nimport asyncio\nimport random\nimport string\nimport httpx\n\n\n\n\ndef get_random_string(length: int = 32) -> str:\n    \"\"\"Generate a random string of fixed length.\"\"\"\n    letters = string.ascii_lowercase + string.digits\n    return ''.join(random.sample(letters, length))\n\n\n# API endpoints for IP address lookup\ntrace_me_url = 'http://ip-api.com/json/'\ntrace_ip_url = 'http://ip-api.com/json/%(query)s'\ntrace_dns_url = f'http://{get_random_string(32)}.edns.ip-api.com/json/'\ntrace_ip_batch_url = 'http://ip-api.com/batch'\n\nheaders = {\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n    'Accept-Encoding': 'gzip, deflate',\n    'Accept-Language': 'en-US,en;q=0.5',\n    'Connection': 'keep-alive',\n    'Upgrade-Insecure-Requests': '1',\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n}\n\n\ndef trace_me(\n    timeout: int = 5,\n    lang: str = 'en',\n) -> Union[IPResponse, None]:\n    \"\"\"Trace your own IP address.\"\"\"\n    try:\n        res = httpx.get(\n            url=trace_me_url,\n            params={'fields': 66842623, 'lang': lang},\n            headers=headers,\n            timeout=timeout,\n        )\n        if res.status_code == 200:\n            return IPResponse(**res.json())\n        else:\n            raise StatusError(f'Invalid status code: {res.status_code}. Expected 200.')\n\n    # 408 Request Timeout\n    except httpx._exceptions.ConnectTimeout:\n        raise ConnectionTimeout(\n            'Connection timeout. The server timed out waiting for the request. According to the HTTP specification, the client is allowed to repeat the request again after some time.'\n        )\n    # 429 Too Many Requests\n    except httpx._exceptions.TooManyRedirects:\n        raise TooManyRequests(\n            'Too many requests. Our endpoints are limited to 45 HTTP requests per minute from an IP address. If you go over this limit your requests will be throttled (HTTP 429) until your rate limit window is reset.'\n        )\n\n\ndef trace_ip(\n    query: str,\n    timeout: int = 5,\n    lang: str = 'en',\n) -> IPResponse:\n    \"\"\"Trace IP address\"\"\"\n    try:\n        res = httpx.get(\n            url=trace_ip_url % {'query': query},\n            params={'fields': 66842623, 'lang': lang},\n            headers=headers,\n            timeout=timeout,\n        )\n        if res.status_code == 200:",
    "suffix": ""
  },
  {
    "name": "Emperor-WS/PyEmber:ember/tensor.py@743",
    "canonical_solution": "    if not cuda_available():",
    "prompt": "import numpy as np\n    import cupy as cp\nimport ember\nfrom ember.cuda import numpy_or_cupy, cuda_available\ntry:\nexcept ModuleNotFoundError:\n    pass\n\n\n\n# Custom exception for backward call error\nclass BackwardCallError(Exception):\n    pass\n\n\n# Custom exception for CUDA not available error\nclass CUDANotAvailableError(Exception):\n    pass\n\n\ndef tensor2string(\n        tensor, prefix=\"\",\n        precision=4,\n        separator=', ',\n        floatmode=None,\n        edgeitems=3,\n        threshold=100,\n        max_line_width=100,\n        suppress_small=True\n):\n    \"\"\"\n    Convert a tensor to a formatted string.\n\n    Args:\n        tensor: Input tensor.\n        prefix (str): Prefix for each line in the string.\n        precision (int): Number of decimal places for floating-point numbers.\n        separator (str): Separator between elements in the string.\n        floatmode: Float formatting mode.\n        edgeitems (int): Number of items at the beginning and end of each dimension to show.\n        threshold (int): Total number of array elements to trigger summarization.\n        max_line_width (int): Maximum width (in characters) of the string.\n        suppress_small (bool): Whether to suppress small numbers.\n\n    Returns:\n        str: Formatted string representation of the tensor.\n    \"\"\"\n    nc = numpy_or_cupy(tensor)\n    array_str = nc.array_str(tensor.data,\n                             precision=precision,\n                             max_line_width=max_line_width,\n                             suppress_small=suppress_small)\n\n    array_str = f\"\\n{prefix}\".join(array_str.split(\"\\n\"))\n    return array_str\n\n\ndef to_numpy(arrayable):\n    \"\"\"\n    Convert an array-like object to a NumPy array.\n\n    Args:\n        arrayable: Input array-like object.\n\n    Returns:\n        np.ndarray: NumPy array.\n    \"\"\"\n    if isinstance(arrayable, Tensor):\n        return np.array(arrayable.data)\n    elif isinstance(arrayable, np.ndarray):\n        return arrayable\n    elif cuda_available() and isinstance(arrayable, cp.ndarray):\n        return cp.asnumpy(arrayable)\n    else:\n        return np.array(arrayable)\n\n\ndef to_cupy(arrayable):\n    \"\"\"\n    Convert an array-like object to a CuPy array.\n\n    Args:\n        arrayable: Input array-like object.\n\n    Returns:\n        cp.ndarray: CuPy array.\n\n    Raises:\n        CUDANotAvailableError: If CUDA is not available.\n    \"\"\"",
    "prefix": "import numpy as np\n    import cupy as cp\nimport ember\nfrom ember.cuda import numpy_or_cupy, cuda_available\ntry:\nexcept ModuleNotFoundError:\n    pass\n\n\n\n# Custom exception for backward call error\nclass BackwardCallError(Exception):\n    pass\n\n\n# Custom exception for CUDA not available error\nclass CUDANotAvailableError(Exception):\n    pass\n\n\ndef tensor2string(\n        tensor, prefix=\"\",\n        precision=4,\n        separator=', ',\n        floatmode=None,\n        edgeitems=3,\n        threshold=100,\n        max_line_width=100,\n        suppress_small=True\n):\n    \"\"\"\n    Convert a tensor to a formatted string.\n\n    Args:\n        tensor: Input tensor.\n        prefix (str): Prefix for each line in the string.\n        precision (int): Number of decimal places for floating-point numbers.\n        separator (str): Separator between elements in the string.\n        floatmode: Float formatting mode.\n        edgeitems (int): Number of items at the beginning and end of each dimension to show.\n        threshold (int): Total number of array elements to trigger summarization.\n        max_line_width (int): Maximum width (in characters) of the string.\n        suppress_small (bool): Whether to suppress small numbers.\n\n    Returns:\n        str: Formatted string representation of the tensor.\n    \"\"\"\n    nc = numpy_or_cupy(tensor)\n    array_str = nc.array_str(tensor.data,\n                             precision=precision,\n                             max_line_width=max_line_width,\n                             suppress_small=suppress_small)\n\n    array_str = f\"\\n{prefix}\".join(array_str.split(\"\\n\"))\n    return array_str\n\n\ndef to_numpy(arrayable):\n    \"\"\"\n    Convert an array-like object to a NumPy array.\n\n    Args:\n        arrayable: Input array-like object.\n\n    Returns:\n        np.ndarray: NumPy array.\n    \"\"\"\n    if isinstance(arrayable, Tensor):\n        return np.array(arrayable.data)\n    elif isinstance(arrayable, np.ndarray):\n        return arrayable\n    elif cuda_available() and isinstance(arrayable, cp.ndarray):\n        return cp.asnumpy(arrayable)\n    else:\n        return np.array(arrayable)\n\n\ndef to_cupy(arrayable):\n    \"\"\"\n    Convert an array-like object to a CuPy array.\n\n    Args:\n        arrayable: Input array-like object.\n\n    Returns:\n        cp.ndarray: CuPy array.\n\n    Raises:\n        CUDANotAvailableError: If CUDA is not available.\n    \"\"\"",
    "suffix": ""
  },
  {
    "name": "Hassi34/iot-device-identification:src/stage_03_preprocess_data.py@1210",
    "canonical_solution": "    gzip_np_arr(y_test, Y_TEST_FILE_PATH)",
    "prompt": "import argparse\nimport joblib\nimport pandas as pd\nfrom src.utils.common import read_yaml\nfrom src.utils.sys_logging import get_logger\nfrom sklearn.preprocessing import LabelEncoder\nfrom src.utils.common import write_dict_to_yaml\nfrom src.utils.data_ops import gzip_np_arr\nfrom sklearn.model_selection import train_test_split\nfrom src.utils.data_ops import get_fitted_pipeline\nfrom pathlib import Path\n\nSTAGE = \"Preprocess Data\"\n\n\ndef preprocess_data():\n    complete_df = pd.read_parquet(RAW_DATA_FILE_PATH)\n    logger.info(\n        f'The raw data file has been loaded from \"{RAW_DATA_FILE_PATH}\" with the shape \"{complete_df.shape}\"'\n    )\n    duplicate_rows = complete_df.duplicated().sum()\n    if duplicate_rows > 0:\n        logger.warning(\n            f\"Found {duplicate_rows} duplicate rows, removing duplicate rows...\"\n        )\n        complete_df = complete_df.drop_duplicates(keep=\"first\")\n    X = complete_df.drop([TARGET_COLUMN_NAME], axis=1)\n    y = complete_df[TARGET_COLUMN_NAME]\n    feature_cols = params[\"input_features_schema\"]\n    feature_cols = list(feature_cols.keys())\n    logger.info(f\"Read {len(feature_cols)} feature columns from params\")\n    data_processing_pipeline = get_fitted_pipeline(\n    X, feature_cols, KNN_IMPUTER_NEIGHBORS=KNN_IMPUTER_NEIGHBORS\n    )\n    Path(DATA_PREPROCESSING_PIPELINE_FILE_PATH).parent.absolute().mkdir(parents=True, exist_ok=True)\n    joblib.dump(data_processing_pipeline, DATA_PREPROCESSING_PIPELINE_FILE_PATH, compress=1)\n    logger.info(f\"Saved the preprocessing pipeline to {DATA_PREPROCESSING_PIPELINE_FILE_PATH}\")\n    data_processing_pipeline = joblib.load(DATA_PREPROCESSING_PIPELINE_FILE_PATH)\n    data_processing_pipeline\n    data_processing_pipeline = joblib.load(DATA_PREPROCESSING_PIPELINE_FILE_PATH)\n    logger.info(\n        f'Loaded sklearn data preprocessing pipeline from \"{DATA_PREPROCESSING_PIPELINE_FILE_PATH}\"'\n    )\n    X_transformed = data_processing_pipeline.transform(X)\n    logger.info(f'Dataframe shape after transformation is \"{X_transformed.shape}\"')\n\n    le = LabelEncoder()\n    le.fit(y)\n    labels_mapping_dict = {\"labels_mapping\": \"\"}\n    le_dict = dict(zip(le.transform(le.classes_), le.classes_))\n    le_dict = {int(k): v for k, v in le_dict.items()}\n\n    labels_mapping_dict[\"labels_mapping\"] = le_dict\n    logger.info(f\"Label encoding map has the dictionary: {le_dict}\")\n    write_dict_to_yaml(labels_mapping_dict, parsed_args.params)\n    logger.info(f'Updated the label encoding map in the file at \"{parsed_args.params}\"')\n    labels_dict = read_yaml(parsed_args.params)[\"labels_mapping\"]\n    reverse_dict = {v: k for k, v in labels_dict.items()}\n    y = y.map(reverse_dict)\n    logger.info(\"Successfully mapped the target column\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_transformed,\n        y,\n        test_size=TEST_SIZE,\n        stratify=y,\n        random_state=DATA_SPLIT_RANDOM_STATE,\n    )\n    logger.info(\n        f\"Data split for training completed with the shapes : {X_train.shape,  y_train.shape, X_test.shape, y_test.shape}\"\n    )\n    Path(X_TRAIN_FILE_PATH).parent.absolute().mkdir(\n    parents=True, exist_ok=True)\n    gzip_np_arr(X_train, X_TRAIN_FILE_PATH)\n    gzip_np_arr(y_train, Y_TRAIN_FILE_PATH)\n    gzip_np_arr(X_test, X_TEST_FILE_PATH)",
    "prefix": "import argparse\nimport joblib\nimport pandas as pd\nfrom src.utils.common import read_yaml\nfrom src.utils.sys_logging import get_logger\nfrom sklearn.preprocessing import LabelEncoder\nfrom src.utils.common import write_dict_to_yaml\nfrom src.utils.data_ops import gzip_np_arr\nfrom sklearn.model_selection import train_test_split\nfrom src.utils.data_ops import get_fitted_pipeline\nfrom pathlib import Path\n\nSTAGE = \"Preprocess Data\"\n\n\ndef preprocess_data():\n    complete_df = pd.read_parquet(RAW_DATA_FILE_PATH)\n    logger.info(\n        f'The raw data file has been loaded from \"{RAW_DATA_FILE_PATH}\" with the shape \"{complete_df.shape}\"'\n    )\n    duplicate_rows = complete_df.duplicated().sum()\n    if duplicate_rows > 0:\n        logger.warning(\n            f\"Found {duplicate_rows} duplicate rows, removing duplicate rows...\"\n        )\n        complete_df = complete_df.drop_duplicates(keep=\"first\")\n    X = complete_df.drop([TARGET_COLUMN_NAME], axis=1)\n    y = complete_df[TARGET_COLUMN_NAME]\n    feature_cols = params[\"input_features_schema\"]\n    feature_cols = list(feature_cols.keys())\n    logger.info(f\"Read {len(feature_cols)} feature columns from params\")\n    data_processing_pipeline = get_fitted_pipeline(\n    X, feature_cols, KNN_IMPUTER_NEIGHBORS=KNN_IMPUTER_NEIGHBORS\n    )\n    Path(DATA_PREPROCESSING_PIPELINE_FILE_PATH).parent.absolute().mkdir(parents=True, exist_ok=True)\n    joblib.dump(data_processing_pipeline, DATA_PREPROCESSING_PIPELINE_FILE_PATH, compress=1)\n    logger.info(f\"Saved the preprocessing pipeline to {DATA_PREPROCESSING_PIPELINE_FILE_PATH}\")\n    data_processing_pipeline = joblib.load(DATA_PREPROCESSING_PIPELINE_FILE_PATH)\n    data_processing_pipeline\n    data_processing_pipeline = joblib.load(DATA_PREPROCESSING_PIPELINE_FILE_PATH)\n    logger.info(\n        f'Loaded sklearn data preprocessing pipeline from \"{DATA_PREPROCESSING_PIPELINE_FILE_PATH}\"'\n    )\n    X_transformed = data_processing_pipeline.transform(X)\n    logger.info(f'Dataframe shape after transformation is \"{X_transformed.shape}\"')\n\n    le = LabelEncoder()\n    le.fit(y)\n    labels_mapping_dict = {\"labels_mapping\": \"\"}\n    le_dict = dict(zip(le.transform(le.classes_), le.classes_))\n    le_dict = {int(k): v for k, v in le_dict.items()}\n\n    labels_mapping_dict[\"labels_mapping\"] = le_dict\n    logger.info(f\"Label encoding map has the dictionary: {le_dict}\")\n    write_dict_to_yaml(labels_mapping_dict, parsed_args.params)\n    logger.info(f'Updated the label encoding map in the file at \"{parsed_args.params}\"')\n    labels_dict = read_yaml(parsed_args.params)[\"labels_mapping\"]\n    reverse_dict = {v: k for k, v in labels_dict.items()}\n    y = y.map(reverse_dict)\n    logger.info(\"Successfully mapped the target column\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_transformed,\n        y,\n        test_size=TEST_SIZE,\n        stratify=y,\n        random_state=DATA_SPLIT_RANDOM_STATE,\n    )\n    logger.info(\n        f\"Data split for training completed with the shapes : {X_train.shape,  y_train.shape, X_test.shape, y_test.shape}\"\n    )\n    Path(X_TRAIN_FILE_PATH).parent.absolute().mkdir(\n    parents=True, exist_ok=True)\n    gzip_np_arr(X_train, X_TRAIN_FILE_PATH)\n    gzip_np_arr(y_train, Y_TRAIN_FILE_PATH)\n    gzip_np_arr(X_test, X_TEST_FILE_PATH)",
    "suffix": ""
  },
  {
    "name": "see2023/Bert-VITS2-ext:oldVersion/V200/text/japanese_bert.py@1087",
    "canonical_solution": "def get_bert_feature_with_token(tokens, word2ph, device=config.bert_gen_config.device):",
    "prompt": "import sys\nimport torch\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\nfrom config import config\nfrom .japanese import text2sep_kata\n\n\n\nLOCAL_PATH = \"./bert/deberta-v2-large-japanese\"\n\ntokenizer = AutoTokenizer.from_pretrained(LOCAL_PATH)\n\nmodels = dict()\n\n\ndef get_bert_feature(text, word2ph, device=config.bert_gen_config.device):\n    sep_text, _, _ = text2sep_kata(text)\n    sep_tokens = [tokenizer.tokenize(t) for t in sep_text]\n    sep_ids = [tokenizer.convert_tokens_to_ids(t) for t in sep_tokens]\n    sep_ids = [2] + [item for sublist in sep_ids for item in sublist] + [3]\n    return get_bert_feature_with_token(sep_ids, word2ph, device)\n\n",
    "prefix": "import sys\nimport torch\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\nfrom config import config\nfrom .japanese import text2sep_kata\n\n\n\nLOCAL_PATH = \"./bert/deberta-v2-large-japanese\"\n\ntokenizer = AutoTokenizer.from_pretrained(LOCAL_PATH)\n\nmodels = dict()\n\n\ndef get_bert_feature(text, word2ph, device=config.bert_gen_config.device):\n    sep_text, _, _ = text2sep_kata(text)\n    sep_tokens = [tokenizer.tokenize(t) for t in sep_text]\n    sep_ids = [tokenizer.convert_tokens_to_ids(t) for t in sep_tokens]\n    sep_ids = [2] + [item for sublist in sep_ids for item in sublist] + [3]\n    return get_bert_feature_with_token(sep_ids, word2ph, device)\n\n",
    "suffix": ""
  },
  {
    "name": "chinhsuanwu/ifusion-threestudio:extern/ldm_zero123/thirdp/psp/model_irse.py@1357",
    "canonical_solution": "                Flatten(),",
    "prompt": "from torch.nn import (\n    BatchNorm1d,\n    BatchNorm2d,\n    Conv2d,\n    Dropout,\n    Linear,\n    Module,\n    PReLU,\n    Sequential,\n)\nfrom extern.ldm_zero123.thirdp.psp.helpers import (\n    Flatten,\n    bottleneck_IR,\n    bottleneck_IR_SE,\n    get_blocks,\n    l2_norm,\n)\n# https://github.com/eladrich/pixel2style2pixel\n\n\n\n\"\"\"\nModified Backbone implementation from [TreB1eN](https://github.com/TreB1eN/InsightFace_Pytorch)\n\"\"\"\n\n\nclass Backbone(Module):\n    def __init__(self, input_size, num_layers, mode=\"ir\", drop_ratio=0.4, affine=True):\n        super(Backbone, self).__init__()\n        assert input_size in [112, 224], \"input_size should be 112 or 224\"\n        assert num_layers in [50, 100, 152], \"num_layers should be 50, 100 or 152\"\n        assert mode in [\"ir\", \"ir_se\"], \"mode should be ir or ir_se\"\n        blocks = get_blocks(num_layers)\n        if mode == \"ir\":\n            unit_module = bottleneck_IR\n        elif mode == \"ir_se\":\n            unit_module = bottleneck_IR_SE\n        self.input_layer = Sequential(\n            Conv2d(3, 64, (3, 3), 1, 1, bias=False), BatchNorm2d(64), PReLU(64)\n        )\n        if input_size == 112:\n            self.output_layer = Sequential(\n                BatchNorm2d(512),\n                Dropout(drop_ratio),\n                Flatten(),\n                Linear(512 * 7 * 7, 512),\n                BatchNorm1d(512, affine=affine),\n            )\n        else:\n            self.output_layer = Sequential(\n                BatchNorm2d(512),\n                Dropout(drop_ratio),",
    "prefix": "from torch.nn import (\n    BatchNorm1d,\n    BatchNorm2d,\n    Conv2d,\n    Dropout,\n    Linear,\n    Module,\n    PReLU,\n    Sequential,\n)\nfrom extern.ldm_zero123.thirdp.psp.helpers import (\n    Flatten,\n    bottleneck_IR,\n    bottleneck_IR_SE,\n    get_blocks,\n    l2_norm,\n)\n# https://github.com/eladrich/pixel2style2pixel\n\n\n\n\"\"\"\nModified Backbone implementation from [TreB1eN](https://github.com/TreB1eN/InsightFace_Pytorch)\n\"\"\"\n\n\nclass Backbone(Module):\n    def __init__(self, input_size, num_layers, mode=\"ir\", drop_ratio=0.4, affine=True):\n        super(Backbone, self).__init__()\n        assert input_size in [112, 224], \"input_size should be 112 or 224\"\n        assert num_layers in [50, 100, 152], \"num_layers should be 50, 100 or 152\"\n        assert mode in [\"ir\", \"ir_se\"], \"mode should be ir or ir_se\"\n        blocks = get_blocks(num_layers)\n        if mode == \"ir\":\n            unit_module = bottleneck_IR\n        elif mode == \"ir_se\":\n            unit_module = bottleneck_IR_SE\n        self.input_layer = Sequential(\n            Conv2d(3, 64, (3, 3), 1, 1, bias=False), BatchNorm2d(64), PReLU(64)\n        )\n        if input_size == 112:\n            self.output_layer = Sequential(\n                BatchNorm2d(512),\n                Dropout(drop_ratio),\n                Flatten(),\n                Linear(512 * 7 * 7, 512),\n                BatchNorm1d(512, affine=affine),\n            )\n        else:\n            self.output_layer = Sequential(\n                BatchNorm2d(512),\n                Dropout(drop_ratio),",
    "suffix": ""
  },
  {
    "name": "Q-MM/PureMM:model/PureMM_arch.py@1004",
    "canonical_solution": "        vision_tower = build_vision_tower(model_args)",
    "prompt": "from abc import ABC, abstractmethod\nfrom .multimodal_encoder.builder import build_vision_tower\nfrom .multimodal_projector.builder import build_vision_projector\nimport torch\nimport torch.nn as nn\n#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\n\n\n\nIGNORE_INDEX = -100\nIMAGE_TOKEN_INDEX = -200\nDEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\n\n\ndef rank0_print(rank, *args):\n    if rank == 0:\n        print(*args)\n\n\nclass PureMMMetaModel:\n\n    def __init__(self, config):\n        super(PureMMMetaModel, self).__init__(config)\n\n        if hasattr(config, \"mm_vision_tower\"):\n            self.vision_tower = build_vision_tower(config, delay_load=True)\n            # self.mm_projector = nn.Linear(config.mm_hidden_size, config.hidden_size)\n            self.mm_projector = build_vision_projector(config)\n            self.vision_tower.requires_grad_(False)\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, 'vision_tower', None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None, rank=0):\n        vision_tower = model_args.vision_tower\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n\n        self.config.mm_vision_tower = vision_tower\n",
    "prefix": "from abc import ABC, abstractmethod\nfrom .multimodal_encoder.builder import build_vision_tower\nfrom .multimodal_projector.builder import build_vision_projector\nimport torch\nimport torch.nn as nn\n#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\n\n\n\nIGNORE_INDEX = -100\nIMAGE_TOKEN_INDEX = -200\nDEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\n\n\ndef rank0_print(rank, *args):\n    if rank == 0:\n        print(*args)\n\n\nclass PureMMMetaModel:\n\n    def __init__(self, config):\n        super(PureMMMetaModel, self).__init__(config)\n\n        if hasattr(config, \"mm_vision_tower\"):\n            self.vision_tower = build_vision_tower(config, delay_load=True)\n            # self.mm_projector = nn.Linear(config.mm_hidden_size, config.hidden_size)\n            self.mm_projector = build_vision_projector(config)\n            self.vision_tower.requires_grad_(False)\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, 'vision_tower', None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None, rank=0):\n        vision_tower = model_args.vision_tower\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n\n        self.config.mm_vision_tower = vision_tower\n",
    "suffix": ""
  },
  {
    "name": "Ananya2001-an/spotify-py-sdk:example.py@1070",
    "canonical_solution": "    api: SpotifyApi = SpotifyApi(os.getenv(\"CLIENT_ID\"), os.getenv(\"CLIENT_SECRET\"), config)",
    "prompt": "import os\nfrom spotify_py_sdk import SpotifyApi, SdkConfig\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\ndef main():\n    config = SdkConfig()\n    config.before_request = lambda x, y: print(\"\u2060\u272f\ud83e\uddf8\ud83c\udfa7\u2615\u2060\u272f\")\n    # TODO async/await request\n    # config.after_request = lambda x, y, z: print(\"After request!\")\n",
    "prefix": "import os\nfrom spotify_py_sdk import SpotifyApi, SdkConfig\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\ndef main():\n    config = SdkConfig()\n    config.before_request = lambda x, y: print(\"\u2060\u272f\ud83e\uddf8\ud83c\udfa7\u2615\u2060\u272f\")\n    # TODO async/await request\n    # config.after_request = lambda x, y, z: print(\"After request!\")\n",
    "suffix": ""
  },
  {
    "name": "giaminhgist/3D-DAM:lib/training/train.py@1257",
    "canonical_solution": "    acc_m = AverageMeter()",
    "prompt": "import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom collections import OrderedDict\nfrom lib.utils.utils import AverageMeter, accuracy\nfrom lib.utils.EarlyStopping import EarlyStopping\nfrom lib.training.train_helper import plot_result\nfrom sklearn.metrics import confusion_matrix\nfrom tqdm import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef train_one_epoch(\n        model,\n        loader,\n        optimizer,\n        epoch_idx: int,\n        lr_scheduler=None,\n):\n    losses_m = AverageMeter()\n    acc_m = AverageMeter()\n\n    model.train()\n    print('Start training epoch: ', epoch_idx)\n    for batch_idx, data in enumerate(tqdm(loader)):\n\n        images, target = data\n        images, target = images.to(device), target.to(device)\n        target = target.flatten()\n\n        output = model(images)\n\n        loss = nn.CrossEntropyLoss()(output, target)\n\n        losses_m.update(loss.item(), images.size(0))\n        acc1 = accuracy(output, target, topk=(1,))\n        acc_m.update(acc1[0].item(), output.size(0))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        torch.cuda.synchronize()\n\n    print(optimizer.param_groups[0]['lr'])\n\n    if hasattr(optimizer, 'sync_lookahead'):\n        optimizer.sync_lookahead()\n\n    metrics = OrderedDict([('loss', losses_m.avg), ('Acc', acc_m.avg)])\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n\n    return metrics\n\n\ndef validate(model, loader):\n    losses_m = AverageMeter()",
    "prefix": "import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom collections import OrderedDict\nfrom lib.utils.utils import AverageMeter, accuracy\nfrom lib.utils.EarlyStopping import EarlyStopping\nfrom lib.training.train_helper import plot_result\nfrom sklearn.metrics import confusion_matrix\nfrom tqdm import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef train_one_epoch(\n        model,\n        loader,\n        optimizer,\n        epoch_idx: int,\n        lr_scheduler=None,\n):\n    losses_m = AverageMeter()\n    acc_m = AverageMeter()\n\n    model.train()\n    print('Start training epoch: ', epoch_idx)\n    for batch_idx, data in enumerate(tqdm(loader)):\n\n        images, target = data\n        images, target = images.to(device), target.to(device)\n        target = target.flatten()\n\n        output = model(images)\n\n        loss = nn.CrossEntropyLoss()(output, target)\n\n        losses_m.update(loss.item(), images.size(0))\n        acc1 = accuracy(output, target, topk=(1,))\n        acc_m.update(acc1[0].item(), output.size(0))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        torch.cuda.synchronize()\n\n    print(optimizer.param_groups[0]['lr'])\n\n    if hasattr(optimizer, 'sync_lookahead'):\n        optimizer.sync_lookahead()\n\n    metrics = OrderedDict([('loss', losses_m.avg), ('Acc', acc_m.avg)])\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n\n    return metrics\n\n\ndef validate(model, loader):\n    losses_m = AverageMeter()",
    "suffix": ""
  },
  {
    "name": "gardenifi/server:tests/api/write_value_test.py@1094",
    "canonical_solution": "            BleData(refresh=\"invalid\")",
    "prompt": "import json\nimport pytest\nfrom pydantic import ValidationError as PydanticValidationError\nfrom fastapi import HTTPException\nfrom app.main_app import write_ble_data, BleData, WifiData\n\"\"\"MIT License\n\nCopyright (c) 2023, Marios Karagiannopoulos\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\n**Attribution Requirement:**\nWhen using or distributing the software, an attribution to Marios Karagiannopoulos must be included.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\"\"\"\n\n\n\nclass TestWriteBleData:\n    \"\"\"\n    Test class for the 'write_ble_data' API endpoint.\n    \"\"\"\n\n    @pytest.mark.asyncio\n    async def test_valid_page_data(self):\n        \"\"\"\n        API call with valid page data sets PAGE_SET and returns {\"message\": PAGE_SET}\n        \"\"\"\n        # Arrange\n        data = BleData(page=9)\n\n        # Act\n        response = await write_ble_data(data)\n\n        # Assert\n        assert json.loads(response.body) == {\"page\": 9}\n\n    @pytest.mark.asyncio\n    async def test_valid_refresh_data(self):\n        \"\"\"\n        API call with valid refresh data sets REFRESH_SET and returns {\"message\": REFRESH_SET}\n        \"\"\"\n        # Arrange\n        data = BleData(refresh=True)\n\n        # Act\n        response = await write_ble_data(data)\n\n        # Assert\n        assert json.loads(response.body) == {\"refresh\": True}\n\n    @pytest.mark.asyncio\n    async def test_valid_ssid_and_wifi_key_data(self):\n        \"\"\"\n        API call with valid ssid and wifi_key data stores ssid and wifi_key and returns {\"message\": connected}\n        \"\"\"\n        # Arrange\n        wifi_data = WifiData(ssid=\"test_ssid\", wifi_key=\"test_key\")\n        data = BleData(wifi_data=wifi_data)\n\n        # Act and Assert\n        with pytest.raises(HTTPException) as exc:\n            await write_ble_data(data)\n        assert exc.value.status_code == 500\n\n    @pytest.mark.asyncio\n    async def test_invalid_page_data_type(self):\n        \"\"\"\n        Invalid data type for refresh\n        \"\"\"\n        # Act and Assert\n        with pytest.raises(PydanticValidationError):\n            BleData(page=\"invalid\", refresh=0)\n\n    @pytest.mark.asyncio\n    async def test_invalid_refresh_data_type(self):\n        \"\"\"\n        Invalid data type for refresh\n        \"\"\"\n        # Arrange\n        with pytest.raises(PydanticValidationError):",
    "prefix": "import json\nimport pytest\nfrom pydantic import ValidationError as PydanticValidationError\nfrom fastapi import HTTPException\nfrom app.main_app import write_ble_data, BleData, WifiData\n\"\"\"MIT License\n\nCopyright (c) 2023, Marios Karagiannopoulos\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\n**Attribution Requirement:**\nWhen using or distributing the software, an attribution to Marios Karagiannopoulos must be included.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\"\"\"\n\n\n\nclass TestWriteBleData:\n    \"\"\"\n    Test class for the 'write_ble_data' API endpoint.\n    \"\"\"\n\n    @pytest.mark.asyncio\n    async def test_valid_page_data(self):\n        \"\"\"\n        API call with valid page data sets PAGE_SET and returns {\"message\": PAGE_SET}\n        \"\"\"\n        # Arrange\n        data = BleData(page=9)\n\n        # Act\n        response = await write_ble_data(data)\n\n        # Assert\n        assert json.loads(response.body) == {\"page\": 9}\n\n    @pytest.mark.asyncio\n    async def test_valid_refresh_data(self):\n        \"\"\"\n        API call with valid refresh data sets REFRESH_SET and returns {\"message\": REFRESH_SET}\n        \"\"\"\n        # Arrange\n        data = BleData(refresh=True)\n\n        # Act\n        response = await write_ble_data(data)\n\n        # Assert\n        assert json.loads(response.body) == {\"refresh\": True}\n\n    @pytest.mark.asyncio\n    async def test_valid_ssid_and_wifi_key_data(self):\n        \"\"\"\n        API call with valid ssid and wifi_key data stores ssid and wifi_key and returns {\"message\": connected}\n        \"\"\"\n        # Arrange\n        wifi_data = WifiData(ssid=\"test_ssid\", wifi_key=\"test_key\")\n        data = BleData(wifi_data=wifi_data)\n\n        # Act and Assert\n        with pytest.raises(HTTPException) as exc:\n            await write_ble_data(data)\n        assert exc.value.status_code == 500\n\n    @pytest.mark.asyncio\n    async def test_invalid_page_data_type(self):\n        \"\"\"\n        Invalid data type for refresh\n        \"\"\"\n        # Act and Assert\n        with pytest.raises(PydanticValidationError):\n            BleData(page=\"invalid\", refresh=0)\n\n    @pytest.mark.asyncio\n    async def test_invalid_refresh_data_type(self):\n        \"\"\"\n        Invalid data type for refresh\n        \"\"\"\n        # Arrange\n        with pytest.raises(PydanticValidationError):",
    "suffix": ""
  },
  {
    "name": "xiaoye0x0/pfgo_tg_bot:utils/task/set_args.py@844",
    "canonical_solution": "    LOGGER = Logmanager.create_logger(\"CheckArgs\")",
    "prompt": "import os\nimport argparse\nfrom .model import Task\nfrom ..log import Logmanager\n\n\n\ndef is_file_exists(file_path) -> bool:\n    r = os.path.exists(file_path)\n    if not r:\n        LOGGER.error(f\"\u6587\u4ef6{file_path}\u4e0d\u5b58\u5728\")\n    return r\n\n\ndef create_folder_if_not_exists(folder_path):\n    if not folder_path:\n        return\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path)\n\n\ndef parse_command_line_args():\n    \"\"\"\n    -c --config: \u914d\u7f6e\u6587\u4ef6\n    --log: \u65e5\u5fd7\u5b58\u653e\u4f4d\u7f6e\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"\u8fd0\u884c\u53c2\u6570\")\n\n    parser.add_argument(\"--config\", \"-c\", type=str, default=\"./config.ini\", help=\"\u914d\u7f6e\u6587\u4ef6\")\n    parser.add_argument(\"--log\", type=str, default=\"./\", help=\"\u65e5\u5fd7\u5b58\u653e\u6587\u4ef6\u5939\u7684\u4f4d\u7f6e,\u9ed8\u8ba4\u653e\u5230\u5f53\u524d\u8def\u5f84\")\n    args = parser.parse_args()\n\n    # \u521d\u59cb\u5316\u65e5\u5fd7\u6a21\u5757\n    global LOGGER\n    create_folder_if_not_exists(args.log)\n    Logmanager(args.log)",
    "prefix": "import os\nimport argparse\nfrom .model import Task\nfrom ..log import Logmanager\n\n\n\ndef is_file_exists(file_path) -> bool:\n    r = os.path.exists(file_path)\n    if not r:\n        LOGGER.error(f\"\u6587\u4ef6{file_path}\u4e0d\u5b58\u5728\")\n    return r\n\n\ndef create_folder_if_not_exists(folder_path):\n    if not folder_path:\n        return\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path)\n\n\ndef parse_command_line_args():\n    \"\"\"\n    -c --config: \u914d\u7f6e\u6587\u4ef6\n    --log: \u65e5\u5fd7\u5b58\u653e\u4f4d\u7f6e\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"\u8fd0\u884c\u53c2\u6570\")\n\n    parser.add_argument(\"--config\", \"-c\", type=str, default=\"./config.ini\", help=\"\u914d\u7f6e\u6587\u4ef6\")\n    parser.add_argument(\"--log\", type=str, default=\"./\", help=\"\u65e5\u5fd7\u5b58\u653e\u6587\u4ef6\u5939\u7684\u4f4d\u7f6e,\u9ed8\u8ba4\u653e\u5230\u5f53\u524d\u8def\u5f84\")\n    args = parser.parse_args()\n\n    # \u521d\u59cb\u5316\u65e5\u5fd7\u6a21\u5757\n    global LOGGER\n    create_folder_if_not_exists(args.log)\n    Logmanager(args.log)",
    "suffix": ""
  },
  {
    "name": "shibing624/chatgpt-webui:src/index_func.py@1337",
    "canonical_solution": "    text_splitter = ChineseRecursiveTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)",
    "prompt": "import os\nimport re\n                import PyPDF2\nfrom typing import List, Optional, Any\nfrom langchain.schema import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom loguru import logger\nfrom tqdm import tqdm\nfrom src.config import local_embedding, retrieve_proxy, chunk_overlap, chunk_size, hf_emb_model_name\nfrom src.presets import OPENAI_API_BASE\nfrom src.utils import excel_to_string, get_files_hash, load_pkl, save_pkl\n                    from src.pdf_func import parse_pdf\n                    from src.config import advance_docs\n                from langchain.document_loaders import UnstructuredWordDocumentLoader\n                from langchain.document_loaders import UnstructuredPowerPointLoader\n                from langchain.document_loaders import UnstructuredEPubLoader\n                from langchain.document_loaders import TextLoader\n    from langchain.vectorstores import FAISS\n    from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n        from langchain.embeddings import OpenAIEmbeddings\n\n\n\npwd_path = os.path.abspath(os.path.dirname(__file__))\n\n\nclass ChineseRecursiveTextSplitter(RecursiveCharacterTextSplitter):\n    \"\"\"Recursive text splitter for Chinese text.\n    copy from: https://github.com/chatchat-space/Langchain-Chatchat/tree/master\n    \"\"\"\n\n    def __init__(\n            self,\n            separators: Optional[List[str]] = None,\n            keep_separator: bool = True,\n            is_separator_regex: bool = True,\n            **kwargs: Any,\n    ) -> None:\n        \"\"\"Create a new TextSplitter.\"\"\"\n        super().__init__(keep_separator=keep_separator, **kwargs)\n        self._separators = separators or [\n            \"\\n\\n\",\n            \"\\n\",\n            \"\u3002|\uff01|\uff1f\",\n            \"\\.\\s|\\!\\s|\\?\\s\",\n            \"\uff1b|;\\s\",\n            \"\uff0c|,\\s\"\n        ]\n        self._is_separator_regex = is_separator_regex\n\n    @staticmethod\n    def _split_text_with_regex_from_end(\n            text: str, separator: str, keep_separator: bool\n    ) -> List[str]:\n        # Now that we have the separator, split the text\n        if separator:\n            if keep_separator:\n                # The parentheses in the pattern keep the delimiters in the result.\n                _splits = re.split(f\"({separator})\", text)\n                splits = [\"\".join(i) for i in zip(_splits[0::2], _splits[1::2])]\n                if len(_splits) % 2 == 1:\n                    splits += _splits[-1:]\n            else:\n                splits = re.split(separator, text)\n        else:\n            splits = list(text)\n        return [s for s in splits if s != \"\"]\n\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\n        \"\"\"Split incoming text and return chunks.\"\"\"\n        final_chunks = []\n        # Get appropriate separator to use\n        separator = separators[-1]\n        new_separators = []\n        for i, _s in enumerate(separators):\n            _separator = _s if self._is_separator_regex else re.escape(_s)\n            if _s == \"\":\n                separator = _s\n                break\n            if re.search(_separator, text):\n                separator = _s\n                new_separators = separators[i + 1:]\n                break\n\n        _separator = separator if self._is_separator_regex else re.escape(separator)\n        splits = self._split_text_with_regex_from_end(text, _separator, self._keep_separator)\n\n        # Now go merging things, recursively splitting longer texts.\n        _good_splits = []\n        _separator = \"\" if self._keep_separator else separator\n        for s in splits:\n            if self._length_function(s) < self._chunk_size:\n                _good_splits.append(s)\n            else:\n                if _good_splits:\n                    merged_text = self._merge_splits(_good_splits, _separator)\n                    final_chunks.extend(merged_text)\n                    _good_splits = []\n                if not new_separators:\n                    final_chunks.append(s)\n                else:\n                    other_info = self._split_text(s, new_separators)\n                    final_chunks.extend(other_info)\n        if _good_splits:\n            merged_text = self._merge_splits(_good_splits, _separator)\n            final_chunks.extend(merged_text)\n        return [re.sub(r\"\\n{2,}\", \"\\n\", chunk.strip()) for chunk in final_chunks if chunk.strip() != \"\"]\n\n\ndef get_documents(file_paths):",
    "prefix": "import os\nimport re\n                import PyPDF2\nfrom typing import List, Optional, Any\nfrom langchain.schema import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom loguru import logger\nfrom tqdm import tqdm\nfrom src.config import local_embedding, retrieve_proxy, chunk_overlap, chunk_size, hf_emb_model_name\nfrom src.presets import OPENAI_API_BASE\nfrom src.utils import excel_to_string, get_files_hash, load_pkl, save_pkl\n                    from src.pdf_func import parse_pdf\n                    from src.config import advance_docs\n                from langchain.document_loaders import UnstructuredWordDocumentLoader\n                from langchain.document_loaders import UnstructuredPowerPointLoader\n                from langchain.document_loaders import UnstructuredEPubLoader\n                from langchain.document_loaders import TextLoader\n    from langchain.vectorstores import FAISS\n    from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n        from langchain.embeddings import OpenAIEmbeddings\n\n\n\npwd_path = os.path.abspath(os.path.dirname(__file__))\n\n\nclass ChineseRecursiveTextSplitter(RecursiveCharacterTextSplitter):\n    \"\"\"Recursive text splitter for Chinese text.\n    copy from: https://github.com/chatchat-space/Langchain-Chatchat/tree/master\n    \"\"\"\n\n    def __init__(\n            self,\n            separators: Optional[List[str]] = None,\n            keep_separator: bool = True,\n            is_separator_regex: bool = True,\n            **kwargs: Any,\n    ) -> None:\n        \"\"\"Create a new TextSplitter.\"\"\"\n        super().__init__(keep_separator=keep_separator, **kwargs)\n        self._separators = separators or [\n            \"\\n\\n\",\n            \"\\n\",\n            \"\u3002|\uff01|\uff1f\",\n            \"\\.\\s|\\!\\s|\\?\\s\",\n            \"\uff1b|;\\s\",\n            \"\uff0c|,\\s\"\n        ]\n        self._is_separator_regex = is_separator_regex\n\n    @staticmethod\n    def _split_text_with_regex_from_end(\n            text: str, separator: str, keep_separator: bool\n    ) -> List[str]:\n        # Now that we have the separator, split the text\n        if separator:\n            if keep_separator:\n                # The parentheses in the pattern keep the delimiters in the result.\n                _splits = re.split(f\"({separator})\", text)\n                splits = [\"\".join(i) for i in zip(_splits[0::2], _splits[1::2])]\n                if len(_splits) % 2 == 1:\n                    splits += _splits[-1:]\n            else:\n                splits = re.split(separator, text)\n        else:\n            splits = list(text)\n        return [s for s in splits if s != \"\"]\n\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\n        \"\"\"Split incoming text and return chunks.\"\"\"\n        final_chunks = []\n        # Get appropriate separator to use\n        separator = separators[-1]\n        new_separators = []\n        for i, _s in enumerate(separators):\n            _separator = _s if self._is_separator_regex else re.escape(_s)\n            if _s == \"\":\n                separator = _s\n                break\n            if re.search(_separator, text):\n                separator = _s\n                new_separators = separators[i + 1:]\n                break\n\n        _separator = separator if self._is_separator_regex else re.escape(separator)\n        splits = self._split_text_with_regex_from_end(text, _separator, self._keep_separator)\n\n        # Now go merging things, recursively splitting longer texts.\n        _good_splits = []\n        _separator = \"\" if self._keep_separator else separator\n        for s in splits:\n            if self._length_function(s) < self._chunk_size:\n                _good_splits.append(s)\n            else:\n                if _good_splits:\n                    merged_text = self._merge_splits(_good_splits, _separator)\n                    final_chunks.extend(merged_text)\n                    _good_splits = []\n                if not new_separators:\n                    final_chunks.append(s)\n                else:\n                    other_info = self._split_text(s, new_separators)\n                    final_chunks.extend(other_info)\n        if _good_splits:\n            merged_text = self._merge_splits(_good_splits, _separator)\n            final_chunks.extend(merged_text)\n        return [re.sub(r\"\\n{2,}\", \"\\n\", chunk.strip()) for chunk in final_chunks if chunk.strip() != \"\"]\n\n\ndef get_documents(file_paths):",
    "suffix": ""
  },
  {
    "name": "camenduru/AnyDoor-online-hf:dinov2/dinov2/data/augmentations.py@761",
    "canonical_solution": "                GaussianBlur(p=0.1),",
    "prompt": "import logging\nfrom torchvision import transforms\nfrom .transforms import (\n    GaussianBlur,\n    make_normalize_transform,\n)\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n\n\n\nlogger = logging.getLogger(\"dinov2\")\n\n\nclass DataAugmentationDINO(object):\n    def __init__(\n        self,\n        global_crops_scale,\n        local_crops_scale,\n        local_crops_number,\n        global_crops_size=224,\n        local_crops_size=96,\n    ):\n        self.global_crops_scale = global_crops_scale\n        self.local_crops_scale = local_crops_scale\n        self.local_crops_number = local_crops_number\n        self.global_crops_size = global_crops_size\n        self.local_crops_size = local_crops_size\n\n        logger.info(\"###################################\")\n        logger.info(\"Using data augmentation parameters:\")\n        logger.info(f\"global_crops_scale: {global_crops_scale}\")\n        logger.info(f\"local_crops_scale: {local_crops_scale}\")\n        logger.info(f\"local_crops_number: {local_crops_number}\")\n        logger.info(f\"global_crops_size: {global_crops_size}\")\n        logger.info(f\"local_crops_size: {local_crops_size}\")\n        logger.info(\"###################################\")\n\n        # random resized crop and flip\n        self.geometric_augmentation_global = transforms.Compose(\n            [\n                transforms.RandomResizedCrop(\n                    global_crops_size, scale=global_crops_scale, interpolation=transforms.InterpolationMode.BICUBIC\n                ),\n                transforms.RandomHorizontalFlip(p=0.5),\n            ]\n        )\n\n        self.geometric_augmentation_local = transforms.Compose(\n            [\n                transforms.RandomResizedCrop(\n                    local_crops_size, scale=local_crops_scale, interpolation=transforms.InterpolationMode.BICUBIC\n                ),\n                transforms.RandomHorizontalFlip(p=0.5),\n            ]\n        )\n\n        # color distorsions / blurring\n        color_jittering = transforms.Compose(\n            [\n                transforms.RandomApply(\n                    [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n                    p=0.8,\n                ),\n                transforms.RandomGrayscale(p=0.2),\n            ]\n        )\n\n        global_transfo1_extra = GaussianBlur(p=1.0)\n\n        global_transfo2_extra = transforms.Compose(\n            [",
    "prefix": "import logging\nfrom torchvision import transforms\nfrom .transforms import (\n    GaussianBlur,\n    make_normalize_transform,\n)\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n\n\n\nlogger = logging.getLogger(\"dinov2\")\n\n\nclass DataAugmentationDINO(object):\n    def __init__(\n        self,\n        global_crops_scale,\n        local_crops_scale,\n        local_crops_number,\n        global_crops_size=224,\n        local_crops_size=96,\n    ):\n        self.global_crops_scale = global_crops_scale\n        self.local_crops_scale = local_crops_scale\n        self.local_crops_number = local_crops_number\n        self.global_crops_size = global_crops_size\n        self.local_crops_size = local_crops_size\n\n        logger.info(\"###################################\")\n        logger.info(\"Using data augmentation parameters:\")\n        logger.info(f\"global_crops_scale: {global_crops_scale}\")\n        logger.info(f\"local_crops_scale: {local_crops_scale}\")\n        logger.info(f\"local_crops_number: {local_crops_number}\")\n        logger.info(f\"global_crops_size: {global_crops_size}\")\n        logger.info(f\"local_crops_size: {local_crops_size}\")\n        logger.info(\"###################################\")\n\n        # random resized crop and flip\n        self.geometric_augmentation_global = transforms.Compose(\n            [\n                transforms.RandomResizedCrop(\n                    global_crops_size, scale=global_crops_scale, interpolation=transforms.InterpolationMode.BICUBIC\n                ),\n                transforms.RandomHorizontalFlip(p=0.5),\n            ]\n        )\n\n        self.geometric_augmentation_local = transforms.Compose(\n            [\n                transforms.RandomResizedCrop(\n                    local_crops_size, scale=local_crops_scale, interpolation=transforms.InterpolationMode.BICUBIC\n                ),\n                transforms.RandomHorizontalFlip(p=0.5),\n            ]\n        )\n\n        # color distorsions / blurring\n        color_jittering = transforms.Compose(\n            [\n                transforms.RandomApply(\n                    [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n                    p=0.8,\n                ),\n                transforms.RandomGrayscale(p=0.2),\n            ]\n        )\n\n        global_transfo1_extra = GaussianBlur(p=1.0)\n\n        global_transfo2_extra = transforms.Compose(\n            [",
    "suffix": ""
  },
  {
    "name": "omkarcloud/google-scraper:src/google_scraper.py@1444",
    "canonical_solution": "            write_output('_all',result, None, lambda x:x)",
    "prompt": "from typing import List,Optional, Union, Dict\nfrom botasaurus import bt\nfrom .write_output import write_output\nfrom .search import FAILED_DUE_TO_CREDITS_EXHAUSTED, FAILED_DUE_TO_NO_KEY,FAILED_DUE_TO_NOT_SUBSCRIBED, FAILED_DUE_TO_UNKNOWN_ERROR, search\n\n\n\ndef clean_data(social_details):\n    success, credits_exhausted, not_subscribed, unknown_error, no_key = [], [], [], [], []\n\n    for detail in social_details:\n        if detail.get(\"error\") is None:\n            success.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_CREDITS_EXHAUSTED:\n            credits_exhausted.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_NOT_SUBSCRIBED:\n            not_subscribed.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_UNKNOWN_ERROR:\n            unknown_error.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_NO_KEY:\n            no_key.append(detail)\n\n    return success, credits_exhausted, not_subscribed, unknown_error, no_key\n\ndef print_data_errors(credits_exhausted, not_subscribed, unknown_error, no_key):\n    \n    if credits_exhausted:\n        name = \"queries\" if len(credits_exhausted) > 1 else \"query\"\n        print(f\"Could not get data for {len(credits_exhausted)} {name} due to credit exhaustion. Please consider upgrading your plan by visiting https://rapidapi.com/Chetan11dev/api/google-scraper/pricing to continue scraping data.\")\n\n    if not_subscribed:\n        name = \"queries\" if len(not_subscribed) > 1 else \"query\"\n        print(f\"Could not get data for {len(not_subscribed)} {name} as you are not subscribed to Google Scraper API. Please subscribe to a free plan by visiting https://rapidapi.com/Chetan11dev/api/google-scraper/pricing\")\n\n    if unknown_error:\n        name = \"queries\" if len(unknown_error) > 1 else \"query\"\n        print(f\"Could not get data for {len(unknown_error)} {name} due to Unknown Error.\")\n\n    if no_key:\n        name = \"queries\" if len(no_key) > 1 else \"query\"\n        print(f\"Could not get data for {len(no_key)} {name} as you are not subscribed to Google Scraper API. Please subscribe to a free plan by visiting https://rapidapi.com/Chetan11dev/api/google-scraper/pricing\")\n\n      \nclass Google:\n    \n    @staticmethod\n    def search(query:  Union[str, List[str]],  max: Optional[int] = None, key: Optional[str] =None,  use_cache: bool = True) -> Dict:\n        \"\"\"\n        Function to scrape data from Google.\n        :param use_cache: Boolean indicating whether to use cached data.\n        :return: List of dictionaries with the scraped data.\n        \"\"\"\n        cache = use_cache\n        if isinstance(query, str):\n            query = [query]  \n\n        query = [{\"query\":query_query, \"max\": max} for query_query in query]\n        result = []\n        for item in query:\n            # TODO: max fixes\n            data = item\n            metadata = {\"key\": key}\n            \n            result_item = search(data, cache=cache, metadata=metadata)\n            \n            success, credits_exhausted, not_subscribed, unknown_error, no_key = clean_data([result_item])\n            print_data_errors(credits_exhausted, not_subscribed, unknown_error, no_key)\n\n            if success:\n                data = result_item.get('data')\n                if not data:\n                    data = {}\n\n                result_item = data.get('results', [])\n                result.extend(result_item)\n                write_output(item['query'], result_item,None)\n\n        if result:\n            # bt.write_json(result, \"result\")",
    "prefix": "from typing import List,Optional, Union, Dict\nfrom botasaurus import bt\nfrom .write_output import write_output\nfrom .search import FAILED_DUE_TO_CREDITS_EXHAUSTED, FAILED_DUE_TO_NO_KEY,FAILED_DUE_TO_NOT_SUBSCRIBED, FAILED_DUE_TO_UNKNOWN_ERROR, search\n\n\n\ndef clean_data(social_details):\n    success, credits_exhausted, not_subscribed, unknown_error, no_key = [], [], [], [], []\n\n    for detail in social_details:\n        if detail.get(\"error\") is None:\n            success.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_CREDITS_EXHAUSTED:\n            credits_exhausted.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_NOT_SUBSCRIBED:\n            not_subscribed.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_UNKNOWN_ERROR:\n            unknown_error.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_NO_KEY:\n            no_key.append(detail)\n\n    return success, credits_exhausted, not_subscribed, unknown_error, no_key\n\ndef print_data_errors(credits_exhausted, not_subscribed, unknown_error, no_key):\n    \n    if credits_exhausted:\n        name = \"queries\" if len(credits_exhausted) > 1 else \"query\"\n        print(f\"Could not get data for {len(credits_exhausted)} {name} due to credit exhaustion. Please consider upgrading your plan by visiting https://rapidapi.com/Chetan11dev/api/google-scraper/pricing to continue scraping data.\")\n\n    if not_subscribed:\n        name = \"queries\" if len(not_subscribed) > 1 else \"query\"\n        print(f\"Could not get data for {len(not_subscribed)} {name} as you are not subscribed to Google Scraper API. Please subscribe to a free plan by visiting https://rapidapi.com/Chetan11dev/api/google-scraper/pricing\")\n\n    if unknown_error:\n        name = \"queries\" if len(unknown_error) > 1 else \"query\"\n        print(f\"Could not get data for {len(unknown_error)} {name} due to Unknown Error.\")\n\n    if no_key:\n        name = \"queries\" if len(no_key) > 1 else \"query\"\n        print(f\"Could not get data for {len(no_key)} {name} as you are not subscribed to Google Scraper API. Please subscribe to a free plan by visiting https://rapidapi.com/Chetan11dev/api/google-scraper/pricing\")\n\n      \nclass Google:\n    \n    @staticmethod\n    def search(query:  Union[str, List[str]],  max: Optional[int] = None, key: Optional[str] =None,  use_cache: bool = True) -> Dict:\n        \"\"\"\n        Function to scrape data from Google.\n        :param use_cache: Boolean indicating whether to use cached data.\n        :return: List of dictionaries with the scraped data.\n        \"\"\"\n        cache = use_cache\n        if isinstance(query, str):\n            query = [query]  \n\n        query = [{\"query\":query_query, \"max\": max} for query_query in query]\n        result = []\n        for item in query:\n            # TODO: max fixes\n            data = item\n            metadata = {\"key\": key}\n            \n            result_item = search(data, cache=cache, metadata=metadata)\n            \n            success, credits_exhausted, not_subscribed, unknown_error, no_key = clean_data([result_item])\n            print_data_errors(credits_exhausted, not_subscribed, unknown_error, no_key)\n\n            if success:\n                data = result_item.get('data')\n                if not data:\n                    data = {}\n\n                result_item = data.get('results', [])\n                result.extend(result_item)\n                write_output(item['query'], result_item,None)\n\n        if result:\n            # bt.write_json(result, \"result\")",
    "suffix": ""
  },
  {
    "name": "AI2lab/comfyUI-tool-2lab:nodes/tool/loader.py@880",
    "canonical_solution": "    NAME = get_project_name('StringHub9')",
    "prompt": "import sys\nfrom ..common import fields\nfrom ..constants import get_project_name, get_project_category\n\n\nNODE_CATEGORY = get_project_category(\"util/loader\")\n\n\nclass VideoParamHub:\n    NAME = get_project_name('VideoParamHub')\n    CATEGORY = NODE_CATEGORY\n    RETURN_TYPES = ( \"INT\", \"INT\",)\n    RETURN_NAMES = (\"frames\", \"fps\",)\n    OUTPUT_NODE = True\n    FUNCTION = \"doWork\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"frames\": ([14,25], {\"default\": 14,}),\n                \"fps\": (\"INT\", {\"default\": 5,\n                       \"min\": 1,\n                       \"max\": 1000,\n                       \"step\": 1}),\n            }\n        }\n\n    def doWork(self, frames, fps):\n        return {\"result\": (frames, fps)}\n\nclass ParamHub:\n    NAME = get_project_name('ParamHub')\n    CATEGORY = NODE_CATEGORY\n    RETURN_TYPES = (\"STRING\", \"STRING\", \"INT\", \"INT\", \"INT\", \"INT\", \"BOOLEAN\", \"STRING\", \"STRING\", \"INT\",)\n    RETURN_NAMES = (\n    \"prompt\", \"negativePrompt\", \"width\", \"height\", \"seed\", \"steps\", \"addWatermark\", \"watermark\", \"segment\", \"batchSize\")\n    OUTPUT_NODE = True\n    FUNCTION = \"doWork\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"prompt\": fields.STRING_ML,\n                \"negativePrompt\": fields.STRING_ML,\n                \"width\": (\"INT\", {\"default\": 1024,\n                       \"min\": 256,\n                       \"max\": 2048,\n                       \"step\": 1}),\n                \"height\": (\"INT\", {\"default\": 1024,\n                       \"min\": 256,\n                       \"max\": 2048,\n                       \"step\": 1}),\n                \"seed\": (\"INT\", {\n                    \"default\": 0,\n                    \"min\": -1,\n                    \"max\": sys.maxsize,\n                }),\n                \"steps\": (\"INT\", {\"default\": 25,\n                       \"min\": 1,\n                       \"max\": 1000,\n                       \"step\": 1}),\n                \"addWatermark\": fields.BOOL_TRUE,\n                \"watermark\": fields.STRING,\n                \"segment\": fields.STRING,\n                \"batchSize\": fields.INT_POSITIVE,\n            }\n        }\n\n    def doWork(self, prompt, negativePrompt, width, height, seed, steps, addWatermark, watermark, segment, batchSize):\n        if addWatermark == False:\n            watermark = ''\n        return {\"result\": (prompt, negativePrompt, width, height, seed, steps, addWatermark, watermark, segment, batchSize)}\n\nclass StringHub9:",
    "prefix": "import sys\nfrom ..common import fields\nfrom ..constants import get_project_name, get_project_category\n\n\nNODE_CATEGORY = get_project_category(\"util/loader\")\n\n\nclass VideoParamHub:\n    NAME = get_project_name('VideoParamHub')\n    CATEGORY = NODE_CATEGORY\n    RETURN_TYPES = ( \"INT\", \"INT\",)\n    RETURN_NAMES = (\"frames\", \"fps\",)\n    OUTPUT_NODE = True\n    FUNCTION = \"doWork\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"frames\": ([14,25], {\"default\": 14,}),\n                \"fps\": (\"INT\", {\"default\": 5,\n                       \"min\": 1,\n                       \"max\": 1000,\n                       \"step\": 1}),\n            }\n        }\n\n    def doWork(self, frames, fps):\n        return {\"result\": (frames, fps)}\n\nclass ParamHub:\n    NAME = get_project_name('ParamHub')\n    CATEGORY = NODE_CATEGORY\n    RETURN_TYPES = (\"STRING\", \"STRING\", \"INT\", \"INT\", \"INT\", \"INT\", \"BOOLEAN\", \"STRING\", \"STRING\", \"INT\",)\n    RETURN_NAMES = (\n    \"prompt\", \"negativePrompt\", \"width\", \"height\", \"seed\", \"steps\", \"addWatermark\", \"watermark\", \"segment\", \"batchSize\")\n    OUTPUT_NODE = True\n    FUNCTION = \"doWork\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"prompt\": fields.STRING_ML,\n                \"negativePrompt\": fields.STRING_ML,\n                \"width\": (\"INT\", {\"default\": 1024,\n                       \"min\": 256,\n                       \"max\": 2048,\n                       \"step\": 1}),\n                \"height\": (\"INT\", {\"default\": 1024,\n                       \"min\": 256,\n                       \"max\": 2048,\n                       \"step\": 1}),\n                \"seed\": (\"INT\", {\n                    \"default\": 0,\n                    \"min\": -1,\n                    \"max\": sys.maxsize,\n                }),\n                \"steps\": (\"INT\", {\"default\": 25,\n                       \"min\": 1,\n                       \"max\": 1000,\n                       \"step\": 1}),\n                \"addWatermark\": fields.BOOL_TRUE,\n                \"watermark\": fields.STRING,\n                \"segment\": fields.STRING,\n                \"batchSize\": fields.INT_POSITIVE,\n            }\n        }\n\n    def doWork(self, prompt, negativePrompt, width, height, seed, steps, addWatermark, watermark, segment, batchSize):\n        if addWatermark == False:\n            watermark = ''\n        return {\"result\": (prompt, negativePrompt, width, height, seed, steps, addWatermark, watermark, segment, batchSize)}\n\nclass StringHub9:",
    "suffix": ""
  },
  {
    "name": "Amirtheahmed/ddd-cqrs-fastapi:src/contexts/photostore/photoregistry/infrastructure/persistence/PyMongoPhotoRegistryRepository.py@1350",
    "canonical_solution": "        entities = [PhotoRegistry.create_from_primitives(result) for result in results]",
    "prompt": "from typing import List, NoReturn, Tuple, Optional\nfrom pymongo import MongoClient, ASCENDING\nfrom pymongo.errors import DuplicateKeyError\nfrom src.contexts.photostore.photoregistry.domain.PhotoRegistryRepository import PhotoRegistryRepository\nfrom src.contexts.photostore.photoregistry.domain.entities.PhotoRegistry import PhotoRegistry\nfrom src.contexts.photostore.photoregistry.domain.errors.PhotoAlreadyExistsError import PhotoRegistryAlreadyExistsError\nfrom src.contexts.shared.Infrastructure.persistence.mongo.PyMongoRepository import PyMongoRepository\nfrom src.contexts.shared.domain.CriteriaQueryMetadata import CriteriaQueryMetadata\nfrom src.contexts.shared.domain.criteria.Criteria import Criteria\n\n\n\n\nclass PyMongoPhotoRegistryRepository(PyMongoRepository, PhotoRegistryRepository):\n\n    __COLLECTION_NAME = 'photo-registry'\n    __DATABASE_NAME = 'python-ddd-example'\n\n    def __init__(self, client: MongoClient):\n        super().__init__(client)\n        super()._get_collection().create_index([\n            ('id', ASCENDING)\n        ], unique=True)\n\n    def get_database_name(self):\n        return self.__DATABASE_NAME\n\n    def get_collection_name(self):\n        return self.__COLLECTION_NAME\n\n    async def find_by_criteria(self, criteria: Criteria) -> Tuple[List[PhotoRegistry], Optional[CriteriaQueryMetadata]]:\n        results, count = await super()._find_by_criteria(criteria)",
    "prefix": "from typing import List, NoReturn, Tuple, Optional\nfrom pymongo import MongoClient, ASCENDING\nfrom pymongo.errors import DuplicateKeyError\nfrom src.contexts.photostore.photoregistry.domain.PhotoRegistryRepository import PhotoRegistryRepository\nfrom src.contexts.photostore.photoregistry.domain.entities.PhotoRegistry import PhotoRegistry\nfrom src.contexts.photostore.photoregistry.domain.errors.PhotoAlreadyExistsError import PhotoRegistryAlreadyExistsError\nfrom src.contexts.shared.Infrastructure.persistence.mongo.PyMongoRepository import PyMongoRepository\nfrom src.contexts.shared.domain.CriteriaQueryMetadata import CriteriaQueryMetadata\nfrom src.contexts.shared.domain.criteria.Criteria import Criteria\n\n\n\n\nclass PyMongoPhotoRegistryRepository(PyMongoRepository, PhotoRegistryRepository):\n\n    __COLLECTION_NAME = 'photo-registry'\n    __DATABASE_NAME = 'python-ddd-example'\n\n    def __init__(self, client: MongoClient):\n        super().__init__(client)\n        super()._get_collection().create_index([\n            ('id', ASCENDING)\n        ], unique=True)\n\n    def get_database_name(self):\n        return self.__DATABASE_NAME\n\n    def get_collection_name(self):\n        return self.__COLLECTION_NAME\n\n    async def find_by_criteria(self, criteria: Criteria) -> Tuple[List[PhotoRegistry], Optional[CriteriaQueryMetadata]]:\n        results, count = await super()._find_by_criteria(criteria)",
    "suffix": ""
  },
  {
    "name": "JINO-ROHIT/RAG-with-Memory:vlite_db/main.py@903",
    "canonical_solution": "        self.model = EmbeddingModel() if model_name is None else EmbeddingModel(model_name)",
    "prompt": "import numpy as np\nimport datetime\nfrom uuid import uuid4\nfrom .model import EmbeddingModel\nfrom .utils import chop_and_chunk, cos_sim\n\nclass VLite:\n    '''\n    vlite is a simple vector database that stores vectors in a numpy array.\n    '''\n    def __init__(self, collection=None,device='mps',model_name=None):\n\n\t\t# Filename must be unique between runs. Saving to the same file will append vectors to previous run's vectors\n        if collection is None:\n            current_datetime = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            collection = f\"vlite_{current_datetime}.npz\"\n            \n        self.collection = collection\n        self.device = device",
    "prefix": "import numpy as np\nimport datetime\nfrom uuid import uuid4\nfrom .model import EmbeddingModel\nfrom .utils import chop_and_chunk, cos_sim\n\nclass VLite:\n    '''\n    vlite is a simple vector database that stores vectors in a numpy array.\n    '''\n    def __init__(self, collection=None,device='mps',model_name=None):\n\n\t\t# Filename must be unique between runs. Saving to the same file will append vectors to previous run's vectors\n        if collection is None:\n            current_datetime = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            collection = f\"vlite_{current_datetime}.npz\"\n            \n        self.collection = collection\n        self.device = device",
    "suffix": ""
  },
  {
    "name": "kvojps/open-source-project-observatory:backend/api/services/dtos/repository.py@704",
    "canonical_solution": "            issues_graphic=IssueGraphicResponse(",
    "prompt": "from typing import List, Optional\nfrom pydantic import BaseModel\nfrom .repository_graphics import (\n    IssueGraphicResponse,\n    LabelResponse,\n    PullRequestGraphicResponse,\n)\n\n\nclass RepositoryResponse(BaseModel):\n    owner: Optional[str]\n    name: Optional[str]\n    description: Optional[str]\n    last_activity: Optional[str]\n    license_url: Optional[str]\n    website_url: Optional[str]\n    topics: Optional[List[str]]\n    stars_amount: Optional[int]\n    readme_url: Optional[str]\n    contribution_url: Optional[str]\n    contributors_amount: Optional[int]\n    forks_amount: Optional[int]\n    branches_amount: Optional[int]\n    issues_amount: Optional[int]\n    prs_amount: Optional[int]\n    commits_amount: Optional[int]\n    issues_graphic: Optional[IssueGraphicResponse]\n    prs_graphic: Optional[PullRequestGraphicResponse]\n    top_ten_labels_graphic: Optional[List[LabelResponse]]\n\n    @classmethod\n    def from_repository_response(\n        cls, owner: str, name: str, repository_data\n    ) -> \"RepositoryResponse\":\n        labels_data = repository_data.get(\"labels\", None).get(\"nodes\", None)\n        sorted_labels_data = sorted(\n            labels_data, key=lambda x: x[\"issues\"][\"totalCount\"], reverse=True\n        )\n\n        return cls(\n            owner=owner,\n            name=name,\n            description=repository_data.get(\"description\", None),\n            last_activity=repository_data.get(\"defaultBranchRef\", None)\n            .get(\"target\", None)\n            .get(\"history\", None)\n            .get(\"edges\", None)[0]\n            .get(\"node\", None)\n            .get(\"committedDate\", None),\n            license_url=None,\n            website_url=repository_data.get(\"homepageUrl\", None),\n            topics=[\n                topic[\"node\"][\"topic\"][\"name\"]\n                for topic in repository_data.get(\"repositoryTopics\", None).get(\n                    \"edges\", []\n                )\n            ],\n            stars_amount=repository_data.get(\"stargazers\", None).get(\n                \"totalCount\", None\n            ),\n            readme_url=None,\n            contribution_url=None,\n            contributors_amount=repository_data.get(\"mentionableUsers\", None).get(\n                \"totalCount\", None\n            ),\n            forks_amount=repository_data.get(\"forks\", None).get(\"totalCount\", None),\n            branches_amount=repository_data.get(\"refs\", None).get(\"totalCount\", None),\n            issues_amount=repository_data.get(\"issues\", None).get(\"totalCount\", None),\n            prs_amount=repository_data.get(\"pullRequests\", None).get(\n                \"totalCount\", None\n            ),\n            commits_amount=repository_data.get(\"defaultBranchRef\", None)\n            .get(\"target\", None)\n            .get(\"totalCommits\", None)\n            .get(\"totalCount\", None),",
    "prefix": "from typing import List, Optional\nfrom pydantic import BaseModel\nfrom .repository_graphics import (\n    IssueGraphicResponse,\n    LabelResponse,\n    PullRequestGraphicResponse,\n)\n\n\nclass RepositoryResponse(BaseModel):\n    owner: Optional[str]\n    name: Optional[str]\n    description: Optional[str]\n    last_activity: Optional[str]\n    license_url: Optional[str]\n    website_url: Optional[str]\n    topics: Optional[List[str]]\n    stars_amount: Optional[int]\n    readme_url: Optional[str]\n    contribution_url: Optional[str]\n    contributors_amount: Optional[int]\n    forks_amount: Optional[int]\n    branches_amount: Optional[int]\n    issues_amount: Optional[int]\n    prs_amount: Optional[int]\n    commits_amount: Optional[int]\n    issues_graphic: Optional[IssueGraphicResponse]\n    prs_graphic: Optional[PullRequestGraphicResponse]\n    top_ten_labels_graphic: Optional[List[LabelResponse]]\n\n    @classmethod\n    def from_repository_response(\n        cls, owner: str, name: str, repository_data\n    ) -> \"RepositoryResponse\":\n        labels_data = repository_data.get(\"labels\", None).get(\"nodes\", None)\n        sorted_labels_data = sorted(\n            labels_data, key=lambda x: x[\"issues\"][\"totalCount\"], reverse=True\n        )\n\n        return cls(\n            owner=owner,\n            name=name,\n            description=repository_data.get(\"description\", None),\n            last_activity=repository_data.get(\"defaultBranchRef\", None)\n            .get(\"target\", None)\n            .get(\"history\", None)\n            .get(\"edges\", None)[0]\n            .get(\"node\", None)\n            .get(\"committedDate\", None),\n            license_url=None,\n            website_url=repository_data.get(\"homepageUrl\", None),\n            topics=[\n                topic[\"node\"][\"topic\"][\"name\"]\n                for topic in repository_data.get(\"repositoryTopics\", None).get(\n                    \"edges\", []\n                )\n            ],\n            stars_amount=repository_data.get(\"stargazers\", None).get(\n                \"totalCount\", None\n            ),\n            readme_url=None,\n            contribution_url=None,\n            contributors_amount=repository_data.get(\"mentionableUsers\", None).get(\n                \"totalCount\", None\n            ),\n            forks_amount=repository_data.get(\"forks\", None).get(\"totalCount\", None),\n            branches_amount=repository_data.get(\"refs\", None).get(\"totalCount\", None),\n            issues_amount=repository_data.get(\"issues\", None).get(\"totalCount\", None),\n            prs_amount=repository_data.get(\"pullRequests\", None).get(\n                \"totalCount\", None\n            ),\n            commits_amount=repository_data.get(\"defaultBranchRef\", None)\n            .get(\"target\", None)\n            .get(\"totalCommits\", None)\n            .get(\"totalCount\", None),",
    "suffix": ""
  },
  {
    "name": "avataar/bg_electricity_regulated_pricing:custom_components/bg_electricity_regulated_pricing/sensor.py@834",
    "canonical_solution": "                       + PROVIDER_PRICES[provider][\"fees\"]) * (1 + VAT_RATE)",
    "prompt": "from homeassistant.components.sensor import SensorEntity, SensorEntityDescription, \\\n    SensorStateClass\nfrom homeassistant.config_entries import ConfigEntry\nfrom homeassistant.core import HomeAssistant\nfrom homeassistant.helpers.entity_platform import AddEntitiesCallback\nfrom homeassistant.util import utcnow\nfrom homeassistant.helpers.device_registry import DeviceEntryType, DeviceInfo\nfrom .const import CONF_TARIFF_TYPE, CONF_PROVIDER, CONF_CUSTOM_DAY_PRICE, \\\n    CONF_CUSTOM_NIGHT_PRICE, PROVIDER_PRICES, CONF_CLOCK_OFFSET, \\\n    BGN_PER_KILOWATT_HOUR, VAT_RATE, DOMAIN\n\"\"\"Sensor platform for bg_electricity_regulated_pricing integration.\"\"\"\nfrom __future__ import annotations\n\n\n\n\n\nasync def async_setup_entry(\n        hass: HomeAssistant,\n        config_entry: ConfigEntry,\n        async_add_entities: AddEntitiesCallback,\n) -> None:\n    \"\"\"Initialize bg_electricity_regulated_pricing config entry.\"\"\"\n    name = config_entry.title\n    unique_id = config_entry.entry_id\n\n    tariff_type = config_entry.options[CONF_TARIFF_TYPE]\n    clock_offset = config_entry.options[CONF_CLOCK_OFFSET]\n    provider = config_entry.options[CONF_PROVIDER]\n    if provider == \"custom\":\n        price_day = config_entry.options[CONF_CUSTOM_DAY_PRICE]\n        price_night = config_entry.options[CONF_CUSTOM_NIGHT_PRICE]\n    else:\n        price_day = (PROVIDER_PRICES[provider][\"day\"]\n                     + PROVIDER_PRICES[provider][\"fees\"]) * (1 + VAT_RATE)\n        price_night = (PROVIDER_PRICES[provider][\"night\"]",
    "prefix": "from homeassistant.components.sensor import SensorEntity, SensorEntityDescription, \\\n    SensorStateClass\nfrom homeassistant.config_entries import ConfigEntry\nfrom homeassistant.core import HomeAssistant\nfrom homeassistant.helpers.entity_platform import AddEntitiesCallback\nfrom homeassistant.util import utcnow\nfrom homeassistant.helpers.device_registry import DeviceEntryType, DeviceInfo\nfrom .const import CONF_TARIFF_TYPE, CONF_PROVIDER, CONF_CUSTOM_DAY_PRICE, \\\n    CONF_CUSTOM_NIGHT_PRICE, PROVIDER_PRICES, CONF_CLOCK_OFFSET, \\\n    BGN_PER_KILOWATT_HOUR, VAT_RATE, DOMAIN\n\"\"\"Sensor platform for bg_electricity_regulated_pricing integration.\"\"\"\nfrom __future__ import annotations\n\n\n\n\n\nasync def async_setup_entry(\n        hass: HomeAssistant,\n        config_entry: ConfigEntry,\n        async_add_entities: AddEntitiesCallback,\n) -> None:\n    \"\"\"Initialize bg_electricity_regulated_pricing config entry.\"\"\"\n    name = config_entry.title\n    unique_id = config_entry.entry_id\n\n    tariff_type = config_entry.options[CONF_TARIFF_TYPE]\n    clock_offset = config_entry.options[CONF_CLOCK_OFFSET]\n    provider = config_entry.options[CONF_PROVIDER]\n    if provider == \"custom\":\n        price_day = config_entry.options[CONF_CUSTOM_DAY_PRICE]\n        price_night = config_entry.options[CONF_CUSTOM_NIGHT_PRICE]\n    else:\n        price_day = (PROVIDER_PRICES[provider][\"day\"]\n                     + PROVIDER_PRICES[provider][\"fees\"]) * (1 + VAT_RATE)\n        price_night = (PROVIDER_PRICES[provider][\"night\"]",
    "suffix": ""
  },
  {
    "name": "smonsays/modular-hyperteacher:examples/maml-omniglot.py@1000",
    "canonical_solution": "        test=Dataset(x=batch[\"test\"].inputs, y=batch[\"test\"].targets),",
    "prompt": "import argparse\nimport jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport optax\nimport metax\nfrom jax_meta.datasets import Omniglot\nfrom metax.data.base import DATAPATH, Dataset, MetaDataset\n\"\"\"\nCopyright (c) Simon Schug\nAll rights reserved.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\"\"\"\n\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--batch_size\", type=int, default=None)\nparser.add_argument(\"--bn_decay\", type=float, default=0.9)\nparser.add_argument(\"--channels\", type=int, default=64)\nparser.add_argument(\"--num_tasks_test\", type=int, default=100)\nparser.add_argument(\"--num_tasks_train\", type=int, default=10000)\nparser.add_argument(\"--num_tasks_valid\", type=int, default=10)\nparser.add_argument(\"--ways\", type=int, default=5)\nparser.add_argument(\"--shots_test\", type=int, default=10)\nparser.add_argument(\"--shots_train\", type=int, default=10)\nparser.add_argument(\"--first_order\", type=bool, default=False)\nparser.add_argument(\"--lr_inner\", type=float, default=0.4)\nparser.add_argument(\"--lr_outer\", type=float, default=0.001)\nparser.add_argument(\"--meta_batch_size\", type=int, default=16)\nparser.add_argument(\"--steps_inner\", type=int, default=1)\nparser.add_argument(\"--steps_outer\", type=int, default=100)\nparser.add_argument(\"--seed\", type=int, default=2022)\nargs = parser.parse_args()\n\n\n# Load data from [jax_meta](https://github.com/tristandeleu/jax-meta-learning)\nmetaloader = Omniglot(\n    DATAPATH,\n    batch_size=args.meta_batch_size,\n    shots=args.shots_train,\n    ways=args.ways,\n)\n\nmetaloader.input_shape = metaloader.shape\nmetaloader.output_dim = metaloader.ways\nmetaloader.sample_input = jnp.array(metaloader.dummy_input)\n\n# Define the loss, meta-model and meta-learning algorithm\nbase_model = metax.models.Conv4(args.channels, args.bn_decay, readout=args.ways)\nmeta_model = metax.module.LearnedInit(\n    loss_fn_inner=metax.energy.CrossEntropy(),\n    loss_fn_outer=metax.energy.CrossEntropy(),\n    base_learner=base_model,\n    reg_strength=None\n)\nmeta_learner = metax.learner.ModelAgnosticMetaLearning(\n    meta_model=meta_model,\n    batch_size=args.batch_size,\n    steps_inner=args.steps_inner,\n    optim_fn_inner=optax.sgd(args.lr_inner),\n    optim_fn_outer=optax.adam(args.lr_outer),\n    first_order=args.first_order,\n)\n\n# Initialize\nrng = jax.random.PRNGKey(args.seed)\nrng_reset, rng_train, rng_test = jax.random.split(rng, 3)\nmeta_state = meta_learner.reset(rng_reset, metaloader.sample_input)\nmeta_update = jax.jit(meta_learner.update)\nmeta_eval = jax.jit(meta_learner.eval, static_argnames=\"steps\")\n\n# Train\nfor idx, batch in zip(range(args.steps_outer), metaloader):\n    # Mangle data into the format expected by metax\n    batch = MetaDataset(\n        train=Dataset(x=batch[\"train\"].inputs, y=batch[\"train\"].targets),",
    "prefix": "import argparse\nimport jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nimport optax\nimport metax\nfrom jax_meta.datasets import Omniglot\nfrom metax.data.base import DATAPATH, Dataset, MetaDataset\n\"\"\"\nCopyright (c) Simon Schug\nAll rights reserved.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\"\"\"\n\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--batch_size\", type=int, default=None)\nparser.add_argument(\"--bn_decay\", type=float, default=0.9)\nparser.add_argument(\"--channels\", type=int, default=64)\nparser.add_argument(\"--num_tasks_test\", type=int, default=100)\nparser.add_argument(\"--num_tasks_train\", type=int, default=10000)\nparser.add_argument(\"--num_tasks_valid\", type=int, default=10)\nparser.add_argument(\"--ways\", type=int, default=5)\nparser.add_argument(\"--shots_test\", type=int, default=10)\nparser.add_argument(\"--shots_train\", type=int, default=10)\nparser.add_argument(\"--first_order\", type=bool, default=False)\nparser.add_argument(\"--lr_inner\", type=float, default=0.4)\nparser.add_argument(\"--lr_outer\", type=float, default=0.001)\nparser.add_argument(\"--meta_batch_size\", type=int, default=16)\nparser.add_argument(\"--steps_inner\", type=int, default=1)\nparser.add_argument(\"--steps_outer\", type=int, default=100)\nparser.add_argument(\"--seed\", type=int, default=2022)\nargs = parser.parse_args()\n\n\n# Load data from [jax_meta](https://github.com/tristandeleu/jax-meta-learning)\nmetaloader = Omniglot(\n    DATAPATH,\n    batch_size=args.meta_batch_size,\n    shots=args.shots_train,\n    ways=args.ways,\n)\n\nmetaloader.input_shape = metaloader.shape\nmetaloader.output_dim = metaloader.ways\nmetaloader.sample_input = jnp.array(metaloader.dummy_input)\n\n# Define the loss, meta-model and meta-learning algorithm\nbase_model = metax.models.Conv4(args.channels, args.bn_decay, readout=args.ways)\nmeta_model = metax.module.LearnedInit(\n    loss_fn_inner=metax.energy.CrossEntropy(),\n    loss_fn_outer=metax.energy.CrossEntropy(),\n    base_learner=base_model,\n    reg_strength=None\n)\nmeta_learner = metax.learner.ModelAgnosticMetaLearning(\n    meta_model=meta_model,\n    batch_size=args.batch_size,\n    steps_inner=args.steps_inner,\n    optim_fn_inner=optax.sgd(args.lr_inner),\n    optim_fn_outer=optax.adam(args.lr_outer),\n    first_order=args.first_order,\n)\n\n# Initialize\nrng = jax.random.PRNGKey(args.seed)\nrng_reset, rng_train, rng_test = jax.random.split(rng, 3)\nmeta_state = meta_learner.reset(rng_reset, metaloader.sample_input)\nmeta_update = jax.jit(meta_learner.update)\nmeta_eval = jax.jit(meta_learner.eval, static_argnames=\"steps\")\n\n# Train\nfor idx, batch in zip(range(args.steps_outer), metaloader):\n    # Mangle data into the format expected by metax\n    batch = MetaDataset(\n        train=Dataset(x=batch[\"train\"].inputs, y=batch[\"train\"].targets),",
    "suffix": ""
  },
  {
    "name": "AContesini/Convert_PDF_to_DOCX_or_vice-versa:venv/Lib/site-packages/tqdm/contrib/telegram.py@1084",
    "canonical_solution": "            tqdm_auto.write(str(e))",
    "prompt": "from os import getenv\nfrom warnings import warn\nfrom requests import Session\nfrom ..auto import tqdm as tqdm_auto\nfrom ..std import TqdmWarning\nfrom .utils_worker import MonoWorker\n\"\"\"\nSends updates to a Telegram bot.\n\nUsage:\n>>> from tqdm.contrib.telegram import tqdm, trange\n>>> for i in trange(10, token='{token}', chat_id='{chat_id}'):\n...     ...\n\n![screenshot](https://tqdm.github.io/img/screenshot-telegram.gif)\n\"\"\"\n\n\n\n__author__ = {\"github.com/\": [\"casperdcl\"]}\n__all__ = ['TelegramIO', 'tqdm_telegram', 'ttgrange', 'tqdm', 'trange']\n\n\nclass TelegramIO(MonoWorker):\n    \"\"\"Non-blocking file-like IO using a Telegram Bot.\"\"\"\n    API = 'https://api.telegram.org/bot'\n\n    def __init__(self, token, chat_id):\n        \"\"\"Creates a new message in the given `chat_id`.\"\"\"\n        super(TelegramIO, self).__init__()\n        self.token = token\n        self.chat_id = chat_id\n        self.session = Session()\n        self.text = self.__class__.__name__\n        self.message_id\n\n    @property\n    def message_id(self):\n        if hasattr(self, '_message_id'):\n            return self._message_id\n        try:\n            res = self.session.post(\n                self.API + '%s/sendMessage' % self.token,\n                data={'text': '`' + self.text + '`', 'chat_id': self.chat_id,\n                      'parse_mode': 'MarkdownV2'}).json()\n        except Exception as e:\n            tqdm_auto.write(str(e))\n        else:\n            if res.get('error_code') == 429:\n                warn(\"Creation rate limit: try increasing `mininterval`.\",\n                     TqdmWarning, stacklevel=2)\n            else:\n                self._message_id = res['result']['message_id']\n                return self._message_id\n\n    def write(self, s):\n        \"\"\"Replaces internal `message_id`'s text with `s`.\"\"\"\n        if not s:\n            s = \"...\"\n        s = s.replace('\\r', '').strip()\n        if s == self.text:\n            return  # avoid duplicate message Bot error\n        message_id = self.message_id\n        if message_id is None:\n            return\n        self.text = s\n        try:\n            future = self.submit(\n                self.session.post, self.API + '%s/editMessageText' % self.token,\n                data={'text': '`' + s + '`', 'chat_id': self.chat_id,\n                      'message_id': message_id, 'parse_mode': 'MarkdownV2'})\n        except Exception as e:\n            tqdm_auto.write(str(e))\n        else:\n            return future\n\n    def delete(self):\n        \"\"\"Deletes internal `message_id`.\"\"\"\n        try:\n            future = self.submit(\n                self.session.post, self.API + '%s/deleteMessage' % self.token,\n                data={'chat_id': self.chat_id, 'message_id': self.message_id})\n        except Exception as e:",
    "prefix": "from os import getenv\nfrom warnings import warn\nfrom requests import Session\nfrom ..auto import tqdm as tqdm_auto\nfrom ..std import TqdmWarning\nfrom .utils_worker import MonoWorker\n\"\"\"\nSends updates to a Telegram bot.\n\nUsage:\n>>> from tqdm.contrib.telegram import tqdm, trange\n>>> for i in trange(10, token='{token}', chat_id='{chat_id}'):\n...     ...\n\n![screenshot](https://tqdm.github.io/img/screenshot-telegram.gif)\n\"\"\"\n\n\n\n__author__ = {\"github.com/\": [\"casperdcl\"]}\n__all__ = ['TelegramIO', 'tqdm_telegram', 'ttgrange', 'tqdm', 'trange']\n\n\nclass TelegramIO(MonoWorker):\n    \"\"\"Non-blocking file-like IO using a Telegram Bot.\"\"\"\n    API = 'https://api.telegram.org/bot'\n\n    def __init__(self, token, chat_id):\n        \"\"\"Creates a new message in the given `chat_id`.\"\"\"\n        super(TelegramIO, self).__init__()\n        self.token = token\n        self.chat_id = chat_id\n        self.session = Session()\n        self.text = self.__class__.__name__\n        self.message_id\n\n    @property\n    def message_id(self):\n        if hasattr(self, '_message_id'):\n            return self._message_id\n        try:\n            res = self.session.post(\n                self.API + '%s/sendMessage' % self.token,\n                data={'text': '`' + self.text + '`', 'chat_id': self.chat_id,\n                      'parse_mode': 'MarkdownV2'}).json()\n        except Exception as e:\n            tqdm_auto.write(str(e))\n        else:\n            if res.get('error_code') == 429:\n                warn(\"Creation rate limit: try increasing `mininterval`.\",\n                     TqdmWarning, stacklevel=2)\n            else:\n                self._message_id = res['result']['message_id']\n                return self._message_id\n\n    def write(self, s):\n        \"\"\"Replaces internal `message_id`'s text with `s`.\"\"\"\n        if not s:\n            s = \"...\"\n        s = s.replace('\\r', '').strip()\n        if s == self.text:\n            return  # avoid duplicate message Bot error\n        message_id = self.message_id\n        if message_id is None:\n            return\n        self.text = s\n        try:\n            future = self.submit(\n                self.session.post, self.API + '%s/editMessageText' % self.token,\n                data={'text': '`' + s + '`', 'chat_id': self.chat_id,\n                      'message_id': message_id, 'parse_mode': 'MarkdownV2'})\n        except Exception as e:\n            tqdm_auto.write(str(e))\n        else:\n            return future\n\n    def delete(self):\n        \"\"\"Deletes internal `message_id`.\"\"\"\n        try:\n            future = self.submit(\n                self.session.post, self.API + '%s/deleteMessage' % self.token,\n                data={'chat_id': self.chat_id, 'message_id': self.message_id})\n        except Exception as e:",
    "suffix": ""
  },
  {
    "name": "willfinnigan/RetroBioCat_2:tests/test_precedent_identification/test_similarity.py@974",
    "canonical_solution": "    assert fps == [get_single_fp('CCCO')]",
    "prompt": "import pandas as pd\nfrom rbc2.precedent_identification.similarity_tools import get_single_fp, bulk_similarity, get_fingerprints, \\\n    get_fingerprints_from_fpdf, make_fp_df\n\n\n\ndef test_bulk_similarity():\n    target_smi = 'CCCC=O'\n    compare_with = ['CCCO', 'CCCCN', 'CCC(C)CC(=O)O']\n    target_fp = get_single_fp(target_smi)\n    sims = bulk_similarity(target_fp, compare_with, get_fingerprints)\n    assert sims == {\"CCCO\": 0.2, 'CCCCN': 0.2727272727272727, 'CCC(C)CC(=O)O': 0.25}\n\ndef test_bulk_similarity_empty_string():\n    target_smi = 'CCCCCCCCC=O'\n    compare_with = ['', '', '']\n    target_fp = get_single_fp(target_smi)\n    sims = bulk_similarity(target_fp, compare_with, get_fingerprints)\n    assert sims == {\"\": 0}\n\ndef test_get_fingerprints():\n    smis = ['CCCO', 'CCCCN', 'CCC(C)CC(=O)O', 'nan', None]\n    fps, fp_smis = get_fingerprints(smis)\n    assert len(fps) == len(fp_smis)\n\ndef test_make_fp_df():\n    smis = ['CCCO', 'CCCCN', 'CCC(C)CC(=O)O']\n    df = pd.DataFrame({'smiles': smis})\n    fp_df = make_fp_df(df, 'smiles')\n    assert fp_df.loc['CCCO'][0] == get_single_fp('CCCO')\n    assert fp_df.shape[0] == 3\n\ndef test_can_lookup_fp_from_fpdf():\n    smis = ['CCCO', 'CCCCN', 'CCC(C)CC(=O)O']\n    df = pd.DataFrame({'smiles': smis})\n    fp_df = make_fp_df(df, 'smiles')\n    fps, c_smis = get_fingerprints_from_fpdf(['CCCO'], fp_df)",
    "prefix": "import pandas as pd\nfrom rbc2.precedent_identification.similarity_tools import get_single_fp, bulk_similarity, get_fingerprints, \\\n    get_fingerprints_from_fpdf, make_fp_df\n\n\n\ndef test_bulk_similarity():\n    target_smi = 'CCCC=O'\n    compare_with = ['CCCO', 'CCCCN', 'CCC(C)CC(=O)O']\n    target_fp = get_single_fp(target_smi)\n    sims = bulk_similarity(target_fp, compare_with, get_fingerprints)\n    assert sims == {\"CCCO\": 0.2, 'CCCCN': 0.2727272727272727, 'CCC(C)CC(=O)O': 0.25}\n\ndef test_bulk_similarity_empty_string():\n    target_smi = 'CCCCCCCCC=O'\n    compare_with = ['', '', '']\n    target_fp = get_single_fp(target_smi)\n    sims = bulk_similarity(target_fp, compare_with, get_fingerprints)\n    assert sims == {\"\": 0}\n\ndef test_get_fingerprints():\n    smis = ['CCCO', 'CCCCN', 'CCC(C)CC(=O)O', 'nan', None]\n    fps, fp_smis = get_fingerprints(smis)\n    assert len(fps) == len(fp_smis)\n\ndef test_make_fp_df():\n    smis = ['CCCO', 'CCCCN', 'CCC(C)CC(=O)O']\n    df = pd.DataFrame({'smiles': smis})\n    fp_df = make_fp_df(df, 'smiles')\n    assert fp_df.loc['CCCO'][0] == get_single_fp('CCCO')\n    assert fp_df.shape[0] == 3\n\ndef test_can_lookup_fp_from_fpdf():\n    smis = ['CCCO', 'CCCCN', 'CCC(C)CC(=O)O']\n    df = pd.DataFrame({'smiles': smis})\n    fp_df = make_fp_df(df, 'smiles')\n    fps, c_smis = get_fingerprints_from_fpdf(['CCCO'], fp_df)",
    "suffix": ""
  },
  {
    "name": "DomingoJoseCab/AutoTube:utils/edition/edit.py@1038",
    "canonical_solution": "        top_clips[top] = title_intro(str.upper(names[top]),top_clips[top])\r",
    "prompt": "import os\r\nimport json\r\nfrom moviepy.editor import CompositeVideoClip\r\nfrom utils.edition.autoediting import load_videos, load_audio, generate_product, generate_intro, generate_outro\r\nfrom utils.edition.autotext import title_intro\r\n    from moviepy.config import change_settings\r\n# ==============================================================================\r\n# AutoTube Script\r\n# Creado por: Domingo Caballero\r\n# Canal de YouTube: https://www.youtube.com/@emprendedomingo?=sub_confirmation=1\r\n# Lista de Correo: https://emprendecondomingo.substack.com/\r\n# ==============================================================================\r\n\r\n\r\n\r\ndef main(videos_path, audios_path, output_path, names, base_path):\r\n    videos = load_videos(videos_path)\r\n    audios = load_audio(audios_path)\r\n\r\n    audio_intro = audios.pop(0)\r\n    audio_outro = audios.pop(-1)\r\n    \r\n    intro = generate_intro(videos, audio_intro)\r\n    intro = title_intro(str.upper(names['titulo']),intro)\r\n    outro = generate_outro(videos, audio_outro)\r\n\r\n    top_clips = {}\r\n    top_names = ['top5','top4','top3','top2','top1']\r\n\r\n    for vid, aud, top in zip(videos, audios, top_names):\r\n        top_clips[top] = generate_product(vid, aud)\r",
    "prefix": "import os\r\nimport json\r\nfrom moviepy.editor import CompositeVideoClip\r\nfrom utils.edition.autoediting import load_videos, load_audio, generate_product, generate_intro, generate_outro\r\nfrom utils.edition.autotext import title_intro\r\n    from moviepy.config import change_settings\r\n# ==============================================================================\r\n# AutoTube Script\r\n# Creado por: Domingo Caballero\r\n# Canal de YouTube: https://www.youtube.com/@emprendedomingo?=sub_confirmation=1\r\n# Lista de Correo: https://emprendecondomingo.substack.com/\r\n# ==============================================================================\r\n\r\n\r\n\r\ndef main(videos_path, audios_path, output_path, names, base_path):\r\n    videos = load_videos(videos_path)\r\n    audios = load_audio(audios_path)\r\n\r\n    audio_intro = audios.pop(0)\r\n    audio_outro = audios.pop(-1)\r\n    \r\n    intro = generate_intro(videos, audio_intro)\r\n    intro = title_intro(str.upper(names['titulo']),intro)\r\n    outro = generate_outro(videos, audio_outro)\r\n\r\n    top_clips = {}\r\n    top_names = ['top5','top4','top3','top2','top1']\r\n\r\n    for vid, aud, top in zip(videos, audios, top_names):\r\n        top_clips[top] = generate_product(vid, aud)\r",
    "suffix": ""
  },
  {
    "name": "gregorybchris/typogenetics:tests/test_search.py@1104",
    "canonical_solution": "        strand = Strand.from_str(\"ACGT\")",
    "prompt": "import numpy as np\nfrom typogenetics.search import Editor, EditType\nfrom typogenetics.typogenetics import Strand\n\n\n\nclass TestSearch:\n    def test_select_edit_type(self) -> None:\n        rng = np.random.default_rng(42)\n        assert Editor.select_edit_type(rng) == EditType.INSERT\n\n    def test_mutate(self) -> None:\n        rng = np.random.default_rng(42)\n        strand = Strand.from_str(\"ACGT\")\n        new_strand = Editor.mutate(strand, rng)\n        assert new_strand == Strand.from_str(\"TCGT\")\n\n    def test_insert(self) -> None:\n        rng = np.random.default_rng(42)\n        strand = Strand.from_str(\"ACGT\")\n        new_strand = Editor.insert(strand, rng)\n        assert new_strand == Strand.from_str(\"TACGT\")\n\n    def test_delete(self) -> None:\n        rng = np.random.default_rng(42)",
    "prefix": "import numpy as np\nfrom typogenetics.search import Editor, EditType\nfrom typogenetics.typogenetics import Strand\n\n\n\nclass TestSearch:\n    def test_select_edit_type(self) -> None:\n        rng = np.random.default_rng(42)\n        assert Editor.select_edit_type(rng) == EditType.INSERT\n\n    def test_mutate(self) -> None:\n        rng = np.random.default_rng(42)\n        strand = Strand.from_str(\"ACGT\")\n        new_strand = Editor.mutate(strand, rng)\n        assert new_strand == Strand.from_str(\"TCGT\")\n\n    def test_insert(self) -> None:\n        rng = np.random.default_rng(42)\n        strand = Strand.from_str(\"ACGT\")\n        new_strand = Editor.insert(strand, rng)\n        assert new_strand == Strand.from_str(\"TACGT\")\n\n    def test_delete(self) -> None:\n        rng = np.random.default_rng(42)",
    "suffix": ""
  },
  {
    "name": "chaoren2357/gsplatstudio:gsplatstudio/models/paramOptim/adam_customlr_paramOptim.py@1096",
    "canonical_solution": "        self.xyz_lr_schedule = get_expon_lr_func(lr_init=self.cfg.position_lr_init*spatial_lr_scale,",
    "prompt": "import torch\nimport torch.nn as nn\nimport gsplatstudio\nfrom gsplatstudio.utils.gaussian_utils import get_expon_lr_func\nfrom gsplatstudio.utils.type_utils import *\nfrom gsplatstudio.models.paramOptim.base_paramOptim import BaseParamOptim\n\n\n@dataclass\nclass AdamWithcustomlrParamOptimConfig:\n    position_lr_delay_mult: float = 0.01\n    position_lr_final: float = 1.6e-06\n    position_lr_init:float =  0.00016\n    position_lr_max_steps: float = 30000\n    feature_lr: float = 0.0025\n    rotation_lr: float = 0.001\n    scaling_lr: float = 0.005\n    opacity_lr: float = 0.05\n\n\n@gsplatstudio.register(\"adam+customLR-paramOptim\")\nclass AdamWithcustomlrParamOptim(BaseParamOptim):\n\n    @property\n    def config_class(self):\n        return AdamWithcustomlrParamOptimConfig\n    \n    @property\n    def state(self):\n        return self.optimizer.state_dict()\n    \n    def _restore(self, state, spatial_lr_scale, param_lr_group, max_iter):\n        self.optimizer = torch.optim.Adam(param_lr_group, lr=0.0, eps=1e-15)\n        self.optimizer.load_state_dict(state)\n        self.spatial_lr_scale = spatial_lr_scale \n        self.xyz_lr_schedule = get_expon_lr_func(lr_init=self.cfg.position_lr_init*spatial_lr_scale,\n                                                        lr_final=self.cfg.position_lr_final*spatial_lr_scale,\n                                                        lr_delay_mult=self.cfg.position_lr_delay_mult,\n                                                        max_steps=self.cfg.position_lr_max_steps)\n        self.max_iter = max_iter\n\n    def init_optim(self, param_lr_group, spatial_lr_scale, max_iter):\n        self.optimizer = torch.optim.Adam(param_lr_group, lr=0.0, eps=1e-15)",
    "prefix": "import torch\nimport torch.nn as nn\nimport gsplatstudio\nfrom gsplatstudio.utils.gaussian_utils import get_expon_lr_func\nfrom gsplatstudio.utils.type_utils import *\nfrom gsplatstudio.models.paramOptim.base_paramOptim import BaseParamOptim\n\n\n@dataclass\nclass AdamWithcustomlrParamOptimConfig:\n    position_lr_delay_mult: float = 0.01\n    position_lr_final: float = 1.6e-06\n    position_lr_init:float =  0.00016\n    position_lr_max_steps: float = 30000\n    feature_lr: float = 0.0025\n    rotation_lr: float = 0.001\n    scaling_lr: float = 0.005\n    opacity_lr: float = 0.05\n\n\n@gsplatstudio.register(\"adam+customLR-paramOptim\")\nclass AdamWithcustomlrParamOptim(BaseParamOptim):\n\n    @property\n    def config_class(self):\n        return AdamWithcustomlrParamOptimConfig\n    \n    @property\n    def state(self):\n        return self.optimizer.state_dict()\n    \n    def _restore(self, state, spatial_lr_scale, param_lr_group, max_iter):\n        self.optimizer = torch.optim.Adam(param_lr_group, lr=0.0, eps=1e-15)\n        self.optimizer.load_state_dict(state)\n        self.spatial_lr_scale = spatial_lr_scale \n        self.xyz_lr_schedule = get_expon_lr_func(lr_init=self.cfg.position_lr_init*spatial_lr_scale,\n                                                        lr_final=self.cfg.position_lr_final*spatial_lr_scale,\n                                                        lr_delay_mult=self.cfg.position_lr_delay_mult,\n                                                        max_steps=self.cfg.position_lr_max_steps)\n        self.max_iter = max_iter\n\n    def init_optim(self, param_lr_group, spatial_lr_scale, max_iter):\n        self.optimizer = torch.optim.Adam(param_lr_group, lr=0.0, eps=1e-15)",
    "suffix": ""
  },
  {
    "name": "ddjerqq/beam:src/util.py@935",
    "canonical_solution": "async def send_webhook(url: str, author: User, video: Video) -> None:",
    "prompt": "import os\nimport httpx\nfrom src.types.user import User\nfrom src.types.video import Video\n\n\n\n\ndef get_env(key: str, default: str = None) -> str:\n    \"\"\"\n    gets the environment variable with the given key,\n    or raises an exception if the default is not supplied.\n    \"\"\"\n    var = os.getenv(\"APP_ID\", default)\n\n    if var is not None:\n        return var\n\n    raise Exception(f\"Environment variable {key} not found.\")\n\n\ndef humanize(num: int) -> str:\n    \"\"\"\n    converts a number to a human readable format.\n    \"\"\"\n    if num < 1000:\n        return str(num)\n\n    num = num / 1000\n\n    if num < 1000:\n        return f\"{num:.1f}k\"\n\n    num = num / 1000\n\n    if num < 1000:\n        return f\"{num:.1f}m\"\n\n    num = num / 1000\n\n    return f\"{num:.1f}b\"\n\n\ndef video_info_to_webhook_payload(author: User, video: Video) -> dict[str, str]:\n    \"\"\"converts a video object to a webhook object\"\"\"\n\n    return {\n        \"content\": f\"|| @everyone ||\\n**{author.username}** just posted a new video!\",\n        \"embeds\": [\n            {\n                \"title\": video.title,\n                \"url\": video.share_url,\n                \"color\": 16711422,\n                \"author\": {\n                    \"name\": author.username,\n                    \"url\": f\"https://discordapp.com/users/{author.id}\",\n                    \"icon_url\": author.avatar_url\n                },\n                \"footer\": {\n                    \"text\": f\"views {humanize(video.view_count)} | \"\n                            f\"likes {humanize(video.like_count)} | \"\n                            f\"shares {humanize(video.share_count)} | \"\n                            f\"posted at\",\n                    \"icon_url\": \"https://raw.githubusercontent.com/ddjerqq/beam/master/tiktoklogo.webp\"\n                },\n                \"timestamp\": video.create_timestamp.isoformat(),\n                \"image\": {\n                    \"url\": video.cover_image_url\n                }\n            }\n        ],\n        \"username\": \"Beam\",\n        \"avatar_url\": \"https://raw.githubusercontent.com/ddjerqq/beam/master/beam.jpg\",\n        \"attachments\": []\n    }\n\n",
    "prefix": "import os\nimport httpx\nfrom src.types.user import User\nfrom src.types.video import Video\n\n\n\n\ndef get_env(key: str, default: str = None) -> str:\n    \"\"\"\n    gets the environment variable with the given key,\n    or raises an exception if the default is not supplied.\n    \"\"\"\n    var = os.getenv(\"APP_ID\", default)\n\n    if var is not None:\n        return var\n\n    raise Exception(f\"Environment variable {key} not found.\")\n\n\ndef humanize(num: int) -> str:\n    \"\"\"\n    converts a number to a human readable format.\n    \"\"\"\n    if num < 1000:\n        return str(num)\n\n    num = num / 1000\n\n    if num < 1000:\n        return f\"{num:.1f}k\"\n\n    num = num / 1000\n\n    if num < 1000:\n        return f\"{num:.1f}m\"\n\n    num = num / 1000\n\n    return f\"{num:.1f}b\"\n\n\ndef video_info_to_webhook_payload(author: User, video: Video) -> dict[str, str]:\n    \"\"\"converts a video object to a webhook object\"\"\"\n\n    return {\n        \"content\": f\"|| @everyone ||\\n**{author.username}** just posted a new video!\",\n        \"embeds\": [\n            {\n                \"title\": video.title,\n                \"url\": video.share_url,\n                \"color\": 16711422,\n                \"author\": {\n                    \"name\": author.username,\n                    \"url\": f\"https://discordapp.com/users/{author.id}\",\n                    \"icon_url\": author.avatar_url\n                },\n                \"footer\": {\n                    \"text\": f\"views {humanize(video.view_count)} | \"\n                            f\"likes {humanize(video.like_count)} | \"\n                            f\"shares {humanize(video.share_count)} | \"\n                            f\"posted at\",\n                    \"icon_url\": \"https://raw.githubusercontent.com/ddjerqq/beam/master/tiktoklogo.webp\"\n                },\n                \"timestamp\": video.create_timestamp.isoformat(),\n                \"image\": {\n                    \"url\": video.cover_image_url\n                }\n            }\n        ],\n        \"username\": \"Beam\",\n        \"avatar_url\": \"https://raw.githubusercontent.com/ddjerqq/beam/master/beam.jpg\",\n        \"attachments\": []\n    }\n\n",
    "suffix": ""
  },
  {
    "name": "onestepai/api_rag:src/utils/apis.py@1321",
    "canonical_solution": "        elif 'en_us' == ServiceApiConfig.prompt_language:",
    "prompt": "import time\nimport datetime\nimport logging\nfrom src.config.ServiceApiConfig import ServiceApiConfig\nfrom src.utils.swagger_reader import SwaggerReader\n\n\n\ndef time_start_end(month=1):\n    t = datetime.datetime.now()\n    timestamp = time.time()\n    hours = month * 30 * 24\n    t2 = (t - datetime.timedelta(hours=hours)).strftime(\"%Y-%m-%d %H:%M:%S\")\n    ts2 = time.mktime(time.strptime(t2, '%Y-%m-%d %H:%M:%S'))\n    now = (int(round(int(timestamp) * 1000)))\n    before = int(str(ts2 * 1000).split(\".\")[0])\n    return now, before\n\n\nclass apis_info():\n    def __init__(self):\n\n        swagger_reader = SwaggerReader()\n        api_dic = swagger_reader.read()\n        print(api_dic)\n        if 'zh_cn' == ServiceApiConfig.prompt_language:\n            prompt_means = ',\u542b\u4e49\u662f:'\n            prompt_type = ',\u7c7b\u578b\u662f:'\n            prompt_object = '\u5bf9\u8c61\u5305\u542b:'\n            prompt_parameter = '\u53c2\u6570\u5305\u62ec:'\n            prompt_return = '\u8fd4\u56de\u503c\u53c2\u6570\u5305\u62ec:'",
    "prefix": "import time\nimport datetime\nimport logging\nfrom src.config.ServiceApiConfig import ServiceApiConfig\nfrom src.utils.swagger_reader import SwaggerReader\n\n\n\ndef time_start_end(month=1):\n    t = datetime.datetime.now()\n    timestamp = time.time()\n    hours = month * 30 * 24\n    t2 = (t - datetime.timedelta(hours=hours)).strftime(\"%Y-%m-%d %H:%M:%S\")\n    ts2 = time.mktime(time.strptime(t2, '%Y-%m-%d %H:%M:%S'))\n    now = (int(round(int(timestamp) * 1000)))\n    before = int(str(ts2 * 1000).split(\".\")[0])\n    return now, before\n\n\nclass apis_info():\n    def __init__(self):\n\n        swagger_reader = SwaggerReader()\n        api_dic = swagger_reader.read()\n        print(api_dic)\n        if 'zh_cn' == ServiceApiConfig.prompt_language:\n            prompt_means = ',\u542b\u4e49\u662f:'\n            prompt_type = ',\u7c7b\u578b\u662f:'\n            prompt_object = '\u5bf9\u8c61\u5305\u542b:'\n            prompt_parameter = '\u53c2\u6570\u5305\u62ec:'\n            prompt_return = '\u8fd4\u56de\u503c\u53c2\u6570\u5305\u62ec:'",
    "suffix": ""
  },
  {
    "name": "DerwenAI/textgraphs:textgraphs/graph.py@1387",
    "canonical_solution": "        tokens: typing.List[ Node ],",
    "prompt": "from collections import OrderedDict\nfrom icecream import ic  # pylint: disable=E0401\nfrom .elem import Edge, Node, NodeEnum, RelEnum\nimport json\nimport typing\nimport networkx as nx  # pylint: disable=E0401\nimport spacy  # pylint: disable=E0401\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\nThis class implements a generic, in-memory graph data structure used\nto represent the _lemma graph_.\n\nsee copyright/license https://huggingface.co/spaces/DerwenAI/textgraphs/blob/main/README.md\n\"\"\"\n\n\n\n\n\n######################################################################\n## class definitions\n\nclass SimpleGraph:\n    \"\"\"\nAn in-memory graph used to build a `MultiDiGraph` in NetworkX.\n    \"\"\"\n\n    def __init__ (\n        self\n        ) -> None:\n        \"\"\"\nConstructor.\n        \"\"\"\n        self.nodes: typing.Dict[ str, Node ] = OrderedDict()\n        self.edges: typing.Dict[ str, Edge ] = {}\n        self.lemma_graph: nx.MultiDiGraph = nx.MultiDiGraph()\n\n\n    def reset (\n        self\n        ) -> None:\n        \"\"\"\nRe-initialize the data structures, resetting all but the configuration.\n        \"\"\"\n        self.nodes = OrderedDict()\n        self.edges = {}\n        self.lemma_graph = nx.MultiDiGraph()\n\n\n    def make_node (  # pylint: disable=R0913,R0914\n        self,",
    "prefix": "from collections import OrderedDict\nfrom icecream import ic  # pylint: disable=E0401\nfrom .elem import Edge, Node, NodeEnum, RelEnum\nimport json\nimport typing\nimport networkx as nx  # pylint: disable=E0401\nimport spacy  # pylint: disable=E0401\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\nThis class implements a generic, in-memory graph data structure used\nto represent the _lemma graph_.\n\nsee copyright/license https://huggingface.co/spaces/DerwenAI/textgraphs/blob/main/README.md\n\"\"\"\n\n\n\n\n\n######################################################################\n## class definitions\n\nclass SimpleGraph:\n    \"\"\"\nAn in-memory graph used to build a `MultiDiGraph` in NetworkX.\n    \"\"\"\n\n    def __init__ (\n        self\n        ) -> None:\n        \"\"\"\nConstructor.\n        \"\"\"\n        self.nodes: typing.Dict[ str, Node ] = OrderedDict()\n        self.edges: typing.Dict[ str, Edge ] = {}\n        self.lemma_graph: nx.MultiDiGraph = nx.MultiDiGraph()\n\n\n    def reset (\n        self\n        ) -> None:\n        \"\"\"\nRe-initialize the data structures, resetting all but the configuration.\n        \"\"\"\n        self.nodes = OrderedDict()\n        self.edges = {}\n        self.lemma_graph = nx.MultiDiGraph()\n\n\n    def make_node (  # pylint: disable=R0913,R0914\n        self,",
    "suffix": ""
  },
  {
    "name": "Noubissie237/StockManagment:StockManagment/App/views.py@1180",
    "canonical_solution": "    data = data_cookie(request)",
    "prompt": "from django.shortcuts import render, redirect\nfrom django.http import JsonResponse, HttpResponse\nfrom .models import *\nfrom django.contrib.auth.decorators import login_required\nfrom datetime import datetime\nfrom .utils import panier_cookie, data_cookie, getDataFromApi\nfrom .forms import LoginForm\nfrom django.contrib.auth import authenticate, login, logout\nimport json, requests\n\n\n@login_required(login_url='/login')\ndef shop(request, *args, **kwargs):\n    \"\"\"Vue des produits\"\"\"\n\n    produits = Produit.objects.all()\n\n    data = data_cookie(request)\n    articles = data['articles']\n    commande = data['commande']\n    nombre_article = data['nombre_article']\n\n    context = {\n        'produits': produits,\n        'nombre_article': nombre_article\n    }\n\n\n    return render(request, 'shop/index.html', context)\n\n@login_required(login_url='/login')\ndef panier(request, *args, **kwargs):\n\n    data = data_cookie(request)\n    articles = data['articles']\n    commande = data['commande']\n    nombre_article = data['nombre_article']\n\n\n    context = {\n        'articles' : articles, \n        'commande': commande,\n        'nombre_article': nombre_article\n    }\n\n    return render(request, 'shop/panier.html', context)\n\n@login_required(login_url='/login')\ndef commande(request, *args, **kwargs):\n",
    "prefix": "from django.shortcuts import render, redirect\nfrom django.http import JsonResponse, HttpResponse\nfrom .models import *\nfrom django.contrib.auth.decorators import login_required\nfrom datetime import datetime\nfrom .utils import panier_cookie, data_cookie, getDataFromApi\nfrom .forms import LoginForm\nfrom django.contrib.auth import authenticate, login, logout\nimport json, requests\n\n\n@login_required(login_url='/login')\ndef shop(request, *args, **kwargs):\n    \"\"\"Vue des produits\"\"\"\n\n    produits = Produit.objects.all()\n\n    data = data_cookie(request)\n    articles = data['articles']\n    commande = data['commande']\n    nombre_article = data['nombre_article']\n\n    context = {\n        'produits': produits,\n        'nombre_article': nombre_article\n    }\n\n\n    return render(request, 'shop/index.html', context)\n\n@login_required(login_url='/login')\ndef panier(request, *args, **kwargs):\n\n    data = data_cookie(request)\n    articles = data['articles']\n    commande = data['commande']\n    nombre_article = data['nombre_article']\n\n\n    context = {\n        'articles' : articles, \n        'commande': commande,\n        'nombre_article': nombre_article\n    }\n\n    return render(request, 'shop/panier.html', context)\n\n@login_required(login_url='/login')\ndef commande(request, *args, **kwargs):\n",
    "suffix": ""
  },
  {
    "name": "karloskar/homeassistant-goecontroller-mqtt:custom_components/goecontroller_mqtt/switch.py@804",
    "canonical_solution": "        description: GoEControllerSwitchEntityDescription,",
    "prompt": "import logging\nfrom homeassistant import config_entries, core\nfrom homeassistant.components import mqtt\nfrom homeassistant.components.switch import SwitchEntity\nfrom homeassistant.core import callback\nfrom .definitions.switch import SWITCHES, GoEControllerSwitchEntityDescription\nfrom .entity import GoEControllerEntity\n\"\"\"The go-eController (MQTT) switch.\"\"\"\n\n\n\n_LOGGER = logging.getLogger(__name__)\n\n\nasync def async_setup_entry(\n    hass: core.HomeAssistant,\n    config_entry: config_entries.ConfigEntry,\n    async_add_entities,\n):\n    \"\"\"Config entry setup.\"\"\"\n    async_add_entities(\n        GoEControllerSwitch(config_entry, description)\n        for description in SWITCHES\n        if not description.disabled\n    )\n\n\nclass GoEControllerSwitch(GoEControllerEntity, SwitchEntity):\n    \"\"\"Representation of a go-eController switch that is updated via MQTT.\"\"\"\n\n    entity_description: GoEControllerSwitchEntityDescription\n\n    def __init__(\n        self,\n        config_entry: config_entries.ConfigEntry,",
    "prefix": "import logging\nfrom homeassistant import config_entries, core\nfrom homeassistant.components import mqtt\nfrom homeassistant.components.switch import SwitchEntity\nfrom homeassistant.core import callback\nfrom .definitions.switch import SWITCHES, GoEControllerSwitchEntityDescription\nfrom .entity import GoEControllerEntity\n\"\"\"The go-eController (MQTT) switch.\"\"\"\n\n\n\n_LOGGER = logging.getLogger(__name__)\n\n\nasync def async_setup_entry(\n    hass: core.HomeAssistant,\n    config_entry: config_entries.ConfigEntry,\n    async_add_entities,\n):\n    \"\"\"Config entry setup.\"\"\"\n    async_add_entities(\n        GoEControllerSwitch(config_entry, description)\n        for description in SWITCHES\n        if not description.disabled\n    )\n\n\nclass GoEControllerSwitch(GoEControllerEntity, SwitchEntity):\n    \"\"\"Representation of a go-eController switch that is updated via MQTT.\"\"\"\n\n    entity_description: GoEControllerSwitchEntityDescription\n\n    def __init__(\n        self,\n        config_entry: config_entries.ConfigEntry,",
    "suffix": ""
  },
  {
    "name": "T0kyoB0y/PotatoWidgets:PotatoWidgets/Widget/_Common/_BasicProps.py@1500",
    "canonical_solution": "        if isinstance(var, (Listener, Variable, Poll)):",
    "prompt": "from ...__Import import *\nfrom ...Variable import Listener, Poll, Variable\n\n\nclass BasicProps(Gtk.Widget):\n    def __init__(\n        self,\n        halign,\n        valign,\n        hexpand,\n        vexpand,\n        active,\n        visible,\n        classname,\n        # tooltip,\n        css,\n        size=[10, 10],\n    ):\n        Gtk.Widget.__init__(self)\n        self.set_hexpand(True if hexpand else False)\n        self.set_vexpand(True if vexpand else False)\n        self.set_halign(halign)\n        self.set_valign(valign)\n        self.set_visible(visible)\n        self.set_sensitive(active) if active is not None else None\n        self.set_classname(classname)\n        self.__clasif_size(size)\n        self.apply_css(css) if css else None\n\n        for key, value in locals().items():\n            callback = {\n                \"halign\": self.set_halign,\n                \"valign\": self.set_valign,\n                \"hexpand\": self.set_hexpand,\n                \"vexpand\": self.set_vexpand,\n                \"active\": self.set_sensitive,\n                \"visible\": self.set_visible,\n                \"size\": self.set_size,\n                \"classname\": self.set_classname,\n            }.get(key)\n\n            self.bind(value, callback) if callback else None\n\n    def set_size(self, size):\n        self.__clasif_size(size)\n\n    def set_halign(self, param):\n        super().set_halign(self.__clasif_align(str(param)))\n\n    def set_valign(self, param):\n        super().set_valign(self.__clasif_align(str(param)))\n\n    def __clasif_size(self, size):\n        if isinstance(size, int):\n            self.set_size_request(size, size)\n        elif isinstance(size, list):\n            if len(size) == 2:\n                self.set_size_request(size[0], size[1])\n            elif len(size) == 1:\n                self.set_size_request(size[0], size[0])\n\n    def __clasif_align(self, param):\n        dict = {\n            \"fill\": Gtk.Align.FILL,\n            \"start\": Gtk.Align.START,\n            \"end\": Gtk.Align.END,\n            \"center\": Gtk.Align.CENTER,\n            \"baseline\": Gtk.Align.BASELINE,\n        }\n        return dict.get(param.lower(), Gtk.Align.FILL)\n\n    def set_classname(self, param):\n        if isinstance(param, (str)):\n            context = self.get_style_context()\n            [context.add_class(i) for i in param.split(\" \") if i != \" \"]\n        elif isinstance(param, (list)):\n            for i in param:\n                if isinstance(i, (Listener, Variable, Poll)):\n                    pass\n\n    def apply_css(self, css):\n        if css:\n            self._selfclass = f\"{self.get_css_name().replace()}_{randint(1111, 9999)}\"\n            context = self.get_style_context()\n            context.add_class(self._selfclass)\n\n            context.add_provider(\n                Gtk.CssProvider().load_from_data(\n                    f\".{self._selfclass} {{css}}\".encode()\n                ),\n                Gtk.STYLE_PROVIDER_PRIORITY_APPLICATION,\n            )\n\n    def bind(self, var, callback):",
    "prefix": "from ...__Import import *\nfrom ...Variable import Listener, Poll, Variable\n\n\nclass BasicProps(Gtk.Widget):\n    def __init__(\n        self,\n        halign,\n        valign,\n        hexpand,\n        vexpand,\n        active,\n        visible,\n        classname,\n        # tooltip,\n        css,\n        size=[10, 10],\n    ):\n        Gtk.Widget.__init__(self)\n        self.set_hexpand(True if hexpand else False)\n        self.set_vexpand(True if vexpand else False)\n        self.set_halign(halign)\n        self.set_valign(valign)\n        self.set_visible(visible)\n        self.set_sensitive(active) if active is not None else None\n        self.set_classname(classname)\n        self.__clasif_size(size)\n        self.apply_css(css) if css else None\n\n        for key, value in locals().items():\n            callback = {\n                \"halign\": self.set_halign,\n                \"valign\": self.set_valign,\n                \"hexpand\": self.set_hexpand,\n                \"vexpand\": self.set_vexpand,\n                \"active\": self.set_sensitive,\n                \"visible\": self.set_visible,\n                \"size\": self.set_size,\n                \"classname\": self.set_classname,\n            }.get(key)\n\n            self.bind(value, callback) if callback else None\n\n    def set_size(self, size):\n        self.__clasif_size(size)\n\n    def set_halign(self, param):\n        super().set_halign(self.__clasif_align(str(param)))\n\n    def set_valign(self, param):\n        super().set_valign(self.__clasif_align(str(param)))\n\n    def __clasif_size(self, size):\n        if isinstance(size, int):\n            self.set_size_request(size, size)\n        elif isinstance(size, list):\n            if len(size) == 2:\n                self.set_size_request(size[0], size[1])\n            elif len(size) == 1:\n                self.set_size_request(size[0], size[0])\n\n    def __clasif_align(self, param):\n        dict = {\n            \"fill\": Gtk.Align.FILL,\n            \"start\": Gtk.Align.START,\n            \"end\": Gtk.Align.END,\n            \"center\": Gtk.Align.CENTER,\n            \"baseline\": Gtk.Align.BASELINE,\n        }\n        return dict.get(param.lower(), Gtk.Align.FILL)\n\n    def set_classname(self, param):\n        if isinstance(param, (str)):\n            context = self.get_style_context()\n            [context.add_class(i) for i in param.split(\" \") if i != \" \"]\n        elif isinstance(param, (list)):\n            for i in param:\n                if isinstance(i, (Listener, Variable, Poll)):\n                    pass\n\n    def apply_css(self, css):\n        if css:\n            self._selfclass = f\"{self.get_css_name().replace()}_{randint(1111, 9999)}\"\n            context = self.get_style_context()\n            context.add_class(self._selfclass)\n\n            context.add_provider(\n                Gtk.CssProvider().load_from_data(\n                    f\".{self._selfclass} {{css}}\".encode()\n                ),\n                Gtk.STYLE_PROVIDER_PRIORITY_APPLICATION,\n            )\n\n    def bind(self, var, callback):",
    "suffix": ""
  },
  {
    "name": "Zerohertz/Streamlit-Quant:lib/visual.py@1165",
    "canonical_solution": "        colors = _color(len(signals.columns))",
    "prompt": "import plotly.graph_objs as go\nimport streamlit as st\nimport zerohertzLib as zz\nfrom plotly.subplots import make_subplots\nfrom lib.layout import _main, _transaction\nfrom lib.util import _color\n\n\n\ndef candle():\n    data, xdata = st.session_state[\"cache\"][\"data\"], st.session_state[\"cache\"][\"xdata\"]\n    st.session_state[\"cache\"][\"candle\"] = go.Candlestick(\n        x=xdata,\n        open=data.Open,\n        high=data.High,\n        low=data.Low,\n        close=data.Close,\n        increasing={\"line\": {\"color\": \"red\"}},\n        decreasing={\"line\": {\"color\": \"blue\"}},\n        name=st.session_state[\"cache\"][\"name\"],\n    )\n    st.session_state[\"logger\"].info(\n        f\"\"\"[Plot] Candle Chart: {st.session_state[\"cache\"][\"name\"]} ({st.session_state[\"cache\"][\"symbol\"]})\"\"\"\n    )\n\n\ndef moving_average():\n    xdata = st.session_state[\"cache\"][\"xdata\"]\n    st.session_state[\"cache\"][\"ma\"] = []\n    colors = _color(4, 0.5, \"Set1\")\n    for idx, window in enumerate([5, 20, 60, 120]):\n        st.session_state[\"cache\"][\"ma\"].append(\n            go.Scatter(\n                x=xdata,\n                y=st.session_state[\"cache\"][\"data\"]\n                .iloc[:, :4]\n                .mean(1)\n                .rolling(window)\n                .mean(),\n                mode=\"lines\",\n                name=f\"MA{window}\",\n                line={\"color\": colors[idx]},\n            )\n        )\n    st.session_state[\"logger\"].info(\n        f\"\"\"[Plot] Moving Average: {st.session_state[\"cache\"][\"name\"]} ({st.session_state[\"cache\"][\"symbol\"]})\"\"\"\n    )\n\n\ndef bollinger_bands():\n    bands = zz.quant.util._bollinger_bands(st.session_state[\"cache\"][\"data\"])\n    xdata = st.session_state[\"cache\"][\"xdata\"]\n    st.session_state[\"cache\"][\"bollinger\"] = []\n    for col_, name_, color_ in zip(\n        [\"lower_band\", \"middle_band\", \"upper_band\"],\n        [\"Lower\", \"Middle\", \"Upper\"],\n        [\"rgba(255, 0, 0, 0.5)\", \"rgba(0, 255, 0, 0.5)\", \"rgba(0, 0, 255, 0.5)\"],\n    ):\n        st.session_state[\"cache\"][\"bollinger\"].append(\n            go.Scatter(\n                x=xdata,\n                y=bands[col_],\n                mode=\"lines\",\n                name=name_,\n                line={\"color\": color_},\n            )\n        )\n    st.session_state[\"logger\"].info(\n        f\"\"\"[Plot] Bollinger Bands: {st.session_state[\"cache\"][\"name\"]} ({st.session_state[\"cache\"][\"symbol\"]})\"\"\"\n    )\n\n\ndef _signal(signals):\n    st.session_state[\"cache\"][\"quant\"] = []\n    if isinstance(signals, zz.quant.Quant):\n        threshold_sell, threshold_buy = signals.threshold_sell, signals.threshold_buy\n        signals = signals.signals\n    else:\n        threshold_sell, threshold_buy = -1, 1",
    "prefix": "import plotly.graph_objs as go\nimport streamlit as st\nimport zerohertzLib as zz\nfrom plotly.subplots import make_subplots\nfrom lib.layout import _main, _transaction\nfrom lib.util import _color\n\n\n\ndef candle():\n    data, xdata = st.session_state[\"cache\"][\"data\"], st.session_state[\"cache\"][\"xdata\"]\n    st.session_state[\"cache\"][\"candle\"] = go.Candlestick(\n        x=xdata,\n        open=data.Open,\n        high=data.High,\n        low=data.Low,\n        close=data.Close,\n        increasing={\"line\": {\"color\": \"red\"}},\n        decreasing={\"line\": {\"color\": \"blue\"}},\n        name=st.session_state[\"cache\"][\"name\"],\n    )\n    st.session_state[\"logger\"].info(\n        f\"\"\"[Plot] Candle Chart: {st.session_state[\"cache\"][\"name\"]} ({st.session_state[\"cache\"][\"symbol\"]})\"\"\"\n    )\n\n\ndef moving_average():\n    xdata = st.session_state[\"cache\"][\"xdata\"]\n    st.session_state[\"cache\"][\"ma\"] = []\n    colors = _color(4, 0.5, \"Set1\")\n    for idx, window in enumerate([5, 20, 60, 120]):\n        st.session_state[\"cache\"][\"ma\"].append(\n            go.Scatter(\n                x=xdata,\n                y=st.session_state[\"cache\"][\"data\"]\n                .iloc[:, :4]\n                .mean(1)\n                .rolling(window)\n                .mean(),\n                mode=\"lines\",\n                name=f\"MA{window}\",\n                line={\"color\": colors[idx]},\n            )\n        )\n    st.session_state[\"logger\"].info(\n        f\"\"\"[Plot] Moving Average: {st.session_state[\"cache\"][\"name\"]} ({st.session_state[\"cache\"][\"symbol\"]})\"\"\"\n    )\n\n\ndef bollinger_bands():\n    bands = zz.quant.util._bollinger_bands(st.session_state[\"cache\"][\"data\"])\n    xdata = st.session_state[\"cache\"][\"xdata\"]\n    st.session_state[\"cache\"][\"bollinger\"] = []\n    for col_, name_, color_ in zip(\n        [\"lower_band\", \"middle_band\", \"upper_band\"],\n        [\"Lower\", \"Middle\", \"Upper\"],\n        [\"rgba(255, 0, 0, 0.5)\", \"rgba(0, 255, 0, 0.5)\", \"rgba(0, 0, 255, 0.5)\"],\n    ):\n        st.session_state[\"cache\"][\"bollinger\"].append(\n            go.Scatter(\n                x=xdata,\n                y=bands[col_],\n                mode=\"lines\",\n                name=name_,\n                line={\"color\": color_},\n            )\n        )\n    st.session_state[\"logger\"].info(\n        f\"\"\"[Plot] Bollinger Bands: {st.session_state[\"cache\"][\"name\"]} ({st.session_state[\"cache\"][\"symbol\"]})\"\"\"\n    )\n\n\ndef _signal(signals):\n    st.session_state[\"cache\"][\"quant\"] = []\n    if isinstance(signals, zz.quant.Quant):\n        threshold_sell, threshold_buy = signals.threshold_sell, signals.threshold_buy\n        signals = signals.signals\n    else:\n        threshold_sell, threshold_buy = -1, 1",
    "suffix": ""
  },
  {
    "name": "acman/py_june:comments/tests.py@898",
    "canonical_solution": "        self.assertIsInstance(response.context[\"form\"], CommentForm)",
    "prompt": "from django.test import TestCase\nfrom django.urls import reverse\nfrom comments.forms import CommentForm\nfrom comments.models import Comment\nfrom core.tests import TestDataMixin\n\n\n\nclass CreateCommentTest(TestDataMixin, TestCase):\n    def setUp(self):\n        super().setUp()\n        self.create_comment_url = reverse(\n            \"comments:create\", kwargs={\"post_slug\": self.post.slug}\n        )\n\n    def test_create_comment_view_get(self):\n        self.client.force_login(self.user)\n\n        response = self.client.get(self.create_comment_url)\n\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"comments/comment_form.html\")\n        self.assertIsInstance(response.context[\"form\"], CommentForm)\n        self.assertEqual(response.context[\"post\"], self.post)\n\n    def test_create_comment_view_post(self):\n        self.client.force_login(self.user)\n\n        data = {\n            \"title\": \"Test Comment1\",\n            \"content\": \"Rest comment content1\",\n        }\n\n        response = self.client.post(self.create_comment_url, data)\n\n        self.assertEqual(response.status_code, 302)\n        self.assertRedirects(\n            response,\n            reverse(\n                \"categories:detail\", kwargs={\"category_slug\": self.post.category.slug}\n            ),\n        )\n\n        comment = Comment.objects.get(title=\"Test Comment1\")\n        self.assertEqual(comment.title, \"Test Comment1\")\n        self.assertEqual(comment.post, self.post)\n\n\nclass UpdateCommentTest(TestDataMixin, TestCase):\n    def setUp(self):\n        super().setUp()\n        self.update_comment_url = reverse(\n            \"comments:update\",\n            kwargs={\"post_slug\": self.post.slug, \"comment_pk\": self.comment.pk},\n        )\n\n    def test_update_comment_view_get(self):\n        self.client.force_login(self.user)\n\n        response = self.client.get(self.update_comment_url)\n\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"comments/comment_update.html\")",
    "prefix": "from django.test import TestCase\nfrom django.urls import reverse\nfrom comments.forms import CommentForm\nfrom comments.models import Comment\nfrom core.tests import TestDataMixin\n\n\n\nclass CreateCommentTest(TestDataMixin, TestCase):\n    def setUp(self):\n        super().setUp()\n        self.create_comment_url = reverse(\n            \"comments:create\", kwargs={\"post_slug\": self.post.slug}\n        )\n\n    def test_create_comment_view_get(self):\n        self.client.force_login(self.user)\n\n        response = self.client.get(self.create_comment_url)\n\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"comments/comment_form.html\")\n        self.assertIsInstance(response.context[\"form\"], CommentForm)\n        self.assertEqual(response.context[\"post\"], self.post)\n\n    def test_create_comment_view_post(self):\n        self.client.force_login(self.user)\n\n        data = {\n            \"title\": \"Test Comment1\",\n            \"content\": \"Rest comment content1\",\n        }\n\n        response = self.client.post(self.create_comment_url, data)\n\n        self.assertEqual(response.status_code, 302)\n        self.assertRedirects(\n            response,\n            reverse(\n                \"categories:detail\", kwargs={\"category_slug\": self.post.category.slug}\n            ),\n        )\n\n        comment = Comment.objects.get(title=\"Test Comment1\")\n        self.assertEqual(comment.title, \"Test Comment1\")\n        self.assertEqual(comment.post, self.post)\n\n\nclass UpdateCommentTest(TestDataMixin, TestCase):\n    def setUp(self):\n        super().setUp()\n        self.update_comment_url = reverse(\n            \"comments:update\",\n            kwargs={\"post_slug\": self.post.slug, \"comment_pk\": self.comment.pk},\n        )\n\n    def test_update_comment_view_get(self):\n        self.client.force_login(self.user)\n\n        response = self.client.get(self.update_comment_url)\n\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"comments/comment_update.html\")",
    "suffix": ""
  },
  {
    "name": "CodeWithEmad/num2fa:num2fa/utils.py@1092",
    "canonical_solution": "        w = HUNDREDS[h] + WORDS_DECIMAL_SEPARATOR",
    "prompt": "from num2fa.constants import (\n    CLASSES,\n    DECIMAL_PLACES,\n    HUNDREDS,\n    NORMALIZATION_TABLE,\n    ONES,\n    TEN_TO_TWENTY,\n    TENS,\n    WORDS_DECIMAL_SEPARATOR,\n    ZERO,\n)\n\n\ndef _normalize_str(number: str) -> str:\n    \"\"\"Normalize the input number string.\"\"\"\n    return str(number).strip().translate(NORMALIZATION_TABLE)\n\n\ndef _point_words(\n    number: str,\n    decimal_separator: str,\n) -> str:\n    before_p, p, after_p = number.partition(\".\")\n    if after_p:\n        if before_p == \"0\":\n            if after_p == \"0\":\n                return ZERO\n            return _natural_words(after_p) + DECIMAL_PLACES[len(after_p)]\n        if after_p != \"0\":\n            return (\n                _natural_words(before_p)\n                + decimal_separator\n                + _natural_words(after_p)\n                + DECIMAL_PLACES[len(after_p)]\n            )\n        return _natural_words(before_p)\n    return _natural_words(before_p)\n\n\ndef _natural_words(str_num: str) -> str:\n    if str_num == \"0\":\n        return ZERO\n    length = len(str_num)\n    if length > len(CLASSES) * 3:\n        raise ValueError(\"out of range\")\n\n    modulo_3 = length % 3\n    if modulo_3:\n        str_num = \"0\" * (3 - modulo_3) + str_num\n        length += 3 - modulo_3\n\n    groups = length // 3\n    group = groups\n    natural_words = \"\"\n    while group > 0:\n        three_digit = str_num[group * 3 - 3 : group * 3]\n        word3 = _three_digit_words(int(three_digit))\n        if word3 and group != groups:\n            if natural_words:\n                natural_words = (\n                    word3\n                    + CLASSES[groups - group]\n                    + WORDS_DECIMAL_SEPARATOR\n                    + natural_words\n                )\n            else:\n                natural_words = word3 + CLASSES[groups - group]\n        else:\n            natural_words = word3 + natural_words\n        group -= 1\n\n    return natural_words\n\n\ndef _three_digit_words(number: int) -> str:\n    \"\"\"Return the word representation of 0 < number < 1000.\"\"\"\n    h, t, o = number // 100, number % 100 // 10, number % 10\n    if h == 0 or (t == o == 0):\n        w = HUNDREDS[h]\n    else:",
    "prefix": "from num2fa.constants import (\n    CLASSES,\n    DECIMAL_PLACES,\n    HUNDREDS,\n    NORMALIZATION_TABLE,\n    ONES,\n    TEN_TO_TWENTY,\n    TENS,\n    WORDS_DECIMAL_SEPARATOR,\n    ZERO,\n)\n\n\ndef _normalize_str(number: str) -> str:\n    \"\"\"Normalize the input number string.\"\"\"\n    return str(number).strip().translate(NORMALIZATION_TABLE)\n\n\ndef _point_words(\n    number: str,\n    decimal_separator: str,\n) -> str:\n    before_p, p, after_p = number.partition(\".\")\n    if after_p:\n        if before_p == \"0\":\n            if after_p == \"0\":\n                return ZERO\n            return _natural_words(after_p) + DECIMAL_PLACES[len(after_p)]\n        if after_p != \"0\":\n            return (\n                _natural_words(before_p)\n                + decimal_separator\n                + _natural_words(after_p)\n                + DECIMAL_PLACES[len(after_p)]\n            )\n        return _natural_words(before_p)\n    return _natural_words(before_p)\n\n\ndef _natural_words(str_num: str) -> str:\n    if str_num == \"0\":\n        return ZERO\n    length = len(str_num)\n    if length > len(CLASSES) * 3:\n        raise ValueError(\"out of range\")\n\n    modulo_3 = length % 3\n    if modulo_3:\n        str_num = \"0\" * (3 - modulo_3) + str_num\n        length += 3 - modulo_3\n\n    groups = length // 3\n    group = groups\n    natural_words = \"\"\n    while group > 0:\n        three_digit = str_num[group * 3 - 3 : group * 3]\n        word3 = _three_digit_words(int(three_digit))\n        if word3 and group != groups:\n            if natural_words:\n                natural_words = (\n                    word3\n                    + CLASSES[groups - group]\n                    + WORDS_DECIMAL_SEPARATOR\n                    + natural_words\n                )\n            else:\n                natural_words = word3 + CLASSES[groups - group]\n        else:\n            natural_words = word3 + natural_words\n        group -= 1\n\n    return natural_words\n\n\ndef _three_digit_words(number: int) -> str:\n    \"\"\"Return the word representation of 0 < number < 1000.\"\"\"\n    h, t, o = number // 100, number % 100 // 10, number % 10\n    if h == 0 or (t == o == 0):\n        w = HUNDREDS[h]\n    else:",
    "suffix": ""
  },
  {
    "name": "the-seeds/cardinal:src/cardinal/core/vectorstore/milvus.py@930",
    "canonical_solution": "        elif isinstance(value, str) and op in [Operator.And, Operator.Or]:",
    "prompt": "import base64\nimport os\nimport pickle\nfrom collections import defaultdict\nfrom typing import Any, List, Optional, Tuple, TypeVar\nfrom pydantic import BaseModel\nfrom typing_extensions import Self\nfrom ..schema import Condition, Operator, VectorStore\nfrom ..utils.import_utils import is_pymilvus_availble\n    from pymilvus import Collection, CollectionSchema, DataType, FieldSchema, connections, utility\n    from pymilvus.orm.types import infer_dtype_bydata\n\n\n\n\nif is_pymilvus_availble():\n\n\nK = List[float]\nV = TypeVar(\"V\", bound=BaseModel)\n\n\nclass MilvusCondition(Condition):\n    def __init__(self, key: str, value: Any, op: Operator) -> None:\n        self._key = key\n        _ops = [\"==\", \"!=\", \">\", \">=\", \"<\", \"<=\", \"in\", \"not in\", \"&&\", \"||\"]\n        if op < len(_ops):\n            self._op = _ops[op]\n        else:\n            raise NotImplementedError\n\n        if isinstance(value, list) and op in [Operator.In, Operator.Notin]:\n            self._value = value",
    "prefix": "import base64\nimport os\nimport pickle\nfrom collections import defaultdict\nfrom typing import Any, List, Optional, Tuple, TypeVar\nfrom pydantic import BaseModel\nfrom typing_extensions import Self\nfrom ..schema import Condition, Operator, VectorStore\nfrom ..utils.import_utils import is_pymilvus_availble\n    from pymilvus import Collection, CollectionSchema, DataType, FieldSchema, connections, utility\n    from pymilvus.orm.types import infer_dtype_bydata\n\n\n\n\nif is_pymilvus_availble():\n\n\nK = List[float]\nV = TypeVar(\"V\", bound=BaseModel)\n\n\nclass MilvusCondition(Condition):\n    def __init__(self, key: str, value: Any, op: Operator) -> None:\n        self._key = key\n        _ops = [\"==\", \"!=\", \">\", \">=\", \"<\", \"<=\", \"in\", \"not in\", \"&&\", \"||\"]\n        if op < len(_ops):\n            self._op = _ops[op]\n        else:\n            raise NotImplementedError\n\n        if isinstance(value, list) and op in [Operator.In, Operator.Notin]:\n            self._value = value",
    "suffix": ""
  },
  {
    "name": "datrocity/pond:tests/metadata/test_git.py@835",
    "canonical_solution": "        GitMetadataSource(git_repo=git.Repo(tmp_path))",
    "prompt": "import os\nimport git\nimport pytest\nfrom unittest.mock import patch, Mock\nfrom git.exc import InvalidGitRepositoryError\nfrom pond.metadata.git import git_repo_name, GitMetadataSource\n\n\n\n\n@patch(\"git.Remote\")\n@patch(\"git.Repo\")\ndef test_git_repo_name_with_origin(MockRepo, MockRemote):\n    repo = MockRepo()\n    origin = MockRemote()\n\n    # with '.git' extension\n    origin.url = 'git@gitserver.com:author/pond.git'\n    repo.remote = Mock(return_value=origin)\n    name = git_repo_name(repo)\n    assert name == 'pond'\n\n    # without '.git' extension\n    origin.url = 'git@gitserver.com:author/pond'\n    repo.remote = Mock(return_value=origin)\n    name = git_repo_name(repo)\n    assert name == 'pond'\n\n\n@patch(\"git.Repo\")\ndef test_git_repo_name_no_remote(MockRepo):\n    repo = MockRepo()\n    repo.remote = Mock(side_effect=KeyError)\n    repo.working_tree_dir = '/blah/foo/pond'\n    name = git_repo_name(repo)\n    assert name == 'pond'\n\n\n@patch(\"git.Remote\")\n@patch(\"git.Repo\")\ndef test_explicit_repo(MockRepo, MockRemote):\n    origin = MockRemote()\n    origin.url = 'git@gitserver.com:author/pond.git'\n\n    repo = MockRepo()\n    repo.remote = Mock(return_value=origin)\n    repo.remote = Mock(return_value=origin)\n    repo.head.object.hexsha = 'bcd'\n    repo.active_branch.name = 'fix'\n\n    source = GitMetadataSource(git_repo=MockRepo())\n    assert source.section_name() == 'git'\n\n    metadata = source.collect()\n    assert metadata['sha'] == 'bcd'\n    assert metadata['name'] == 'pond'\n    assert metadata['branch'] == 'fix'\n\n\ndef test_missing_git_repository_default(tmp_path):\n    \"\"\" Raise exception if the repository does not exist. \"\"\"\n    # Passing path with no git repository\n    with pytest.raises(InvalidGitRepositoryError):",
    "prefix": "import os\nimport git\nimport pytest\nfrom unittest.mock import patch, Mock\nfrom git.exc import InvalidGitRepositoryError\nfrom pond.metadata.git import git_repo_name, GitMetadataSource\n\n\n\n\n@patch(\"git.Remote\")\n@patch(\"git.Repo\")\ndef test_git_repo_name_with_origin(MockRepo, MockRemote):\n    repo = MockRepo()\n    origin = MockRemote()\n\n    # with '.git' extension\n    origin.url = 'git@gitserver.com:author/pond.git'\n    repo.remote = Mock(return_value=origin)\n    name = git_repo_name(repo)\n    assert name == 'pond'\n\n    # without '.git' extension\n    origin.url = 'git@gitserver.com:author/pond'\n    repo.remote = Mock(return_value=origin)\n    name = git_repo_name(repo)\n    assert name == 'pond'\n\n\n@patch(\"git.Repo\")\ndef test_git_repo_name_no_remote(MockRepo):\n    repo = MockRepo()\n    repo.remote = Mock(side_effect=KeyError)\n    repo.working_tree_dir = '/blah/foo/pond'\n    name = git_repo_name(repo)\n    assert name == 'pond'\n\n\n@patch(\"git.Remote\")\n@patch(\"git.Repo\")\ndef test_explicit_repo(MockRepo, MockRemote):\n    origin = MockRemote()\n    origin.url = 'git@gitserver.com:author/pond.git'\n\n    repo = MockRepo()\n    repo.remote = Mock(return_value=origin)\n    repo.remote = Mock(return_value=origin)\n    repo.head.object.hexsha = 'bcd'\n    repo.active_branch.name = 'fix'\n\n    source = GitMetadataSource(git_repo=MockRepo())\n    assert source.section_name() == 'git'\n\n    metadata = source.collect()\n    assert metadata['sha'] == 'bcd'\n    assert metadata['name'] == 'pond'\n    assert metadata['branch'] == 'fix'\n\n\ndef test_missing_git_repository_default(tmp_path):\n    \"\"\" Raise exception if the repository does not exist. \"\"\"\n    # Passing path with no git repository\n    with pytest.raises(InvalidGitRepositoryError):",
    "suffix": ""
  },
  {
    "name": "Zitronenjoghurt/Colonaut:src/constants/config.py@935",
    "canonical_solution": "        self.GLOBAL_STATE_FILE_PATH: str = construct_path(file_paths.get(\"global_state\", DEFAULT[\"file_paths\"][\"global_state\"]))",
    "prompt": "import src.utils.validator as validator\nfrom src.constants.physical_units import EXISTING_CLASSES\nfrom src.utils.file_operations import file_to_dict, construct_path\n\nCONFIG_FILE_PATH = construct_path(\"src/config.json\")\n\nDEFAULT = {\n    \"file_paths\": {\n        \"game_state\": \"src/\",\n        \"global_state\": \"src/\"\n    },\n    \"save_file_mode\": \"cbor\",\n    \"decimal_digits\": 2,\n    \"scientific_notation_upper_treshhold\": 1e7,\n    \"scientific_notation_lower_treshold\": 1e-2,\n    \"config_units\": {\n        \"temperature\": \"\u00b0K\",\n        \"density\": \"kg/m^3\",\n        \"length\": \"km\",\n        \"mass\": \"kg\",\n        \"volume\": \"m^3\",\n        \"time\": \"s\"\n    },\n    \"display_units\": {\n        \"temperature\": \"\u00b0C\",\n        \"density\": \"g/cm^3\",\n        \"length\": \"km\",\n        \"mass\": \"kg\",\n        \"volume\": \"m^3\",\n        \"time\": \"h\"\n    },\n    \"display_units_conveniently\": [\"time\"],\n    \"default_ship_console_char_delay\": 20,\n    \"default_ship_console_line_delay\": 800,\n    \"default_ship_console_style_tag\": \"computer\"\n}\n\nSAVE_FILE_MODES = [\"json\", \"cbor\"]\n\nclass Config():\n    _instance = None\n\n    def __init__(self) -> None:\n        if Config._instance is not None:\n            raise RuntimeError(\"Tried to initialize multiple instances of Config.\")\n        \n        config_data = file_to_dict(CONFIG_FILE_PATH)\n\n        self.DECIMAL_DIGITS: int = config_data.get(\"decimal_digits\", DEFAULT[\"decimal_digits\"])\n        self.SCIENTIFIC_NOTATION_UPPER_TRESHOLD: float = config_data.get(\"scientific_notation_upper_treshold\", DEFAULT[\"scientific_notation_upper_treshhold\"])\n        self.SCIENTIFIC_NOTATION_LOWER_TRESHOLD: float = config_data.get(\"scientific_notation_lower_treshold\", DEFAULT[\"scientific_notation_lower_treshold\"])\n        self.DISPLAY_UNITS_CONVENIENTLY: list[str] = config_data.get(\"display_units_conveniently\", DEFAULT[\"display_units_conveniently\"])\n        self.DEFAULT_SHIP_CONSOLE_CHAR_DELAY: int = config_data.get(\"default_ship_console_char_delay\", DEFAULT[\"default_ship_console_char_delay\"])\n        self.DEFAULT_SHIP_CONSOLE_LINE_DELAY: int = config_data.get(\"default_ship_console_line_delay\", DEFAULT[\"default_ship_console_line_delay\"])\n        self.DEFAULT_SHIP_CONSOLE_STYLE_TAG: str = config_data.get(\"default_ship_console_style_tag\", DEFAULT[\"default_ship_console_style_tag\"])\n        \n        self.SAVE_FILE_MODE: str = config_data.get(\"save_file_mode\", DEFAULT[\"save_file_mode\"])\n        if self.SAVE_FILE_MODE not in SAVE_FILE_MODES:\n            self.SAVE_FILE_MODE = DEFAULT[\"save_file_mode\"]\n        \n        self.CONFIG_UNITS: dict[str, str] = config_data.get(\"config_units\", DEFAULT[\"config_units\"])\n        for unit_class, unit in DEFAULT[\"config_units\"].items():\n            if self.CONFIG_UNITS.get(unit_class, None) is None:\n                self.CONFIG_UNITS[unit_class] = unit\n\n        self.DISPLAY_UNITS: dict[str, str] = config_data.get(\"display_units\", DEFAULT[\"display_units\"])\n        for unit_class, unit in DEFAULT[\"display_units\"].items():\n            if self.DISPLAY_UNITS.get(unit_class, None) is None:\n                self.DISPLAY_UNITS[unit_class] = unit\n        \n        file_paths: dict = config_data.get(\"file_paths\", DEFAULT[\"file_paths\"])\n        self.GAME_STATE_FILE_PATH: str = construct_path(file_paths.get(\"game_state\", DEFAULT[\"file_paths\"][\"game_state\"]))",
    "prefix": "import src.utils.validator as validator\nfrom src.constants.physical_units import EXISTING_CLASSES\nfrom src.utils.file_operations import file_to_dict, construct_path\n\nCONFIG_FILE_PATH = construct_path(\"src/config.json\")\n\nDEFAULT = {\n    \"file_paths\": {\n        \"game_state\": \"src/\",\n        \"global_state\": \"src/\"\n    },\n    \"save_file_mode\": \"cbor\",\n    \"decimal_digits\": 2,\n    \"scientific_notation_upper_treshhold\": 1e7,\n    \"scientific_notation_lower_treshold\": 1e-2,\n    \"config_units\": {\n        \"temperature\": \"\u00b0K\",\n        \"density\": \"kg/m^3\",\n        \"length\": \"km\",\n        \"mass\": \"kg\",\n        \"volume\": \"m^3\",\n        \"time\": \"s\"\n    },\n    \"display_units\": {\n        \"temperature\": \"\u00b0C\",\n        \"density\": \"g/cm^3\",\n        \"length\": \"km\",\n        \"mass\": \"kg\",\n        \"volume\": \"m^3\",\n        \"time\": \"h\"\n    },\n    \"display_units_conveniently\": [\"time\"],\n    \"default_ship_console_char_delay\": 20,\n    \"default_ship_console_line_delay\": 800,\n    \"default_ship_console_style_tag\": \"computer\"\n}\n\nSAVE_FILE_MODES = [\"json\", \"cbor\"]\n\nclass Config():\n    _instance = None\n\n    def __init__(self) -> None:\n        if Config._instance is not None:\n            raise RuntimeError(\"Tried to initialize multiple instances of Config.\")\n        \n        config_data = file_to_dict(CONFIG_FILE_PATH)\n\n        self.DECIMAL_DIGITS: int = config_data.get(\"decimal_digits\", DEFAULT[\"decimal_digits\"])\n        self.SCIENTIFIC_NOTATION_UPPER_TRESHOLD: float = config_data.get(\"scientific_notation_upper_treshold\", DEFAULT[\"scientific_notation_upper_treshhold\"])\n        self.SCIENTIFIC_NOTATION_LOWER_TRESHOLD: float = config_data.get(\"scientific_notation_lower_treshold\", DEFAULT[\"scientific_notation_lower_treshold\"])\n        self.DISPLAY_UNITS_CONVENIENTLY: list[str] = config_data.get(\"display_units_conveniently\", DEFAULT[\"display_units_conveniently\"])\n        self.DEFAULT_SHIP_CONSOLE_CHAR_DELAY: int = config_data.get(\"default_ship_console_char_delay\", DEFAULT[\"default_ship_console_char_delay\"])\n        self.DEFAULT_SHIP_CONSOLE_LINE_DELAY: int = config_data.get(\"default_ship_console_line_delay\", DEFAULT[\"default_ship_console_line_delay\"])\n        self.DEFAULT_SHIP_CONSOLE_STYLE_TAG: str = config_data.get(\"default_ship_console_style_tag\", DEFAULT[\"default_ship_console_style_tag\"])\n        \n        self.SAVE_FILE_MODE: str = config_data.get(\"save_file_mode\", DEFAULT[\"save_file_mode\"])\n        if self.SAVE_FILE_MODE not in SAVE_FILE_MODES:\n            self.SAVE_FILE_MODE = DEFAULT[\"save_file_mode\"]\n        \n        self.CONFIG_UNITS: dict[str, str] = config_data.get(\"config_units\", DEFAULT[\"config_units\"])\n        for unit_class, unit in DEFAULT[\"config_units\"].items():\n            if self.CONFIG_UNITS.get(unit_class, None) is None:\n                self.CONFIG_UNITS[unit_class] = unit\n\n        self.DISPLAY_UNITS: dict[str, str] = config_data.get(\"display_units\", DEFAULT[\"display_units\"])\n        for unit_class, unit in DEFAULT[\"display_units\"].items():\n            if self.DISPLAY_UNITS.get(unit_class, None) is None:\n                self.DISPLAY_UNITS[unit_class] = unit\n        \n        file_paths: dict = config_data.get(\"file_paths\", DEFAULT[\"file_paths\"])\n        self.GAME_STATE_FILE_PATH: str = construct_path(file_paths.get(\"game_state\", DEFAULT[\"file_paths\"][\"game_state\"]))",
    "suffix": ""
  },
  {
    "name": "xIMRANx/secret_postcard:app/handlers/owner/stuff.py@924",
    "canonical_solution": "@router.message(IsOwner(is_owner=True), Command(commands=[\"send_postcard\"]))",
    "prompt": "import time\nfrom aiogram import Router\nfrom aiogram.filters import Command\nfrom aiogram.types import Message\nfrom app.filters.is_owner import IsOwner\nfrom app.db.functions import User, Card\nfrom random import choice\n\n\n\nrouter = Router()\n\n\n@router.message(IsOwner(is_owner=True), Command(commands=[\"ping\"]))\nasync def ping_handler(message: Message):\n    start = time.perf_counter_ns()\n    reply_message = await message.answer(\"<code>\u23f1 Checking ping...</code>\")\n    end = time.perf_counter_ns()\n    ping = (end - start) * 0.000001\n    await reply_message.edit_text(\n        f\"<b>\u23f1 Ping -</b> <code>{round(ping, 3)}</code> <b>ms</b>\"\n    )\n\n",
    "prefix": "import time\nfrom aiogram import Router\nfrom aiogram.filters import Command\nfrom aiogram.types import Message\nfrom app.filters.is_owner import IsOwner\nfrom app.db.functions import User, Card\nfrom random import choice\n\n\n\nrouter = Router()\n\n\n@router.message(IsOwner(is_owner=True), Command(commands=[\"ping\"]))\nasync def ping_handler(message: Message):\n    start = time.perf_counter_ns()\n    reply_message = await message.answer(\"<code>\u23f1 Checking ping...</code>\")\n    end = time.perf_counter_ns()\n    ping = (end - start) * 0.000001\n    await reply_message.edit_text(\n        f\"<b>\u23f1 Ping -</b> <code>{round(ping, 3)}</code> <b>ms</b>\"\n    )\n\n",
    "suffix": ""
  },
  {
    "name": "Asa-Nisi-Masa/christmas-tree:christmas_tree/calculations/compute_coords.py@1263",
    "canonical_solution": "        for i in range(TOTAL_LEDS):",
    "prompt": "from collections import defaultdict, namedtuple\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\nfrom tqdm import tqdm\nfrom christmas_tree.common.settings import PATH_SAVE, TOTAL_LEDS\nimport cv2\nimport numpy as np\n\n\n\n### Adjust these three parameters if lots of LEDs cannot be detected\nLOWER_THRESHOLD = 135\nUPPER_THRESHOLD = 255\nMAX_DIST = 40\n###\nANGLES = [0, 45, 90, 135, 180, 225, 270, 315]\n\nPoint = namedtuple(\"Point\", [\"x\", \"y\"])\n\n# get height and width of images from one of the frames\npath = Path(\"frames\") / str(ANGLES[0]) / \"0.jpg\"\nframe = cv2.imread(str(path))\nheight, width, _ = frame.shape\n\n\ndef _get_uv(center: Point, width: int, height: int) -> Point:\n    px, py = center\n\n    u = 2 / width * px - 1\n    v = -2 / height * py + 1\n\n    return Point(u, v)\n\n\ndef _compute_naive_positions(image: np.ndarray) -> List[Point]:\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    _, thresh = cv2.threshold(gray, LOWER_THRESHOLD, UPPER_THRESHOLD, cv2.THRESH_BINARY)\n    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    centers = []\n    for contour in contours:\n        M = cv2.moments(contour)\n        if M[\"m00\"] != 0:\n            cX = int(M[\"m10\"] / M[\"m00\"])\n            cY = int(M[\"m01\"] / M[\"m00\"])\n\n            centers.append(Point(cX, cY))\n\n    return centers\n\n\ndef _compute_correct_positions(contour_centers: List[Point]) -> Optional[Point]:\n    if len(contour_centers) == 0:\n        return None\n\n    if len(contour_centers) == 1:\n        return contour_centers[0]\n\n    min_dist = float(\"inf\")\n    for i in range(len(contour_centers)):\n        for j in range(i, len(contour_centers)):\n            if i == j:\n                continue\n\n            xi, yi = contour_centers[i]\n            xj, yj = contour_centers[j]\n\n            dist2 = (xi - xj) ** 2 + (yi - yj) ** 2\n\n            if dist2 < min_dist:\n                min_dist = dist2\n\n    if min_dist < MAX_DIST**2:\n        centers = np.array(contour_centers).mean(axis=0)\n        return Point(int(centers[0]), int(centers[1]))\n\n    return None\n\n\ndef _get_map_from_index_to_position(angle: int) -> Dict[int, Point]:\n    map_index_to_position = {}\n\n    total_errors = 0\n    for i in range(TOTAL_LEDS):\n        path = Path(\"frames\") / str(angle) / f\"{i}.jpg\"\n        frame = cv2.imread(str(path))\n        contour_centers = _compute_naive_positions(frame)\n\n        center = _compute_correct_positions(contour_centers)\n        if center is None:\n            total_errors += 1\n            map_index_to_position[i] = None\n        else:\n            map_index_to_position[i] = _get_uv(center, width, height)\n\n    return map_index_to_position\n\n\ndef get_map_index_to_angle_position() -> Dict[int, Dict[int, Point]]:\n    # map_index_to_angle_position = map from LED index to a map from angle to LED position\n    angles_to_centers = {}\n    map_index_to_angle_position = defaultdict(dict)\n\n    for angle in tqdm(ANGLES):\n        map_index_to_position = _get_map_from_index_to_position(angle)\n        angles_to_centers[angle] = map_index_to_position\n\n        for i in range(TOTAL_LEDS):\n            map_index_to_angle_position[i][angle] = map_index_to_position[i]\n\n    return map_index_to_angle_position\n\n\ndef validate_led_positions(map_index_to_angle_position: Dict[int, Dict[int, Point]]) -> None:\n    total_no_centers = 0\n    for i in range(TOTAL_LEDS):\n        num_angles_center_is_defined = sum(el is not None for el in map_index_to_angle_position[i].values())\n\n        if num_angles_center_is_defined < 1:\n            print(f\"No center can be found for {i} LED\")\n            total_no_centers += 1\n\n    print(\"Total no LED positions found:\", total_no_centers)\n\n\ndef get_frames_to_xyz(map_index_to_angle_position: Dict[int, Dict[int, Point]]) -> Dict[int, tuple]:\n    # frames_to_xyz = map from LED index to LED position\n    frames_to_xyz = {}\n    for i in range(TOTAL_LEDS):\n        sum_x = 0\n        sum_z = 0\n        sum_y = 0\n\n        non_nulls = 0\n        for angle in ANGLES:\n            radian = np.pi / 180 * angle\n            center = map_index_to_angle_position[i][angle]\n            if center is not None:\n                sum_x += center.x * np.cos(radian)\n                sum_z += center.x * np.sin(radian)\n                sum_y += center.y\n\n                non_nulls += 1\n\n        if non_nulls > 0:\n            x = 1 / non_nulls * sum_x\n            z = 1 / non_nulls * sum_z\n            y = 1 / non_nulls * sum_y\n\n            frames_to_xyz[i] = (x, y, z)\n        else:\n            frames_to_xyz[i] = None\n\n    return frames_to_xyz\n\n\ndef save_to_file(frames_to_xyz: Dict[int, tuple]):\n    with open(PATH_SAVE, \"w\") as file:",
    "prefix": "from collections import defaultdict, namedtuple\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\nfrom tqdm import tqdm\nfrom christmas_tree.common.settings import PATH_SAVE, TOTAL_LEDS\nimport cv2\nimport numpy as np\n\n\n\n### Adjust these three parameters if lots of LEDs cannot be detected\nLOWER_THRESHOLD = 135\nUPPER_THRESHOLD = 255\nMAX_DIST = 40\n###\nANGLES = [0, 45, 90, 135, 180, 225, 270, 315]\n\nPoint = namedtuple(\"Point\", [\"x\", \"y\"])\n\n# get height and width of images from one of the frames\npath = Path(\"frames\") / str(ANGLES[0]) / \"0.jpg\"\nframe = cv2.imread(str(path))\nheight, width, _ = frame.shape\n\n\ndef _get_uv(center: Point, width: int, height: int) -> Point:\n    px, py = center\n\n    u = 2 / width * px - 1\n    v = -2 / height * py + 1\n\n    return Point(u, v)\n\n\ndef _compute_naive_positions(image: np.ndarray) -> List[Point]:\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    _, thresh = cv2.threshold(gray, LOWER_THRESHOLD, UPPER_THRESHOLD, cv2.THRESH_BINARY)\n    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    centers = []\n    for contour in contours:\n        M = cv2.moments(contour)\n        if M[\"m00\"] != 0:\n            cX = int(M[\"m10\"] / M[\"m00\"])\n            cY = int(M[\"m01\"] / M[\"m00\"])\n\n            centers.append(Point(cX, cY))\n\n    return centers\n\n\ndef _compute_correct_positions(contour_centers: List[Point]) -> Optional[Point]:\n    if len(contour_centers) == 0:\n        return None\n\n    if len(contour_centers) == 1:\n        return contour_centers[0]\n\n    min_dist = float(\"inf\")\n    for i in range(len(contour_centers)):\n        for j in range(i, len(contour_centers)):\n            if i == j:\n                continue\n\n            xi, yi = contour_centers[i]\n            xj, yj = contour_centers[j]\n\n            dist2 = (xi - xj) ** 2 + (yi - yj) ** 2\n\n            if dist2 < min_dist:\n                min_dist = dist2\n\n    if min_dist < MAX_DIST**2:\n        centers = np.array(contour_centers).mean(axis=0)\n        return Point(int(centers[0]), int(centers[1]))\n\n    return None\n\n\ndef _get_map_from_index_to_position(angle: int) -> Dict[int, Point]:\n    map_index_to_position = {}\n\n    total_errors = 0\n    for i in range(TOTAL_LEDS):\n        path = Path(\"frames\") / str(angle) / f\"{i}.jpg\"\n        frame = cv2.imread(str(path))\n        contour_centers = _compute_naive_positions(frame)\n\n        center = _compute_correct_positions(contour_centers)\n        if center is None:\n            total_errors += 1\n            map_index_to_position[i] = None\n        else:\n            map_index_to_position[i] = _get_uv(center, width, height)\n\n    return map_index_to_position\n\n\ndef get_map_index_to_angle_position() -> Dict[int, Dict[int, Point]]:\n    # map_index_to_angle_position = map from LED index to a map from angle to LED position\n    angles_to_centers = {}\n    map_index_to_angle_position = defaultdict(dict)\n\n    for angle in tqdm(ANGLES):\n        map_index_to_position = _get_map_from_index_to_position(angle)\n        angles_to_centers[angle] = map_index_to_position\n\n        for i in range(TOTAL_LEDS):\n            map_index_to_angle_position[i][angle] = map_index_to_position[i]\n\n    return map_index_to_angle_position\n\n\ndef validate_led_positions(map_index_to_angle_position: Dict[int, Dict[int, Point]]) -> None:\n    total_no_centers = 0\n    for i in range(TOTAL_LEDS):\n        num_angles_center_is_defined = sum(el is not None for el in map_index_to_angle_position[i].values())\n\n        if num_angles_center_is_defined < 1:\n            print(f\"No center can be found for {i} LED\")\n            total_no_centers += 1\n\n    print(\"Total no LED positions found:\", total_no_centers)\n\n\ndef get_frames_to_xyz(map_index_to_angle_position: Dict[int, Dict[int, Point]]) -> Dict[int, tuple]:\n    # frames_to_xyz = map from LED index to LED position\n    frames_to_xyz = {}\n    for i in range(TOTAL_LEDS):\n        sum_x = 0\n        sum_z = 0\n        sum_y = 0\n\n        non_nulls = 0\n        for angle in ANGLES:\n            radian = np.pi / 180 * angle\n            center = map_index_to_angle_position[i][angle]\n            if center is not None:\n                sum_x += center.x * np.cos(radian)\n                sum_z += center.x * np.sin(radian)\n                sum_y += center.y\n\n                non_nulls += 1\n\n        if non_nulls > 0:\n            x = 1 / non_nulls * sum_x\n            z = 1 / non_nulls * sum_z\n            y = 1 / non_nulls * sum_y\n\n            frames_to_xyz[i] = (x, y, z)\n        else:\n            frames_to_xyz[i] = None\n\n    return frames_to_xyz\n\n\ndef save_to_file(frames_to_xyz: Dict[int, tuple]):\n    with open(PATH_SAVE, \"w\") as file:",
    "suffix": ""
  },
  {
    "name": "YYJeffrey/july_server:app/model/base.py@941",
    "canonical_solution": "            raise NotFound",
    "prompt": "from uuid import uuid4\nfrom sqlalchemy import Column, String, DateTime, func, orm, inspect\nfrom app.lib.exception import NotFound\nfrom app.patch.db import SQLAlchemy, BaseQuery\nfrom app.validator.forms import PaginateValidator\n# -*- coding: utf-8 -*-\n\"\"\"\n    :copyright: (c) 2023 by Jeffrey.\n    :license: Apache 2.0, see LICENSE for more details.\n\"\"\"\n\n\n\ndb = SQLAlchemy(query_class=BaseQuery)\n\n\nclass BaseModel(db.Model):\n    \"\"\"\n    \u57fa\u7840\u6a21\u578b\n    \"\"\"\n    __abstract__ = True\n\n    id = Column('id', String(32), default=lambda: uuid4().hex, primary_key=True, comment='\u4e3b\u952e\u6807\u8bc6')\n    create_time = Column('create_time', DateTime, server_default=func.now(), index=True, comment='\u521b\u5efa\u65f6\u95f4')\n    update_time = Column('update_time', DateTime, onupdate=func.now(), comment='\u66f4\u65b0\u65f6\u95f4')\n    delete_time = Column('delete_time', DateTime, comment='\u5220\u9664\u65f6\u95f4')\n\n    def __getitem__(self, key):\n        return getattr(self, key)\n\n    @orm.reconstructor\n    def init_on_load(self):\n        self._fields = ['status']\n        self._exclude = ['delete_time', 'update_time']\n\n        self._set_fields()\n        self.__set_fields()\n\n    def __set_fields(self):\n        columns = inspect(self.__class__).columns\n        all_columns = set([column.name for column in columns])\n        self._fields.extend(list(all_columns - set(self._exclude)))\n\n    def _set_fields(self):\n        \"\"\"\n        \u5b50\u7c7b\u8c03\u7528 \u9690\u85cf\u548c\u6dfb\u52a0\u5b57\u6bb5\n        \"\"\"\n        pass\n\n    def keys(self):\n        return self._fields\n\n    def hide(self, *keys):\n        for key in keys:\n            hasattr(self, key) and self._fields.remove(key)\n        return self\n\n    def append(self, *keys):\n        for key in keys:\n            hasattr(self, key) and self._fields.append(key)\n        return self\n\n    @property\n    def status(self):\n        \"\"\"\n        \u5bf9\u8c61\u72b6\u6001 \u662f\u5426\u672a\u5220\u9664\n        \"\"\"\n        return not self.delete_time\n\n    @classmethod\n    def get_or_404(cls, **kwargs):\n        rv = cls.query.filter_by(**kwargs).first()\n        if not rv:\n            raise NotFound\n        return rv\n\n    @classmethod\n    def all_or_404(cls, **kwargs):\n        rv = cls.query.filter_by(**kwargs).all()\n        if not rv:",
    "prefix": "from uuid import uuid4\nfrom sqlalchemy import Column, String, DateTime, func, orm, inspect\nfrom app.lib.exception import NotFound\nfrom app.patch.db import SQLAlchemy, BaseQuery\nfrom app.validator.forms import PaginateValidator\n# -*- coding: utf-8 -*-\n\"\"\"\n    :copyright: (c) 2023 by Jeffrey.\n    :license: Apache 2.0, see LICENSE for more details.\n\"\"\"\n\n\n\ndb = SQLAlchemy(query_class=BaseQuery)\n\n\nclass BaseModel(db.Model):\n    \"\"\"\n    \u57fa\u7840\u6a21\u578b\n    \"\"\"\n    __abstract__ = True\n\n    id = Column('id', String(32), default=lambda: uuid4().hex, primary_key=True, comment='\u4e3b\u952e\u6807\u8bc6')\n    create_time = Column('create_time', DateTime, server_default=func.now(), index=True, comment='\u521b\u5efa\u65f6\u95f4')\n    update_time = Column('update_time', DateTime, onupdate=func.now(), comment='\u66f4\u65b0\u65f6\u95f4')\n    delete_time = Column('delete_time', DateTime, comment='\u5220\u9664\u65f6\u95f4')\n\n    def __getitem__(self, key):\n        return getattr(self, key)\n\n    @orm.reconstructor\n    def init_on_load(self):\n        self._fields = ['status']\n        self._exclude = ['delete_time', 'update_time']\n\n        self._set_fields()\n        self.__set_fields()\n\n    def __set_fields(self):\n        columns = inspect(self.__class__).columns\n        all_columns = set([column.name for column in columns])\n        self._fields.extend(list(all_columns - set(self._exclude)))\n\n    def _set_fields(self):\n        \"\"\"\n        \u5b50\u7c7b\u8c03\u7528 \u9690\u85cf\u548c\u6dfb\u52a0\u5b57\u6bb5\n        \"\"\"\n        pass\n\n    def keys(self):\n        return self._fields\n\n    def hide(self, *keys):\n        for key in keys:\n            hasattr(self, key) and self._fields.remove(key)\n        return self\n\n    def append(self, *keys):\n        for key in keys:\n            hasattr(self, key) and self._fields.append(key)\n        return self\n\n    @property\n    def status(self):\n        \"\"\"\n        \u5bf9\u8c61\u72b6\u6001 \u662f\u5426\u672a\u5220\u9664\n        \"\"\"\n        return not self.delete_time\n\n    @classmethod\n    def get_or_404(cls, **kwargs):\n        rv = cls.query.filter_by(**kwargs).first()\n        if not rv:\n            raise NotFound\n        return rv\n\n    @classmethod\n    def all_or_404(cls, **kwargs):\n        rv = cls.query.filter_by(**kwargs).all()\n        if not rv:",
    "suffix": ""
  },
  {
    "name": "Hatins/DEOE:models/detection/yolox_extension/models/yolo_pafpn.py@1361",
    "canonical_solution": "        self.lateral_conv0 = BaseConv(",
    "prompt": "from typing import Dict, Optional, Tuple\n    from torch import compile as th_compile\nfrom ...yolox.models.network_blocks import BaseConv, CSPLayer, DWConv\nfrom data.utils.types import BackboneFeatures\nimport torch as th\nimport torch.nn as nn\n\"\"\"\nOriginal Yolox PAFPN code with slight modifications\n\"\"\"\n\n\ntry:\nexcept ImportError:\n    th_compile = None\n\n\n\nclass YOLOPAFPN(nn.Module):\n    \"\"\"\n    Removed the direct dependency on the backbone.\n    \"\"\"\n\n    def __init__(\n            self,\n            depth: float = 1.0,\n            in_stages: Tuple[int, ...] = (2, 3, 4),\n            in_channels: Tuple[int, ...] = (256, 512, 1024),\n            depthwise: bool = False,\n            act: str = \"silu\",\n            compile_cfg: Optional[Dict] = None,\n    ):\n        super().__init__()\n        assert len(in_stages) == len(in_channels)\n        assert len(in_channels) == 3, 'Current implementation only for 3 feature maps'\n        self.in_features = in_stages\n        self.in_channels = in_channels\n        Conv = DWConv if depthwise else BaseConv\n\n        ###### Compile if requested ######\n        if compile_cfg is not None:\n            compile_mdl = compile_cfg['enable']\n            if compile_mdl and th_compile is not None:\n                self.forward = th_compile(self.forward, **compile_cfg['args'])\n            elif compile_mdl:\n                print('Could not compile PAFPN because torch.compile is not available')\n\n        ##################################\n\n        self.upsample = lambda x: nn.functional.interpolate(x, scale_factor=2, mode='nearest-exact')",
    "prefix": "from typing import Dict, Optional, Tuple\n    from torch import compile as th_compile\nfrom ...yolox.models.network_blocks import BaseConv, CSPLayer, DWConv\nfrom data.utils.types import BackboneFeatures\nimport torch as th\nimport torch.nn as nn\n\"\"\"\nOriginal Yolox PAFPN code with slight modifications\n\"\"\"\n\n\ntry:\nexcept ImportError:\n    th_compile = None\n\n\n\nclass YOLOPAFPN(nn.Module):\n    \"\"\"\n    Removed the direct dependency on the backbone.\n    \"\"\"\n\n    def __init__(\n            self,\n            depth: float = 1.0,\n            in_stages: Tuple[int, ...] = (2, 3, 4),\n            in_channels: Tuple[int, ...] = (256, 512, 1024),\n            depthwise: bool = False,\n            act: str = \"silu\",\n            compile_cfg: Optional[Dict] = None,\n    ):\n        super().__init__()\n        assert len(in_stages) == len(in_channels)\n        assert len(in_channels) == 3, 'Current implementation only for 3 feature maps'\n        self.in_features = in_stages\n        self.in_channels = in_channels\n        Conv = DWConv if depthwise else BaseConv\n\n        ###### Compile if requested ######\n        if compile_cfg is not None:\n            compile_mdl = compile_cfg['enable']\n            if compile_mdl and th_compile is not None:\n                self.forward = th_compile(self.forward, **compile_cfg['args'])\n            elif compile_mdl:\n                print('Could not compile PAFPN because torch.compile is not available')\n\n        ##################################\n\n        self.upsample = lambda x: nn.functional.interpolate(x, scale_factor=2, mode='nearest-exact')",
    "suffix": ""
  },
  {
    "name": "yeyingdege/ctr-din-pytorch:din/model.py@966",
    "canonical_solution": "        self.cat_embeddings = EmbeddingLayer(n_cat, self.embedding_dim)",
    "prompt": "import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom .embedding import EmbeddingLayer\nfrom .fc import FCLayer\nfrom .attention import DinAttentionLayer\n\n\n\nclass DeepInterestNetwork(nn.Module):\n    def __init__(self, n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_DIM=[162,200,80,2]):\n        super(DeepInterestNetwork, self).__init__()\n        self.embedding_dim = EMBEDDING_DIM\n        self.hid_dim = HIDDEN_DIM\n\n        # embeddings\n        self.uid_embeddings = EmbeddingLayer(n_uid, self.embedding_dim)\n        self.mid_embeddings = EmbeddingLayer(n_mid, self.embedding_dim)",
    "prefix": "import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom .embedding import EmbeddingLayer\nfrom .fc import FCLayer\nfrom .attention import DinAttentionLayer\n\n\n\nclass DeepInterestNetwork(nn.Module):\n    def __init__(self, n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_DIM=[162,200,80,2]):\n        super(DeepInterestNetwork, self).__init__()\n        self.embedding_dim = EMBEDDING_DIM\n        self.hid_dim = HIDDEN_DIM\n\n        # embeddings\n        self.uid_embeddings = EmbeddingLayer(n_uid, self.embedding_dim)\n        self.mid_embeddings = EmbeddingLayer(n_mid, self.embedding_dim)",
    "suffix": ""
  },
  {
    "name": "iamlooper/VIC-TG-Bot:app/core/client/filters.py@953",
    "canonical_solution": "    cmd = cmd_check(message, Config.TRIGGER)",
    "prompt": "from pyrogram import filters as _filters\nfrom pyrogram.types import Message\nfrom app import Config\nfrom app.core.client.conversation import Conversation\n\n\n# Overall BOT filters\n\nconvo_filter = _filters.create(\n    lambda _, __, message: (message.chat.id in Conversation.CONVO_DICT.keys())\n    and (not message.reactions)\n)\n\n\ndef cmd_check(message: Message, trigger: str) -> bool:\n    start_str = message.text.split(maxsplit=1)[0]\n    cmd = start_str.replace(trigger, \"\", 1)\n    return bool(cmd in Config.CMD_DICT.keys())\n\n\ndef basic_check(message: Message):\n    if message.reactions or not message.text or not message.from_user:\n        return True\n\n\ndef users_check(filters, client, message: Message) -> bool:\n    if (\n        basic_check(message)\n        or not message.text.startswith(Config.TRIGGER)\n        or message.from_user.id not in Config.USERS\n    ):\n        return False",
    "prefix": "from pyrogram import filters as _filters\nfrom pyrogram.types import Message\nfrom app import Config\nfrom app.core.client.conversation import Conversation\n\n\n# Overall BOT filters\n\nconvo_filter = _filters.create(\n    lambda _, __, message: (message.chat.id in Conversation.CONVO_DICT.keys())\n    and (not message.reactions)\n)\n\n\ndef cmd_check(message: Message, trigger: str) -> bool:\n    start_str = message.text.split(maxsplit=1)[0]\n    cmd = start_str.replace(trigger, \"\", 1)\n    return bool(cmd in Config.CMD_DICT.keys())\n\n\ndef basic_check(message: Message):\n    if message.reactions or not message.text or not message.from_user:\n        return True\n\n\ndef users_check(filters, client, message: Message) -> bool:\n    if (\n        basic_check(message)\n        or not message.text.startswith(Config.TRIGGER)\n        or message.from_user.id not in Config.USERS\n    ):\n        return False",
    "suffix": ""
  },
  {
    "name": "Enthusiasm23/primkit:src/primkit/utils/LoggerSetup.py@689",
    "canonical_solution": "            log_level = LOG_LEVEL",
    "prompt": "import logging\nimport logging.handlers\nfrom ..config import LOG_LEVEL, LOG_FILE, LOG_FORMAT, \\\n    LOG_FILE_MODE, MAX_LOG_SIZE, BACKUP_COUNT, LOG_STREAM\n\n\ndef setup_logging(\n    level=None,\n    log_file=None,\n    format=None,\n    log_file_mode=None,\n    max_log_size=None,\n    backup_count=None,\n    stream=None\n):\n    \"\"\"\n    Configure logging for the application.\n\n    :param level: The logging level, e.g., 'DEBUG', 'INFO', 'WARNING'. Defaults to value from config.py but can be overridden by user input.\n    :param log_file: Path to the log file. If specified, logs will be written to the file. Defaults to value from config.py but can be overridden by user input.\n    :param format: The format for the logging messages. Defaults to value from config.py but can be overridden by user input.\n    :param log_file_mode: The mode for writing to the log file, e.g., 'a' for append mode. Defaults to value from config.py but can be overridden by user input.\n    :param max_log_size: The maximum size of the log file in bytes. When exceeded, the log will rotate. Defaults to value from config.py but can be overridden by user input.\n    :param backup_count: The number of backup log files to keep. Defaults to value from config.py but can be overridden by user input.\n    :param stream: Whether to output logs to the console. Defaults to value from config.py but can be overridden by user input.\n\n    The function uses the default configuration or configuration provided by the user. Logging can be directed to a file, console, or both based on parameters.\n    \"\"\"\n\n    # Use the default configuration or user-provided configuration\n    if level is not None:\n        if isinstance(level, int):\n            log_level = level\n        else:\n            log_level = getattr(logging, level.upper(), logging.INFO)\n    else:\n        if isinstance(LOG_LEVEL, int):",
    "prefix": "import logging\nimport logging.handlers\nfrom ..config import LOG_LEVEL, LOG_FILE, LOG_FORMAT, \\\n    LOG_FILE_MODE, MAX_LOG_SIZE, BACKUP_COUNT, LOG_STREAM\n\n\ndef setup_logging(\n    level=None,\n    log_file=None,\n    format=None,\n    log_file_mode=None,\n    max_log_size=None,\n    backup_count=None,\n    stream=None\n):\n    \"\"\"\n    Configure logging for the application.\n\n    :param level: The logging level, e.g., 'DEBUG', 'INFO', 'WARNING'. Defaults to value from config.py but can be overridden by user input.\n    :param log_file: Path to the log file. If specified, logs will be written to the file. Defaults to value from config.py but can be overridden by user input.\n    :param format: The format for the logging messages. Defaults to value from config.py but can be overridden by user input.\n    :param log_file_mode: The mode for writing to the log file, e.g., 'a' for append mode. Defaults to value from config.py but can be overridden by user input.\n    :param max_log_size: The maximum size of the log file in bytes. When exceeded, the log will rotate. Defaults to value from config.py but can be overridden by user input.\n    :param backup_count: The number of backup log files to keep. Defaults to value from config.py but can be overridden by user input.\n    :param stream: Whether to output logs to the console. Defaults to value from config.py but can be overridden by user input.\n\n    The function uses the default configuration or configuration provided by the user. Logging can be directed to a file, console, or both based on parameters.\n    \"\"\"\n\n    # Use the default configuration or user-provided configuration\n    if level is not None:\n        if isinstance(level, int):\n            log_level = level\n        else:\n            log_level = getattr(logging, level.upper(), logging.INFO)\n    else:\n        if isinstance(LOG_LEVEL, int):",
    "suffix": ""
  },
  {
    "name": "karthicksivakumarp/gui_read_csv:main.py@703",
    "canonical_solution": "report_gen = generate_report.generate_report()\r",
    "prompt": "from read_from_csv import read_csv_file\r\nfrom data_analysis import analyze_data\r\nfrom report_generation import generate_report\r\nfrom tkinter import Tk\r\nfrom user_interface import gui\r\n# Import necessary modules\r\n\r\n# Initialize CSV reader instance\r\nread_csv = read_csv_file.read_csv_data()\r\n\r\n# Obtain the function/method for reading multiple CSV files\r\n# Note: \"read_mult_csv_file\" is a function or method defined in the \"read_csv_file\" module\r\nmain_read_csv = read_csv.read_mult_csv_file\r\n\r\n# Initialize data analyzer instance\r\nanalyze_data = analyze_data.analyze_csv_data()\r\n\r\n# Initialize report generator instance\r",
    "prefix": "from read_from_csv import read_csv_file\r\nfrom data_analysis import analyze_data\r\nfrom report_generation import generate_report\r\nfrom tkinter import Tk\r\nfrom user_interface import gui\r\n# Import necessary modules\r\n\r\n# Initialize CSV reader instance\r\nread_csv = read_csv_file.read_csv_data()\r\n\r\n# Obtain the function/method for reading multiple CSV files\r\n# Note: \"read_mult_csv_file\" is a function or method defined in the \"read_csv_file\" module\r\nmain_read_csv = read_csv.read_mult_csv_file\r\n\r\n# Initialize data analyzer instance\r\nanalyze_data = analyze_data.analyze_csv_data()\r\n\r\n# Initialize report generator instance\r",
    "suffix": ""
  },
  {
    "name": "davidsvy/fractal_video:src/prepare_data/gdrive.py@1342",
    "canonical_solution": "    assert dataset in lut_id and dataset in lut_info",
    "prompt": "import itertools\nimport os\nimport pprint\nimport shutil\nfrom .gdrive_ids import lut_id, lut_info\nfrom ..utils.data import find_classes, find_files, split_stats\nfrom ..utils.other import run_bash\n\n\n\ndef parse_gdrive_filename(msg):\n    if msg.returncode != 0:\n        print(\n            f'Command \"{msg.args}\" has following error:\\n{msg.stdout}\\n{msg.stderr}')\n\n    filaname = msg.stderr.split('\\n')[3][4:]\n    filaname = os.path.basename(filaname)\n\n    return filaname\n\n\ndef gdrive(dataset, root):\n    assert dataset in lut_id and dataset in lut_info\n\n    ids = lut_id[dataset]\n    *_, extension = lut_info[dataset]\n\n    print(f'\\nDownloading & extracting {dataset} dataset...')\n    print('#' * 50)\n\n    for idx, id in enumerate(ids, 1):\n        dir_tgt = os.path.join(root, f'{idx}')\n        os.makedirs(dir_tgt, exist_ok=True)\n\n        msg_gdrive = run_bash(f'gdown {id}')\n        path_src = parse_gdrive_filename(msg_gdrive)\n\n        run_bash(f'unzip {path_src} -d {dir_tgt}')\n        os.remove(path_src)\n\n        n_files = len(find_files(dir=dir_tgt, ext=extension))\n        print(f'{path_src} ({idx}/{len(ids)}) -> {n_files} files')\n\n    all_files = find_files(dir=root, ext=extension)\n\n    print('#' * 50)\n    print(f'Downloaded {len(all_files)} files')\n\n    return all_files\n\n\ndef gdrive_supervised(dataset, root):\n    assert dataset in lut_id and dataset in lut_info\n    n_train, n_classes, n_val_class, ext = lut_info[dataset]\n\n    paths_src = gdrive(dataset=dataset, root=root)\n\n    print('#' * 50)\n    print(f'Splitting {dataset} dataset into train & val set...')\n    print('#' * 50)\n    dir_train = os.path.join(root, 'train')\n    dir_val = os.path.join(root, 'val')\n\n    labels, _ = find_classes(paths_src)\n    n_classes_full = labels.max() + 1\n    if n_classes is None:\n        n_classes = n_classes_full\n    else:\n        n_classes = min(n_classes, n_classes_full)\n\n    n_val = n_val_class * n_classes\n    #assert len(paths_src) >= n_train + n_val\n\n    bins_class = [[] for _ in range(n_classes_full)]\n\n    for path, label in zip(paths_src, labels):\n        bins_class[label].append(path)\n\n    bins_class = bins_class[:n_classes]\n\n    assert n_val_class <= min(len(bin) for bin in bins_class)\n\n    paths_src_sorted = []\n    for group in itertools.zip_longest(*bins_class):\n        paths_src_sorted += [path for path in group if path is not None]\n\n    paths_src_val = paths_src_sorted[:n_val]\n    paths_src_train = paths_src_sorted[n_val: n_val + n_train]\n\n    for path_src in paths_src_train:\n        path_rel = os.path.relpath(path_src, root)\n        path_tgt = os.path.join(dir_train, path_rel)\n        dir_tgt = os.path.dirname(path_tgt)\n\n        os.makedirs(dir_tgt, exist_ok=True)\n        shutil.move(path_src, path_tgt)\n\n    for path_src in paths_src_val:\n        path_rel = os.path.relpath(path_src, root)\n        path_tgt = os.path.join(dir_val, path_rel)\n        dir_tgt = os.path.dirname(path_tgt)\n\n        os.makedirs(dir_tgt, exist_ok=True)\n        shutil.move(path_src, path_tgt)\n\n    all_subdir = os.listdir(root)\n    all_subdir = [os.path.join(root, path) for path in all_subdir]\n    all_subdir = [path for path in all_subdir if os.path.isdir(path)]\n\n    for path in all_subdir:\n        if not path in [dir_train, dir_val]:\n            shutil.rmtree(path)\n\n    # Sanity check\n    stats_train = split_stats(dir=dir_train, ext=ext)\n    print('-Train stats:')\n    pprint.pprint(stats_train)\n\n    stats_val = split_stats(dir=dir_val, ext=ext)\n    print('-Val stats:')\n    pprint.pprint(stats_val)\n\n\ndef gdrive_unsupervised(dataset, root):",
    "prefix": "import itertools\nimport os\nimport pprint\nimport shutil\nfrom .gdrive_ids import lut_id, lut_info\nfrom ..utils.data import find_classes, find_files, split_stats\nfrom ..utils.other import run_bash\n\n\n\ndef parse_gdrive_filename(msg):\n    if msg.returncode != 0:\n        print(\n            f'Command \"{msg.args}\" has following error:\\n{msg.stdout}\\n{msg.stderr}')\n\n    filaname = msg.stderr.split('\\n')[3][4:]\n    filaname = os.path.basename(filaname)\n\n    return filaname\n\n\ndef gdrive(dataset, root):\n    assert dataset in lut_id and dataset in lut_info\n\n    ids = lut_id[dataset]\n    *_, extension = lut_info[dataset]\n\n    print(f'\\nDownloading & extracting {dataset} dataset...')\n    print('#' * 50)\n\n    for idx, id in enumerate(ids, 1):\n        dir_tgt = os.path.join(root, f'{idx}')\n        os.makedirs(dir_tgt, exist_ok=True)\n\n        msg_gdrive = run_bash(f'gdown {id}')\n        path_src = parse_gdrive_filename(msg_gdrive)\n\n        run_bash(f'unzip {path_src} -d {dir_tgt}')\n        os.remove(path_src)\n\n        n_files = len(find_files(dir=dir_tgt, ext=extension))\n        print(f'{path_src} ({idx}/{len(ids)}) -> {n_files} files')\n\n    all_files = find_files(dir=root, ext=extension)\n\n    print('#' * 50)\n    print(f'Downloaded {len(all_files)} files')\n\n    return all_files\n\n\ndef gdrive_supervised(dataset, root):\n    assert dataset in lut_id and dataset in lut_info\n    n_train, n_classes, n_val_class, ext = lut_info[dataset]\n\n    paths_src = gdrive(dataset=dataset, root=root)\n\n    print('#' * 50)\n    print(f'Splitting {dataset} dataset into train & val set...')\n    print('#' * 50)\n    dir_train = os.path.join(root, 'train')\n    dir_val = os.path.join(root, 'val')\n\n    labels, _ = find_classes(paths_src)\n    n_classes_full = labels.max() + 1\n    if n_classes is None:\n        n_classes = n_classes_full\n    else:\n        n_classes = min(n_classes, n_classes_full)\n\n    n_val = n_val_class * n_classes\n    #assert len(paths_src) >= n_train + n_val\n\n    bins_class = [[] for _ in range(n_classes_full)]\n\n    for path, label in zip(paths_src, labels):\n        bins_class[label].append(path)\n\n    bins_class = bins_class[:n_classes]\n\n    assert n_val_class <= min(len(bin) for bin in bins_class)\n\n    paths_src_sorted = []\n    for group in itertools.zip_longest(*bins_class):\n        paths_src_sorted += [path for path in group if path is not None]\n\n    paths_src_val = paths_src_sorted[:n_val]\n    paths_src_train = paths_src_sorted[n_val: n_val + n_train]\n\n    for path_src in paths_src_train:\n        path_rel = os.path.relpath(path_src, root)\n        path_tgt = os.path.join(dir_train, path_rel)\n        dir_tgt = os.path.dirname(path_tgt)\n\n        os.makedirs(dir_tgt, exist_ok=True)\n        shutil.move(path_src, path_tgt)\n\n    for path_src in paths_src_val:\n        path_rel = os.path.relpath(path_src, root)\n        path_tgt = os.path.join(dir_val, path_rel)\n        dir_tgt = os.path.dirname(path_tgt)\n\n        os.makedirs(dir_tgt, exist_ok=True)\n        shutil.move(path_src, path_tgt)\n\n    all_subdir = os.listdir(root)\n    all_subdir = [os.path.join(root, path) for path in all_subdir]\n    all_subdir = [path for path in all_subdir if os.path.isdir(path)]\n\n    for path in all_subdir:\n        if not path in [dir_train, dir_val]:\n            shutil.rmtree(path)\n\n    # Sanity check\n    stats_train = split_stats(dir=dir_train, ext=ext)\n    print('-Train stats:')\n    pprint.pprint(stats_train)\n\n    stats_val = split_stats(dir=dir_val, ext=ext)\n    print('-Val stats:')\n    pprint.pprint(stats_val)\n\n\ndef gdrive_unsupervised(dataset, root):",
    "suffix": ""
  },
  {
    "name": "OpenBrickProtocolFoundation/client:main.py@816",
    "canonical_solution": "                        tetrion.enqueue_event(Event(key=Key.DROP, type=EventType.PRESSED, frame=frame))",
    "prompt": "import pygame\nfrom tetrion import Event\nfrom tetrion import EventType\nfrom tetrion import Key\nfrom tetrion import Tetrion\n\n\n\ndef main() -> None:\n    frame = 0\n\n    with Tetrion() as tetrion:\n        pygame.init()\n\n        RECT_SIZE = 30\n        size = (RECT_SIZE * tetrion.width, (RECT_SIZE + 2) * tetrion.height)\n        screen = pygame.display.set_mode(size)\n\n        COLORS = [(0, 0, 0),\n                  (0, 240, 240),\n                  (0, 0, 240),\n                  (240, 160, 0),\n                  (240, 240, 0),\n                  (0, 240, 0),\n                  (160, 0, 240),\n                  (240, 0, 0)]\n\n        done = False\n\n        clock = pygame.time.Clock()\n\n        while not done:\n            for event in pygame.event.get():\n                if event.type == pygame.QUIT:\n                    done = True\n                elif event.type == pygame.KEYDOWN:\n                    if event.key == pygame.K_ESCAPE:\n                        done = True\n                    elif event.key == pygame.K_a:\n                        tetrion.enqueue_event(Event(key=Key.LEFT, type=EventType.PRESSED, frame=frame))\n                    elif event.key == pygame.K_d:\n                        tetrion.enqueue_event(Event(key=Key.RIGHT, type=EventType.PRESSED, frame=frame))\n                    elif event.key == pygame.K_SPACE:",
    "prefix": "import pygame\nfrom tetrion import Event\nfrom tetrion import EventType\nfrom tetrion import Key\nfrom tetrion import Tetrion\n\n\n\ndef main() -> None:\n    frame = 0\n\n    with Tetrion() as tetrion:\n        pygame.init()\n\n        RECT_SIZE = 30\n        size = (RECT_SIZE * tetrion.width, (RECT_SIZE + 2) * tetrion.height)\n        screen = pygame.display.set_mode(size)\n\n        COLORS = [(0, 0, 0),\n                  (0, 240, 240),\n                  (0, 0, 240),\n                  (240, 160, 0),\n                  (240, 240, 0),\n                  (0, 240, 0),\n                  (160, 0, 240),\n                  (240, 0, 0)]\n\n        done = False\n\n        clock = pygame.time.Clock()\n\n        while not done:\n            for event in pygame.event.get():\n                if event.type == pygame.QUIT:\n                    done = True\n                elif event.type == pygame.KEYDOWN:\n                    if event.key == pygame.K_ESCAPE:\n                        done = True\n                    elif event.key == pygame.K_a:\n                        tetrion.enqueue_event(Event(key=Key.LEFT, type=EventType.PRESSED, frame=frame))\n                    elif event.key == pygame.K_d:\n                        tetrion.enqueue_event(Event(key=Key.RIGHT, type=EventType.PRESSED, frame=frame))\n                    elif event.key == pygame.K_SPACE:",
    "suffix": ""
  },
  {
    "name": "camenduru/MotionCtrl-hf:lvdm/modules/attention.py@1504",
    "canonical_solution": "            sim.masked_fill_(~(mask>0.5), max_neg_value)",
    "prompt": "import math\nimport torch\nimport torch.nn.functional as F\n    import xformers\n    import xformers.ops\nfrom functools import partial\nfrom inspect import isfunction\nfrom einops import rearrange, repeat\nfrom torch import einsum, nn\nfrom lvdm.basics import conv_nd, normalization, zero_module\nfrom lvdm.common import checkpoint, default, exists, init_, max_neg_value, uniq\n\n\ntry:\n    XFORMERS_IS_AVAILBLE = True\nexcept:\n    XFORMERS_IS_AVAILBLE = False\n\n\nclass RelativePosition(nn.Module):\n    \"\"\" https://github.com/evelinehong/Transformer_Relative_Position_PyTorch/blob/master/relative_position.py \"\"\"\n\n    def __init__(self, num_units, max_relative_position):\n        super().__init__()\n        self.num_units = num_units\n        self.max_relative_position = max_relative_position\n        self.embeddings_table = nn.Parameter(torch.Tensor(max_relative_position * 2 + 1, num_units))\n        nn.init.xavier_uniform_(self.embeddings_table)\n\n    def forward(self, length_q, length_k):\n        device = self.embeddings_table.device\n        range_vec_q = torch.arange(length_q, device=device)\n        range_vec_k = torch.arange(length_k, device=device)\n        distance_mat = range_vec_k[None, :] - range_vec_q[:, None]\n        distance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)\n        final_mat = distance_mat_clipped + self.max_relative_position\n        # final_mat = th.LongTensor(final_mat).to(self.embeddings_table.device)\n        # final_mat = th.tensor(final_mat, device=self.embeddings_table.device, dtype=torch.long)\n        final_mat = final_mat.long()\n        embeddings = self.embeddings_table[final_mat]\n        return embeddings\n\n\nclass CrossAttention(nn.Module):\n\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0., \n                 relative_position=False, temporal_length=None):\n        super().__init__()\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, query_dim)\n\n        self.scale = dim_head**-0.5\n        self.heads = heads\n        self.dim_head = dim_head\n        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n\n        self.to_out = nn.Sequential(nn.Linear(inner_dim, query_dim), nn.Dropout(dropout))\n        \n        self.relative_position = relative_position\n        if self.relative_position:\n            assert(temporal_length is not None)\n            self.relative_position_k = RelativePosition(num_units=dim_head, max_relative_position=temporal_length)\n            self.relative_position_v = RelativePosition(num_units=dim_head, max_relative_position=temporal_length)\n        else:\n            ## only used for spatial attention, while NOT for temporal attention\n            if XFORMERS_IS_AVAILBLE and temporal_length is None:\n                self.forward = self.efficient_forward\n\n    def forward(self, x, context=None, mask=None):\n        h = self.heads\n\n        q = self.to_q(x)\n        context = default(context, x)\n        k = self.to_k(context)\n        v = self.to_v(context)\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n        sim = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale\n        if self.relative_position:\n            len_q, len_k, len_v = q.shape[1], k.shape[1], v.shape[1]\n            k2 = self.relative_position_k(len_q, len_k)\n            sim2 = einsum('b t d, t s d -> b t s', q, k2) * self.scale # TODO check \n            sim += sim2\n        del q, k\n\n        if exists(mask):\n            ## feasible for causal attention mask only\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = repeat(mask, 'b i j -> (b h) i j', h=h)",
    "prefix": "import math\nimport torch\nimport torch.nn.functional as F\n    import xformers\n    import xformers.ops\nfrom functools import partial\nfrom inspect import isfunction\nfrom einops import rearrange, repeat\nfrom torch import einsum, nn\nfrom lvdm.basics import conv_nd, normalization, zero_module\nfrom lvdm.common import checkpoint, default, exists, init_, max_neg_value, uniq\n\n\ntry:\n    XFORMERS_IS_AVAILBLE = True\nexcept:\n    XFORMERS_IS_AVAILBLE = False\n\n\nclass RelativePosition(nn.Module):\n    \"\"\" https://github.com/evelinehong/Transformer_Relative_Position_PyTorch/blob/master/relative_position.py \"\"\"\n\n    def __init__(self, num_units, max_relative_position):\n        super().__init__()\n        self.num_units = num_units\n        self.max_relative_position = max_relative_position\n        self.embeddings_table = nn.Parameter(torch.Tensor(max_relative_position * 2 + 1, num_units))\n        nn.init.xavier_uniform_(self.embeddings_table)\n\n    def forward(self, length_q, length_k):\n        device = self.embeddings_table.device\n        range_vec_q = torch.arange(length_q, device=device)\n        range_vec_k = torch.arange(length_k, device=device)\n        distance_mat = range_vec_k[None, :] - range_vec_q[:, None]\n        distance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)\n        final_mat = distance_mat_clipped + self.max_relative_position\n        # final_mat = th.LongTensor(final_mat).to(self.embeddings_table.device)\n        # final_mat = th.tensor(final_mat, device=self.embeddings_table.device, dtype=torch.long)\n        final_mat = final_mat.long()\n        embeddings = self.embeddings_table[final_mat]\n        return embeddings\n\n\nclass CrossAttention(nn.Module):\n\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0., \n                 relative_position=False, temporal_length=None):\n        super().__init__()\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, query_dim)\n\n        self.scale = dim_head**-0.5\n        self.heads = heads\n        self.dim_head = dim_head\n        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n\n        self.to_out = nn.Sequential(nn.Linear(inner_dim, query_dim), nn.Dropout(dropout))\n        \n        self.relative_position = relative_position\n        if self.relative_position:\n            assert(temporal_length is not None)\n            self.relative_position_k = RelativePosition(num_units=dim_head, max_relative_position=temporal_length)\n            self.relative_position_v = RelativePosition(num_units=dim_head, max_relative_position=temporal_length)\n        else:\n            ## only used for spatial attention, while NOT for temporal attention\n            if XFORMERS_IS_AVAILBLE and temporal_length is None:\n                self.forward = self.efficient_forward\n\n    def forward(self, x, context=None, mask=None):\n        h = self.heads\n\n        q = self.to_q(x)\n        context = default(context, x)\n        k = self.to_k(context)\n        v = self.to_v(context)\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n        sim = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale\n        if self.relative_position:\n            len_q, len_k, len_v = q.shape[1], k.shape[1], v.shape[1]\n            k2 = self.relative_position_k(len_q, len_k)\n            sim2 = einsum('b t d, t s d -> b t s', q, k2) * self.scale # TODO check \n            sim += sim2\n        del q, k\n\n        if exists(mask):\n            ## feasible for causal attention mask only\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = repeat(mask, 'b i j -> (b h) i j', h=h)",
    "suffix": ""
  },
  {
    "name": "facebookresearch/ca_body:ca_body/nn/shadow.py@1189",
    "canonical_solution": "        self.shadow_pred.apply(weights_initializer(1.0))",
    "prompt": "import logging\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport ca_body.nn.layers as la\nfrom typing import Optional, Dict\nfrom ca_body.nn.blocks import tile2d, weights_initializer\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# \n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n\n\n# TODO: use shared utils here?\n\nlogger = logging.getLogger(__name__)\n\n\nclass ShadowUNet(nn.Module):\n    def __init__(\n        self,\n        uv_size,\n        ao_mean,\n        shadow_size,\n        lrelu_slope=0.2,\n        beta=1.0,\n        n_dims=64,\n        interp_mode=\"bilinear\",\n        biases=True,\n        trainable_mean=False,\n    ):\n        super().__init__()\n\n        # this is the size of the output\n        self.uv_size = uv_size\n        self.shadow_size = shadow_size\n\n        ao_mean = F.interpolate(\n            th.as_tensor(ao_mean)[np.newaxis],\n            size=(self.shadow_size, self.shadow_size),\n        )[0]\n        if not trainable_mean:\n            # TODO:\n            self.register_buffer(\"ao_mean\", ao_mean)\n        else:\n            self.register_parameter(\"ao_mean\", th.nn.Parameter(ao_mean))\n\n        self.depth = 3\n        self.lrelu_slope = lrelu_slope\n        self.interp_mode = interp_mode\n        self.align_corners = None\n        if interp_mode == \"bilinear\":\n            self.align_corners = False\n\n        # the base number of dimensions for the shadow maps\n        n_dims = n_dims\n\n        # TODO: generate this?\n        self.n_enc_dims = [\n            (1, n_dims),\n            (n_dims, n_dims),\n            (n_dims, n_dims),\n            (n_dims, n_dims),\n        ]\n\n        self.sizes = [shadow_size // (2**i) for i in range(len(self.n_enc_dims))]\n\n        logger.debug(f\"sizes: {self.sizes}\")\n\n        self.enc_layers = nn.ModuleList()\n        for i, size in enumerate(self.sizes):\n            n_in, n_out = self.n_enc_dims[i]\n            logger.debug(f\"EncoderLayers({i}): {n_in}, {n_out}, {size}\")\n            self.enc_layers.append(\n                nn.Sequential(\n                    la.Conv2dWNUB(\n                        n_in,\n                        n_out,\n                        kernel_size=3,\n                        height=size,\n                        width=size,\n                        stride=1,\n                        padding=1,\n                    ),\n                    nn.LeakyReLU(self.lrelu_slope, inplace=True),\n                )\n            )\n\n        self.n_dec_dims = [\n            (n_dims, n_dims),\n            (n_dims * 2, n_dims),\n            (n_dims * 2, n_dims),\n            (n_dims * 2, n_dims),\n        ]\n        self.dec_layers = nn.ModuleList()\n        for i in range(len(self.sizes)):\n            size = self.sizes[-i - 1]\n            n_in, n_out = self.n_dec_dims[i]\n            logger.debug(f\"DecoderLayer({i}): {n_in}, {n_out}, {size}\")\n\n            self.dec_layers.append(\n                nn.Sequential(\n                    la.Conv2dWNUB(\n                        n_in,\n                        n_out,\n                        kernel_size=3,\n                        height=size,\n                        width=size,\n                        stride=1,\n                        padding=1,\n                    ),\n                    nn.LeakyReLU(self.lrelu_slope, inplace=True),\n                )\n            )\n\n        self.apply(weights_initializer(self.lrelu_slope))\n\n        if biases:\n            self.shadow_pred = la.Conv2dWNUB(\n                self.n_dec_dims[-1][-1],\n                1,\n                kernel_size=3,\n                height=self.sizes[0],\n                width=self.sizes[0],\n                stride=1,\n                padding=1,\n            )\n        else:\n            self.shadow_pred = la.Conv2dWN(\n                self.n_dec_dims[-1][-1],\n                1,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n            )\n",
    "prefix": "import logging\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport ca_body.nn.layers as la\nfrom typing import Optional, Dict\nfrom ca_body.nn.blocks import tile2d, weights_initializer\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# \n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n\n\n# TODO: use shared utils here?\n\nlogger = logging.getLogger(__name__)\n\n\nclass ShadowUNet(nn.Module):\n    def __init__(\n        self,\n        uv_size,\n        ao_mean,\n        shadow_size,\n        lrelu_slope=0.2,\n        beta=1.0,\n        n_dims=64,\n        interp_mode=\"bilinear\",\n        biases=True,\n        trainable_mean=False,\n    ):\n        super().__init__()\n\n        # this is the size of the output\n        self.uv_size = uv_size\n        self.shadow_size = shadow_size\n\n        ao_mean = F.interpolate(\n            th.as_tensor(ao_mean)[np.newaxis],\n            size=(self.shadow_size, self.shadow_size),\n        )[0]\n        if not trainable_mean:\n            # TODO:\n            self.register_buffer(\"ao_mean\", ao_mean)\n        else:\n            self.register_parameter(\"ao_mean\", th.nn.Parameter(ao_mean))\n\n        self.depth = 3\n        self.lrelu_slope = lrelu_slope\n        self.interp_mode = interp_mode\n        self.align_corners = None\n        if interp_mode == \"bilinear\":\n            self.align_corners = False\n\n        # the base number of dimensions for the shadow maps\n        n_dims = n_dims\n\n        # TODO: generate this?\n        self.n_enc_dims = [\n            (1, n_dims),\n            (n_dims, n_dims),\n            (n_dims, n_dims),\n            (n_dims, n_dims),\n        ]\n\n        self.sizes = [shadow_size // (2**i) for i in range(len(self.n_enc_dims))]\n\n        logger.debug(f\"sizes: {self.sizes}\")\n\n        self.enc_layers = nn.ModuleList()\n        for i, size in enumerate(self.sizes):\n            n_in, n_out = self.n_enc_dims[i]\n            logger.debug(f\"EncoderLayers({i}): {n_in}, {n_out}, {size}\")\n            self.enc_layers.append(\n                nn.Sequential(\n                    la.Conv2dWNUB(\n                        n_in,\n                        n_out,\n                        kernel_size=3,\n                        height=size,\n                        width=size,\n                        stride=1,\n                        padding=1,\n                    ),\n                    nn.LeakyReLU(self.lrelu_slope, inplace=True),\n                )\n            )\n\n        self.n_dec_dims = [\n            (n_dims, n_dims),\n            (n_dims * 2, n_dims),\n            (n_dims * 2, n_dims),\n            (n_dims * 2, n_dims),\n        ]\n        self.dec_layers = nn.ModuleList()\n        for i in range(len(self.sizes)):\n            size = self.sizes[-i - 1]\n            n_in, n_out = self.n_dec_dims[i]\n            logger.debug(f\"DecoderLayer({i}): {n_in}, {n_out}, {size}\")\n\n            self.dec_layers.append(\n                nn.Sequential(\n                    la.Conv2dWNUB(\n                        n_in,\n                        n_out,\n                        kernel_size=3,\n                        height=size,\n                        width=size,\n                        stride=1,\n                        padding=1,\n                    ),\n                    nn.LeakyReLU(self.lrelu_slope, inplace=True),\n                )\n            )\n\n        self.apply(weights_initializer(self.lrelu_slope))\n\n        if biases:\n            self.shadow_pred = la.Conv2dWNUB(\n                self.n_dec_dims[-1][-1],\n                1,\n                kernel_size=3,\n                height=self.sizes[0],\n                width=self.sizes[0],\n                stride=1,\n                padding=1,\n            )\n        else:\n            self.shadow_pred = la.Conv2dWN(\n                self.n_dec_dims[-1][-1],\n                1,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n            )\n",
    "suffix": ""
  },
  {
    "name": "zkarpinski/codeinsight-sdk-python:codeinsight_sdk/handlers.py@1000",
    "canonical_solution": "    def all(self) -> List[Project]:",
    "prompt": "import abc\nfrom typing import List\nfrom codeinsight_sdk.models import Project, ProjectInventory, ProjectInventoryItem, Report\nfrom codeinsight_sdk.exceptions import CodeInsightError\n\n\nclass Handler(abc.ABC):\n    def __init__(self, client):\n        self.client = client\n        self.cls = None\n    \n    @staticmethod\n    def create(client, cls):\n        k = cls.__name__\n        handlers = {\"Project\": ProjectHandler,\n                    \"Report\": ReportHandler\n            }\n        handler = handlers.get(k)\n        if handler is None:\n            raise ValueError(f\"Handler not found for class '{k}'\")\n        return handler(client)\n    \n    @abc.abstractmethod\n    def get(self):\n        pass\n\nclass ProjectHandler(Handler):\n    def __init__(self, client):\n        super().__init__(client)\n        self.cls = Project\n\n    def create(self, name:str, description:str = None, folder:str = None,\n               scanProfileName:str = None,\n               owner:str = None,\n               risk:str = None,\n               folderId:int = None,\n               customFields:List[dict] = None,\n               ) -> int:\n        \"\"\"\n        Creates a project.\n\n        Args:\n            name (str): The name of the project.\n            description (str, optional): The description of the project. Defaults to None.\n            folder (str, optional): The folder of the project. Defaults to None.\n\n        Returns:\n            Project: The created project id.\n        \"\"\"\n        path = \"projects\"\n        data = {\"name\": name,\n                \"description\": description,\n                \"folderName\": folder,\n                \"scanProfileName\": scanProfileName,\n                \"owner\": owner,\n                \"risk\": risk,\n                \"folderId\": folderId,\n                \"customFields\": customFields}\n        resp = self.client.request(\"POST\", url_part=path, body=data)\n        try:\n            project_id = resp.json()['data']['id']\n        except KeyError:\n            raise CodeInsightError(resp)\n        return project_id\n\n\n    #Note API endpoints switch between projects and project...",
    "prefix": "import abc\nfrom typing import List\nfrom codeinsight_sdk.models import Project, ProjectInventory, ProjectInventoryItem, Report\nfrom codeinsight_sdk.exceptions import CodeInsightError\n\n\nclass Handler(abc.ABC):\n    def __init__(self, client):\n        self.client = client\n        self.cls = None\n    \n    @staticmethod\n    def create(client, cls):\n        k = cls.__name__\n        handlers = {\"Project\": ProjectHandler,\n                    \"Report\": ReportHandler\n            }\n        handler = handlers.get(k)\n        if handler is None:\n            raise ValueError(f\"Handler not found for class '{k}'\")\n        return handler(client)\n    \n    @abc.abstractmethod\n    def get(self):\n        pass\n\nclass ProjectHandler(Handler):\n    def __init__(self, client):\n        super().__init__(client)\n        self.cls = Project\n\n    def create(self, name:str, description:str = None, folder:str = None,\n               scanProfileName:str = None,\n               owner:str = None,\n               risk:str = None,\n               folderId:int = None,\n               customFields:List[dict] = None,\n               ) -> int:\n        \"\"\"\n        Creates a project.\n\n        Args:\n            name (str): The name of the project.\n            description (str, optional): The description of the project. Defaults to None.\n            folder (str, optional): The folder of the project. Defaults to None.\n\n        Returns:\n            Project: The created project id.\n        \"\"\"\n        path = \"projects\"\n        data = {\"name\": name,\n                \"description\": description,\n                \"folderName\": folder,\n                \"scanProfileName\": scanProfileName,\n                \"owner\": owner,\n                \"risk\": risk,\n                \"folderId\": folderId,\n                \"customFields\": customFields}\n        resp = self.client.request(\"POST\", url_part=path, body=data)\n        try:\n            project_id = resp.json()['data']['id']\n        except KeyError:\n            raise CodeInsightError(resp)\n        return project_id\n\n\n    #Note API endpoints switch between projects and project...",
    "suffix": ""
  },
  {
    "name": "chebupelka8/Engine:scripts/loop.py@1074",
    "canonical_solution": "    def window_size(self) -> Vec2:\r",
    "prompt": "import pygame, sys\r\nfrom pygame.locals import *\r\nfrom .math import Vec2\r\nfrom .image import Image\r\n\r\n\r\nclass WindowLoop:\r\n\r\n    def __init__(self, __size: Vec2, fps: int = 144) -> None:\r\n        pygame.init()\r\n\r\n        self.__display = pygame.display.set_mode((__size.x, __size.y))\r\n        pygame.display.set_caption(\"Engine: v0.1\")\r\n        pygame.display.set_icon(Image(\"Engine/assets/icon.png\").image)\r\n        \r\n        self.__clock = pygame.time.Clock()\r\n        self.__fps = fps\r\n    \r\n    def set_window_title(self, __title: str) -> None:\r\n        pygame.display.set_caption(__title)\r\n    \r\n    def set_window_icon(self, __icon: Image) -> None:\r\n        pygame.display.set_icon(__icon.image)\r\n    \r\n    @property\r\n    def display(self) -> pygame.Surface:\r\n        return self.__display\r\n    \r\n    @property\r\n    def fps(self) -> float:\r\n        return self.__clock.get_fps()\r\n    \r\n    @property\r",
    "prefix": "import pygame, sys\r\nfrom pygame.locals import *\r\nfrom .math import Vec2\r\nfrom .image import Image\r\n\r\n\r\nclass WindowLoop:\r\n\r\n    def __init__(self, __size: Vec2, fps: int = 144) -> None:\r\n        pygame.init()\r\n\r\n        self.__display = pygame.display.set_mode((__size.x, __size.y))\r\n        pygame.display.set_caption(\"Engine: v0.1\")\r\n        pygame.display.set_icon(Image(\"Engine/assets/icon.png\").image)\r\n        \r\n        self.__clock = pygame.time.Clock()\r\n        self.__fps = fps\r\n    \r\n    def set_window_title(self, __title: str) -> None:\r\n        pygame.display.set_caption(__title)\r\n    \r\n    def set_window_icon(self, __icon: Image) -> None:\r\n        pygame.display.set_icon(__icon.image)\r\n    \r\n    @property\r\n    def display(self) -> pygame.Surface:\r\n        return self.__display\r\n    \r\n    @property\r\n    def fps(self) -> float:\r\n        return self.__clock.get_fps()\r\n    \r\n    @property\r",
    "suffix": ""
  },
  {
    "name": "lxbme/TSPLifesaver:test/test_optimizer.py@742",
    "canonical_solution": "        self.route = BasicRoute(self.points, name=\"test route\")",
    "prompt": "import unittest\nfrom TSPLifesaver.optimizer import *\nfrom TSPLifesaver.structure import BasicRoute, PointWithEuclideanDistance\n\n\n\nclass TestStructure(unittest.TestCase):\n\n    def test_SimulatedAnnealing(self):\n        self.points = [PointWithEuclideanDistance(i) for i in\n                       [[565, 575], [25, 185], [345, 750], [945, 685], [845, 655], [880, 660],\n                        [25, 230], [525, 1000], [580, 1175], [650, 1130], [1605, 620], [1220, 580], [1465, 200],\n                        [1530, 5],\n                        [845, 680], [725, 370]]]\n        self.route = BasicRoute(self.points, name=\"test route\")\n        optimizer = SimulatedAnnealing(self.route, 10000, 0.003, 1)\n        self.route = optimizer.optimize()\n        print(\"\\n\")\n        print(self.route.distance())\n\n    def test_ExhaustiveIndexing(self):\n        self.points = [PointWithEuclideanDistance(i) for i in\n                       [[565, 575], [25, 185], [345, 750], [945, 685], [845, 655], [880, 660],\n                        [25, 230], [525, 1000]]]",
    "prefix": "import unittest\nfrom TSPLifesaver.optimizer import *\nfrom TSPLifesaver.structure import BasicRoute, PointWithEuclideanDistance\n\n\n\nclass TestStructure(unittest.TestCase):\n\n    def test_SimulatedAnnealing(self):\n        self.points = [PointWithEuclideanDistance(i) for i in\n                       [[565, 575], [25, 185], [345, 750], [945, 685], [845, 655], [880, 660],\n                        [25, 230], [525, 1000], [580, 1175], [650, 1130], [1605, 620], [1220, 580], [1465, 200],\n                        [1530, 5],\n                        [845, 680], [725, 370]]]\n        self.route = BasicRoute(self.points, name=\"test route\")\n        optimizer = SimulatedAnnealing(self.route, 10000, 0.003, 1)\n        self.route = optimizer.optimize()\n        print(\"\\n\")\n        print(self.route.distance())\n\n    def test_ExhaustiveIndexing(self):\n        self.points = [PointWithEuclideanDistance(i) for i in\n                       [[565, 575], [25, 185], [345, 750], [945, 685], [845, 655], [880, 660],\n                        [25, 230], [525, 1000]]]",
    "suffix": ""
  },
  {
    "name": "open-mmlab/Amphion:models/base/base_inference.py@1271",
    "canonical_solution": "        self.cfg.vocoder = load_config(self.vocoder_cfg, lowercase=True)",
    "prompt": "import argparse\nimport os\nimport re\nimport time\nimport torch\nfrom pathlib import Path\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom models.vocoders.vocoder_inference import synthesis\nfrom torch.utils.data import DataLoader\nfrom utils.util import set_all_random_seed\nfrom utils.util import load_config\n# Copyright (c) 2023 Amphion.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n\n\n\ndef parse_vocoder(vocoder_dir):\n    r\"\"\"Parse vocoder config\"\"\"\n    vocoder_dir = os.path.abspath(vocoder_dir)\n    ckpt_list = [ckpt for ckpt in Path(vocoder_dir).glob(\"*.pt\")]\n    ckpt_list.sort(key=lambda x: int(x.stem), reverse=True)\n    ckpt_path = str(ckpt_list[0])\n    vocoder_cfg = load_config(os.path.join(vocoder_dir, \"args.json\"), lowercase=True)\n    vocoder_cfg.model.bigvgan = vocoder_cfg.vocoder\n    return vocoder_cfg, ckpt_path\n\n\nclass BaseInference(object):\n    def __init__(self, cfg, args):\n        self.cfg = cfg\n        self.args = args\n        self.model_type = cfg.model_type\n        self.avg_rtf = list()\n        set_all_random_seed(10086)\n        os.makedirs(args.output_dir, exist_ok=True)\n\n        if torch.cuda.is_available():\n            self.device = torch.device(\"cuda\")\n        else:\n            self.device = torch.device(\"cpu\")\n            torch.set_num_threads(10)  # inference on 1 core cpu.\n\n        # Load acoustic model\n        self.model = self.create_model().to(self.device)\n        state_dict = self.load_state_dict()\n        self.load_model(state_dict)\n        self.model.eval()\n\n        # Load vocoder model if necessary\n        if self.args.checkpoint_dir_vocoder is not None:\n            self.get_vocoder_info()\n\n    def create_model(self):\n        raise NotImplementedError\n\n    def load_state_dict(self):\n        self.checkpoint_file = self.args.checkpoint_file\n        if self.checkpoint_file is None:\n            assert self.args.checkpoint_dir is not None\n            checkpoint_path = os.path.join(self.args.checkpoint_dir, \"checkpoint\")\n            checkpoint_filename = open(checkpoint_path).readlines()[-1].strip()\n            self.checkpoint_file = os.path.join(\n                self.args.checkpoint_dir, checkpoint_filename\n            )\n\n        self.checkpoint_dir = os.path.split(self.checkpoint_file)[0]\n\n        print(\"Restore acoustic model from {}\".format(self.checkpoint_file))\n        raw_state_dict = torch.load(self.checkpoint_file, map_location=self.device)\n        self.am_restore_step = re.findall(r\"step-(.+?)_loss\", self.checkpoint_file)[0]\n\n        return raw_state_dict\n\n    def load_model(self, model):\n        raise NotImplementedError\n\n    def get_vocoder_info(self):\n        self.checkpoint_dir_vocoder = self.args.checkpoint_dir_vocoder\n        self.vocoder_cfg = os.path.join(\n            os.path.dirname(self.checkpoint_dir_vocoder), \"args.json\"\n        )",
    "prefix": "import argparse\nimport os\nimport re\nimport time\nimport torch\nfrom pathlib import Path\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom models.vocoders.vocoder_inference import synthesis\nfrom torch.utils.data import DataLoader\nfrom utils.util import set_all_random_seed\nfrom utils.util import load_config\n# Copyright (c) 2023 Amphion.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n\n\n\ndef parse_vocoder(vocoder_dir):\n    r\"\"\"Parse vocoder config\"\"\"\n    vocoder_dir = os.path.abspath(vocoder_dir)\n    ckpt_list = [ckpt for ckpt in Path(vocoder_dir).glob(\"*.pt\")]\n    ckpt_list.sort(key=lambda x: int(x.stem), reverse=True)\n    ckpt_path = str(ckpt_list[0])\n    vocoder_cfg = load_config(os.path.join(vocoder_dir, \"args.json\"), lowercase=True)\n    vocoder_cfg.model.bigvgan = vocoder_cfg.vocoder\n    return vocoder_cfg, ckpt_path\n\n\nclass BaseInference(object):\n    def __init__(self, cfg, args):\n        self.cfg = cfg\n        self.args = args\n        self.model_type = cfg.model_type\n        self.avg_rtf = list()\n        set_all_random_seed(10086)\n        os.makedirs(args.output_dir, exist_ok=True)\n\n        if torch.cuda.is_available():\n            self.device = torch.device(\"cuda\")\n        else:\n            self.device = torch.device(\"cpu\")\n            torch.set_num_threads(10)  # inference on 1 core cpu.\n\n        # Load acoustic model\n        self.model = self.create_model().to(self.device)\n        state_dict = self.load_state_dict()\n        self.load_model(state_dict)\n        self.model.eval()\n\n        # Load vocoder model if necessary\n        if self.args.checkpoint_dir_vocoder is not None:\n            self.get_vocoder_info()\n\n    def create_model(self):\n        raise NotImplementedError\n\n    def load_state_dict(self):\n        self.checkpoint_file = self.args.checkpoint_file\n        if self.checkpoint_file is None:\n            assert self.args.checkpoint_dir is not None\n            checkpoint_path = os.path.join(self.args.checkpoint_dir, \"checkpoint\")\n            checkpoint_filename = open(checkpoint_path).readlines()[-1].strip()\n            self.checkpoint_file = os.path.join(\n                self.args.checkpoint_dir, checkpoint_filename\n            )\n\n        self.checkpoint_dir = os.path.split(self.checkpoint_file)[0]\n\n        print(\"Restore acoustic model from {}\".format(self.checkpoint_file))\n        raw_state_dict = torch.load(self.checkpoint_file, map_location=self.device)\n        self.am_restore_step = re.findall(r\"step-(.+?)_loss\", self.checkpoint_file)[0]\n\n        return raw_state_dict\n\n    def load_model(self, model):\n        raise NotImplementedError\n\n    def get_vocoder_info(self):\n        self.checkpoint_dir_vocoder = self.args.checkpoint_dir_vocoder\n        self.vocoder_cfg = os.path.join(\n            os.path.dirname(self.checkpoint_dir_vocoder), \"args.json\"\n        )",
    "suffix": ""
  },
  {
    "name": "KwaiKEG/KwaiAgents:kwaiagents/tools/browser.py@1318",
    "canonical_solution": "def browse_website(url: str, question: str, cfg: Config = None) -> tuple[str, WebDriver]:",
    "prompt": "import logging\nimport kwaiagents.utils.nlp_utils as summary\nfrom pathlib import Path\nfrom bs4 import BeautifulSoup\nfrom selenium.webdriver.remote.webdriver import WebDriver\nfrom kwaiagents.utils.html_utils import extract_hyperlinks, format_hyperlinks\nfrom kwaiagents.config import Config\nfrom kwaiagents.tools.base import BaseTool, BaseResult\nfrom kwaiagents.utils.selenium_utils import get_pagesource_with_selenium\nfrom __future__ import annotations\n\n\n\nFILE_DIR = Path(__file__).parent.parent\n\n\nclass BrowseResult(BaseResult):\n    @property\n    def answer(self):\n        s = f\"{self.json_data['summary']}\"\n        return s\n\n    @property\n    def prompt_responses(self):\n        return self.json_data[\"prompt_responses\"]\n\nclass BrowserTool(BaseTool):\n    \"\"\"\n    Browse a specific website using the provided URL link. \n    Recommended to use URLs from `web_search` to avoid invalid links.\n\n    Args:\n        url (str): The website's URL link.\n        question (str): The specific content or topic sought on the website.\n\n    Returns:\n        str: The webpage content.\n    \"\"\"\n    name = \"browse_website\"\n    zh_name = \"\u7f51\u9875\u6d4f\u89c8\u5668\"\n    description = \"Browse Website:\\\"browse_website\\\",args:\\\"url\\\":\\\"<url>, \\\"question\\\":\\\"<what_you_want_to_find_on_website>\\\"\"\n    tips = \"Browse a specific website using the provided URL link. Recommended to use URLs from `web_search` to avoid invalid links.\"\n\n    def __init__(self, cfg=None, *args, **kwargs):\n        self.cfg = cfg if cfg else Config()\n\n    def __call__(self, url, question=\"\",  *args, **kwargs):\n        summary, urls, prompt_responses = browse_website(url, question, self.cfg)\n        return BrowseResult({\n            \"summary\": summary,\n            \"urls\": urls,\n            \"prompt_responses\": prompt_responses\n        })\n\n",
    "prefix": "import logging\nimport kwaiagents.utils.nlp_utils as summary\nfrom pathlib import Path\nfrom bs4 import BeautifulSoup\nfrom selenium.webdriver.remote.webdriver import WebDriver\nfrom kwaiagents.utils.html_utils import extract_hyperlinks, format_hyperlinks\nfrom kwaiagents.config import Config\nfrom kwaiagents.tools.base import BaseTool, BaseResult\nfrom kwaiagents.utils.selenium_utils import get_pagesource_with_selenium\nfrom __future__ import annotations\n\n\n\nFILE_DIR = Path(__file__).parent.parent\n\n\nclass BrowseResult(BaseResult):\n    @property\n    def answer(self):\n        s = f\"{self.json_data['summary']}\"\n        return s\n\n    @property\n    def prompt_responses(self):\n        return self.json_data[\"prompt_responses\"]\n\nclass BrowserTool(BaseTool):\n    \"\"\"\n    Browse a specific website using the provided URL link. \n    Recommended to use URLs from `web_search` to avoid invalid links.\n\n    Args:\n        url (str): The website's URL link.\n        question (str): The specific content or topic sought on the website.\n\n    Returns:\n        str: The webpage content.\n    \"\"\"\n    name = \"browse_website\"\n    zh_name = \"\u7f51\u9875\u6d4f\u89c8\u5668\"\n    description = \"Browse Website:\\\"browse_website\\\",args:\\\"url\\\":\\\"<url>, \\\"question\\\":\\\"<what_you_want_to_find_on_website>\\\"\"\n    tips = \"Browse a specific website using the provided URL link. Recommended to use URLs from `web_search` to avoid invalid links.\"\n\n    def __init__(self, cfg=None, *args, **kwargs):\n        self.cfg = cfg if cfg else Config()\n\n    def __call__(self, url, question=\"\",  *args, **kwargs):\n        summary, urls, prompt_responses = browse_website(url, question, self.cfg)\n        return BrowseResult({\n            \"summary\": summary,\n            \"urls\": urls,\n            \"prompt_responses\": prompt_responses\n        })\n\n",
    "suffix": ""
  },
  {
    "name": "resemble-ai/resemble-enhance:resemble_enhance/utils/train_loop.py@872",
    "canonical_solution": "    def __call__(self, engine: Engine, batch: dict[str, Tensor]) -> tuple[Tensor, dict[str, Tensor]]:",
    "prompt": "import json\nimport logging\nimport time\nimport torch\nfrom dataclasses import KW_ONLY, dataclass\nfrom pathlib import Path\nfrom typing import Protocol\nfrom torch import Tensor\nfrom torch.utils.data import DataLoader\nfrom .control import non_blocking_input\nfrom .distributed import is_global_leader\nfrom .engine import Engine\nfrom .utils import tree_map\n\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass EvalFn(Protocol):\n    def __call__(self, engine: Engine, eval_dir: Path) -> None:\n        ...\n\n\nclass EngineLoader(Protocol):\n    def __call__(self, run_dir: Path) -> Engine:\n        ...\n\n\nclass GenFeeder(Protocol):",
    "prefix": "import json\nimport logging\nimport time\nimport torch\nfrom dataclasses import KW_ONLY, dataclass\nfrom pathlib import Path\nfrom typing import Protocol\nfrom torch import Tensor\nfrom torch.utils.data import DataLoader\nfrom .control import non_blocking_input\nfrom .distributed import is_global_leader\nfrom .engine import Engine\nfrom .utils import tree_map\n\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass EvalFn(Protocol):\n    def __call__(self, engine: Engine, eval_dir: Path) -> None:\n        ...\n\n\nclass EngineLoader(Protocol):\n    def __call__(self, run_dir: Path) -> Engine:\n        ...\n\n\nclass GenFeeder(Protocol):",
    "suffix": ""
  },
  {
    "name": "PKU-YuanGroup/Chat-UniVi:ChatUniVi/model/arch.py@1389",
    "canonical_solution": "        vision_tower = build_vision_tower(model_args)",
    "prompt": "from abc import ABC, abstractmethod\nfrom .multimodal_encoder.builder import build_vision_tower\nfrom ChatUniVi.constants import *\nfrom .cluster import CTM, TCBlock\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\n\n\nclass MetaModel:\n    def __init__(self, config):\n        super(MetaModel, self).__init__(config)\n\n        if hasattr(config, \"mm_vision_tower\"):\n            self.vision_tower = build_vision_tower(config, delay_load=True)\n            self.mm_projector = nn.Linear(config.mm_hidden_size, config.hidden_size)\n\n        if hasattr(config, \"config\"):\n            self.use_cluster = config.config[\"use_cluster\"]\n            if self.use_cluster:\n                self.ctm0 = CTM(sample_ratio=config.config[\"spatial_cluster_rate0\"], embed_dim=self.config.mm_hidden_size, dim_out=self.config.mm_hidden_size, k=5)\n                self.block0 = TCBlock(dim=self.config.mm_hidden_size, num_heads=8)\n\n                self.ctm1 = CTM(sample_ratio=config.config[\"spatial_cluster_rate1\"], embed_dim=self.config.mm_hidden_size, dim_out=self.config.mm_hidden_size, k=3)\n                self.block1 = TCBlock(dim=self.config.mm_hidden_size, num_heads=8)\n\n                self.ctm2 = CTM(sample_ratio=config.config[\"spatial_cluster_rate2\"], embed_dim=self.config.mm_hidden_size, dim_out=self.config.mm_hidden_size, k=3)\n                self.block2 = TCBlock(dim=self.config.mm_hidden_size, num_heads=8)\n\n                self.ctm3 = CTM(sample_ratio=config.config[\"temporal_cluster_rate\"], embed_dim=self.config.mm_hidden_size, dim_out=self.config.mm_hidden_size, k=5)\n                self.block3 = TCBlock(dim=self.config.mm_hidden_size, num_heads=8)\n        else:\n            self.use_cluster = False\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, 'vision_tower', None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None):\n        vision_tower = model_args.vision_tower\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n\n        self.config.mm_vision_tower = vision_tower\n",
    "prefix": "from abc import ABC, abstractmethod\nfrom .multimodal_encoder.builder import build_vision_tower\nfrom ChatUniVi.constants import *\nfrom .cluster import CTM, TCBlock\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\n\n\nclass MetaModel:\n    def __init__(self, config):\n        super(MetaModel, self).__init__(config)\n\n        if hasattr(config, \"mm_vision_tower\"):\n            self.vision_tower = build_vision_tower(config, delay_load=True)\n            self.mm_projector = nn.Linear(config.mm_hidden_size, config.hidden_size)\n\n        if hasattr(config, \"config\"):\n            self.use_cluster = config.config[\"use_cluster\"]\n            if self.use_cluster:\n                self.ctm0 = CTM(sample_ratio=config.config[\"spatial_cluster_rate0\"], embed_dim=self.config.mm_hidden_size, dim_out=self.config.mm_hidden_size, k=5)\n                self.block0 = TCBlock(dim=self.config.mm_hidden_size, num_heads=8)\n\n                self.ctm1 = CTM(sample_ratio=config.config[\"spatial_cluster_rate1\"], embed_dim=self.config.mm_hidden_size, dim_out=self.config.mm_hidden_size, k=3)\n                self.block1 = TCBlock(dim=self.config.mm_hidden_size, num_heads=8)\n\n                self.ctm2 = CTM(sample_ratio=config.config[\"spatial_cluster_rate2\"], embed_dim=self.config.mm_hidden_size, dim_out=self.config.mm_hidden_size, k=3)\n                self.block2 = TCBlock(dim=self.config.mm_hidden_size, num_heads=8)\n\n                self.ctm3 = CTM(sample_ratio=config.config[\"temporal_cluster_rate\"], embed_dim=self.config.mm_hidden_size, dim_out=self.config.mm_hidden_size, k=5)\n                self.block3 = TCBlock(dim=self.config.mm_hidden_size, num_heads=8)\n        else:\n            self.use_cluster = False\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, 'vision_tower', None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None):\n        vision_tower = model_args.vision_tower\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n\n        self.config.mm_vision_tower = vision_tower\n",
    "suffix": ""
  },
  {
    "name": "tatsu-lab/gpt_paper_assistant:filter_papers.py@1380",
    "canonical_solution": "    papers: List[Paper], base_prompt: str, criterion: str",
    "prompt": "import configparser\nimport dataclasses\nimport json\nimport os\nimport re\nimport retry\nfrom collections import defaultdict\nfrom typing import List\nfrom openai import OpenAI\nfrom tqdm import tqdm\nfrom arxiv_scraper import Paper\nfrom arxiv_scraper import EnhancedJSONEncoder\n\n\n\n\ndef filter_by_author(all_authors, papers, author_targets, config):\n    # filter and parse the papers\n    selected_papers = {}  # pass to output\n    all_papers = {}  # dict for later filtering\n    sort_dict = {}  # dict storing key and score\n\n    # author based selection\n    for paper in papers:\n        all_papers[paper.arxiv_id] = paper\n        if config[\"FILTERING\"].getboolean(\"author_match\"):\n            for author in paper.authors:\n                if author in all_authors:\n                    for alias in all_authors[author]:\n                        if alias[\"authorId\"] in author_targets:\n                            selected_papers[paper.arxiv_id] = {\n                                **dataclasses.asdict(paper),\n                                **{\"COMMENT\": \"Author match\"},\n                            }\n                            sort_dict[paper.arxiv_id] = float(\n                                config[\"SELECTION\"][\"author_match_score\"]\n                            )\n                            break\n    return selected_papers, all_papers, sort_dict\n\n\ndef filter_papers_by_hindex(all_authors, papers, config):\n    # filters papers by checking to see if there's at least one author with > hcutoff hindex\n    paper_list = []\n    for paper in papers:\n        max_h = 0\n        for author in paper.authors:\n            if author in all_authors:\n                max_h = max(\n                    max_h, max([alias[\"hIndex\"] for alias in all_authors[author]])\n                )\n        if max_h >= float(config[\"FILTERING\"][\"hcutoff\"]):\n            paper_list.append(paper)\n    return paper_list\n\n\ndef calc_price(model, usage):\n    if model == \"gpt-4-1106-preview\":\n        return (0.01 * usage.prompt_tokens + 0.03 * usage.completion_tokens) / 1000.0\n    if model == \"gpt-4\":\n        return (0.03 * usage.prompt_tokens + 0.06 * usage.completion_tokens) / 1000.0\n    if (model == \"gpt-3.5-turbo\") or (model == \"gpt-3.5-turbo-1106\"):\n        return (0.0015 * usage.prompt_tokens + 0.002 * usage.completion_tokens) / 1000.0\n\n\n@retry.retry(tries=3, delay=2)\ndef call_chatgpt(full_prompt, openai_client, model, num_samples):\n    return openai_client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": full_prompt}],\n        temperature=0.0,\n        n=int(num_samples),\n        seed=0,\n    )\n\n\ndef run_and_parse_chatgpt(full_prompt, openai_client, config):\n    # just runs the chatgpt prompt, tries to parse the resulting JSON\n    completion = call_chatgpt(\n        full_prompt,\n        openai_client,\n        config[\"SELECTION\"][\"model\"],\n        config[\"FILTERING\"][\"num_samples\"],\n    )\n    json_dicts = defaultdict(list)\n    for choice in completion.choices:\n        out_text = choice.message.content\n        out_text = re.sub(\"```jsonl\\n\", \"\", out_text)\n        out_text = re.sub(\"```\", \"\", out_text)\n        out_text = re.sub(r\"\\n+\", \"\\n\", out_text)\n        out_text = re.sub(\"},\", \"}\", out_text).strip()\n        # split out_text line by line and parse each as a json.\n        for line in out_text.split(\"\\n\"):\n            # try catch block to attempt to parse json\n            try:\n                loaded_output = json.loads(line)\n                json_dicts[loaded_output[\"ARXIVID\"]].append(loaded_output)\n            except Exception as ex:\n                if config[\"OUTPUT\"].getboolean(\"debug_messages\"):\n                    print(\"Exception happened \" + str(ex))\n                    print(\"Failed to parse LM output as json\")\n                    print(out_text)\n                    print(\"RAW output\")\n                    print(completion.choices[0].message.content)\n                continue\n    all_dict = []\n    for id, json_list in json_dicts.items():\n        rel_score = sum([float(jdict[\"RELEVANCE\"]) for jdict in json_list]) / float(\n            len(json_list)\n        )\n        nov_score = sum([float(jdict[\"NOVELTY\"]) for jdict in json_list]) / float(\n            len(json_list)\n        )\n        new_dict = {\n            \"ARXIVID\": json_list[0][\"ARXIVID\"],\n            \"COMMENT\": json_list[0][\"COMMENT\"],\n            \"RELEVANCE\": rel_score,\n            \"NOVELTY\": nov_score,\n        }\n        all_dict.append(new_dict)\n    return all_dict, calc_price(config[\"SELECTION\"][\"model\"], completion.usage)\n\n\ndef paper_to_string(paper_entry: Paper) -> str:\n    # renders each paper into a string to be processed by GPT\n    new_str = (\n        \"ArXiv ID: \"\n        + paper_entry.arxiv_id\n        + \"\\n\"\n        + \"Title: \"\n        + paper_entry.title\n        + \"\\n\"\n        + \"Authors: \"\n        + \" and \".join(paper_entry.authors)\n        + \"\\n\"\n        + \"Abstract: \"\n        + paper_entry.abstract[:4000]\n    )\n    return new_str\n\n\ndef batched(items, batch_size):\n    # takes a list and returns a list of list with batch_size\n    return [items[i : i + batch_size] for i in range(0, len(items), batch_size)]\n\n\ndef filter_papers_by_title(",
    "prefix": "import configparser\nimport dataclasses\nimport json\nimport os\nimport re\nimport retry\nfrom collections import defaultdict\nfrom typing import List\nfrom openai import OpenAI\nfrom tqdm import tqdm\nfrom arxiv_scraper import Paper\nfrom arxiv_scraper import EnhancedJSONEncoder\n\n\n\n\ndef filter_by_author(all_authors, papers, author_targets, config):\n    # filter and parse the papers\n    selected_papers = {}  # pass to output\n    all_papers = {}  # dict for later filtering\n    sort_dict = {}  # dict storing key and score\n\n    # author based selection\n    for paper in papers:\n        all_papers[paper.arxiv_id] = paper\n        if config[\"FILTERING\"].getboolean(\"author_match\"):\n            for author in paper.authors:\n                if author in all_authors:\n                    for alias in all_authors[author]:\n                        if alias[\"authorId\"] in author_targets:\n                            selected_papers[paper.arxiv_id] = {\n                                **dataclasses.asdict(paper),\n                                **{\"COMMENT\": \"Author match\"},\n                            }\n                            sort_dict[paper.arxiv_id] = float(\n                                config[\"SELECTION\"][\"author_match_score\"]\n                            )\n                            break\n    return selected_papers, all_papers, sort_dict\n\n\ndef filter_papers_by_hindex(all_authors, papers, config):\n    # filters papers by checking to see if there's at least one author with > hcutoff hindex\n    paper_list = []\n    for paper in papers:\n        max_h = 0\n        for author in paper.authors:\n            if author in all_authors:\n                max_h = max(\n                    max_h, max([alias[\"hIndex\"] for alias in all_authors[author]])\n                )\n        if max_h >= float(config[\"FILTERING\"][\"hcutoff\"]):\n            paper_list.append(paper)\n    return paper_list\n\n\ndef calc_price(model, usage):\n    if model == \"gpt-4-1106-preview\":\n        return (0.01 * usage.prompt_tokens + 0.03 * usage.completion_tokens) / 1000.0\n    if model == \"gpt-4\":\n        return (0.03 * usage.prompt_tokens + 0.06 * usage.completion_tokens) / 1000.0\n    if (model == \"gpt-3.5-turbo\") or (model == \"gpt-3.5-turbo-1106\"):\n        return (0.0015 * usage.prompt_tokens + 0.002 * usage.completion_tokens) / 1000.0\n\n\n@retry.retry(tries=3, delay=2)\ndef call_chatgpt(full_prompt, openai_client, model, num_samples):\n    return openai_client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": full_prompt}],\n        temperature=0.0,\n        n=int(num_samples),\n        seed=0,\n    )\n\n\ndef run_and_parse_chatgpt(full_prompt, openai_client, config):\n    # just runs the chatgpt prompt, tries to parse the resulting JSON\n    completion = call_chatgpt(\n        full_prompt,\n        openai_client,\n        config[\"SELECTION\"][\"model\"],\n        config[\"FILTERING\"][\"num_samples\"],\n    )\n    json_dicts = defaultdict(list)\n    for choice in completion.choices:\n        out_text = choice.message.content\n        out_text = re.sub(\"```jsonl\\n\", \"\", out_text)\n        out_text = re.sub(\"```\", \"\", out_text)\n        out_text = re.sub(r\"\\n+\", \"\\n\", out_text)\n        out_text = re.sub(\"},\", \"}\", out_text).strip()\n        # split out_text line by line and parse each as a json.\n        for line in out_text.split(\"\\n\"):\n            # try catch block to attempt to parse json\n            try:\n                loaded_output = json.loads(line)\n                json_dicts[loaded_output[\"ARXIVID\"]].append(loaded_output)\n            except Exception as ex:\n                if config[\"OUTPUT\"].getboolean(\"debug_messages\"):\n                    print(\"Exception happened \" + str(ex))\n                    print(\"Failed to parse LM output as json\")\n                    print(out_text)\n                    print(\"RAW output\")\n                    print(completion.choices[0].message.content)\n                continue\n    all_dict = []\n    for id, json_list in json_dicts.items():\n        rel_score = sum([float(jdict[\"RELEVANCE\"]) for jdict in json_list]) / float(\n            len(json_list)\n        )\n        nov_score = sum([float(jdict[\"NOVELTY\"]) for jdict in json_list]) / float(\n            len(json_list)\n        )\n        new_dict = {\n            \"ARXIVID\": json_list[0][\"ARXIVID\"],\n            \"COMMENT\": json_list[0][\"COMMENT\"],\n            \"RELEVANCE\": rel_score,\n            \"NOVELTY\": nov_score,\n        }\n        all_dict.append(new_dict)\n    return all_dict, calc_price(config[\"SELECTION\"][\"model\"], completion.usage)\n\n\ndef paper_to_string(paper_entry: Paper) -> str:\n    # renders each paper into a string to be processed by GPT\n    new_str = (\n        \"ArXiv ID: \"\n        + paper_entry.arxiv_id\n        + \"\\n\"\n        + \"Title: \"\n        + paper_entry.title\n        + \"\\n\"\n        + \"Authors: \"\n        + \" and \".join(paper_entry.authors)\n        + \"\\n\"\n        + \"Abstract: \"\n        + paper_entry.abstract[:4000]\n    )\n    return new_str\n\n\ndef batched(items, batch_size):\n    # takes a list and returns a list of list with batch_size\n    return [items[i : i + batch_size] for i in range(0, len(items), batch_size)]\n\n\ndef filter_papers_by_title(",
    "suffix": ""
  },
  {
    "name": "BobaZooba/xllm:tests/unit/utils/test_logger.py@696",
    "canonical_solution": "        enums.LogLevel.critical,",
    "prompt": "import pytest\nfrom src.xllm import enums\nfrom src.xllm.utils.logger import dist_logger\n# Copyright 2023 Boris Zubarev. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\n\n@pytest.mark.parametrize(\"message\", [\"Hello world\", \"Hey!\", \"All good\"])\ndef test_info(message: str):\n    dist_logger.info(message=message)\n\n\n@pytest.mark.parametrize(\"message\", [\"Hello world\", \"Hey!\", \"All good\"])\ndef test_waring(message: str):\n    dist_logger.warning(message=message)\n\n\n@pytest.mark.parametrize(\"message\", [\"Hello world\", \"Hey!\", \"All good\"])\ndef test_error(message: str):\n    dist_logger.error(message=message)\n\n\n@pytest.mark.parametrize(\"message\", [\"Hello world\", \"Hey!\", \"All good\"])\ndef test_critical(message: str):\n    dist_logger.critical(message=message)\n\n\n@pytest.mark.parametrize(\"message\", [\"Hello world\", \"Hey!\", \"All good\"])\n@pytest.mark.parametrize(\n    \"level\",\n    [\n        enums.LogLevel.info,\n        enums.LogLevel.warning,\n        enums.LogLevel.error,\n        enums.LogLevel.critical,\n        None,\n        \"some\",\n    ],\n)\ndef test_log(message: str, level: str):\n    dist_logger.log(message=message, level=level)\n\n\n@pytest.mark.parametrize(\"message\", [\"Hello world\", \"Hey!\", \"All good\"])\n@pytest.mark.parametrize(\n    \"level\",\n    [\n        enums.LogLevel.info,\n        enums.LogLevel.warning,\n        enums.LogLevel.error,",
    "prefix": "import pytest\nfrom src.xllm import enums\nfrom src.xllm.utils.logger import dist_logger\n# Copyright 2023 Boris Zubarev. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\n\n@pytest.mark.parametrize(\"message\", [\"Hello world\", \"Hey!\", \"All good\"])\ndef test_info(message: str):\n    dist_logger.info(message=message)\n\n\n@pytest.mark.parametrize(\"message\", [\"Hello world\", \"Hey!\", \"All good\"])\ndef test_waring(message: str):\n    dist_logger.warning(message=message)\n\n\n@pytest.mark.parametrize(\"message\", [\"Hello world\", \"Hey!\", \"All good\"])\ndef test_error(message: str):\n    dist_logger.error(message=message)\n\n\n@pytest.mark.parametrize(\"message\", [\"Hello world\", \"Hey!\", \"All good\"])\ndef test_critical(message: str):\n    dist_logger.critical(message=message)\n\n\n@pytest.mark.parametrize(\"message\", [\"Hello world\", \"Hey!\", \"All good\"])\n@pytest.mark.parametrize(\n    \"level\",\n    [\n        enums.LogLevel.info,\n        enums.LogLevel.warning,\n        enums.LogLevel.error,\n        enums.LogLevel.critical,\n        None,\n        \"some\",\n    ],\n)\ndef test_log(message: str, level: str):\n    dist_logger.log(message=message, level=level)\n\n\n@pytest.mark.parametrize(\"message\", [\"Hello world\", \"Hey!\", \"All good\"])\n@pytest.mark.parametrize(\n    \"level\",\n    [\n        enums.LogLevel.info,\n        enums.LogLevel.warning,\n        enums.LogLevel.error,",
    "suffix": ""
  },
  {
    "name": "innovatorved/subtitle:app/utils/utils.py@878",
    "canonical_solution": "        return model_names[model]",
    "prompt": "import os\nimport re\nimport urllib\nimport subprocess\nimport uuid\nimport logging\nimport wave\nimport gdown\nimport ffmpeg\nfrom tqdm import tqdm\nfrom app.models import model_names\nfrom .checks import chack_file_exist\nfrom .contant import NO_OF_THREADS, NO_OF_PROCESSORS\n\n\n\n\ndef transcribe_file(path: str = None, model=\"ggml-model-whisper-tiny.en-q5_1.bin\"):\n    \"\"\"./binary/whisper -m models/ggml-tiny.en.bin -f Rev.mp3 out.wav -nt --output-text out1.txt\"\"\"\n    try:\n        if path is None:\n            raise Exception(\"No path provided\")\n        rand = uuid.uuid4()\n        outputFilePath: str = f\"transcribe/{rand}.txt\"\n        output_audio_path: str = f\"audio/{rand}.wav\"\n        command: str = f\"./binary/whisper -m models/{model} -f {path} {output_audio_path} -nt --output-text {outputFilePath}\"\n        execute_command(command)\n        f = open(outputFilePath, \"r\")\n        data = f.read()\n        f.close()\n        return [data, output_audio_path]\n    except Exception as exc:\n        logging.error(exc)\n        raise Exception(exc.__str__())\n\n\ndef generate_vtt_file(path: str = None, model=\"ggml-tiny.bin\"):\n    \"\"\"./whisper -m models/ggml-tiny.en.bin -f Rev.mp3 out.wav -nt --output-vtt\"\"\"\n    try:\n        if path is None or not chack_file_exist(path):\n            raise Exception(\"PATH Error!\")\n        rand = uuid.uuid4()\n        output_audio_path: str = f\"data/{rand}.wav\"\n        vtt_file_path: str = f\"data/{rand}.wav.vtt\"\n        command: str = f\"./binary/whisper -t {NO_OF_THREADS} -p {NO_OF_PROCESSORS} -m models/{model} -f {path} {output_audio_path} -nt --output-vtt\"\n        execute_command(command)\n        return [rand, output_audio_path, vtt_file_path]\n    except Exception as exc:\n        logging.error(exc)\n        raise Exception(exc.__str__())\n\n\ndef execute_command(command: str) -> str:\n    try:\n        result = subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n        return result.decode(\"utf-8\").strip()\n    except subprocess.CalledProcessError as exc:\n        logging.error(exc.output.decode(\"utf-8\").strip())\n        raise Exception(\"Error while transcribing\")\n\n\ndef save_audio_file(file=None):\n    if file is None:\n        return \"\"\n    path = f\"audio/{uuid.uuid4()}.mp3\"\n    with open(path, \"wb\") as f:\n        f.write(file.file.read())\n    return path\n\n\ndef get_audio_duration(audio_file):\n    \"\"\"Gets the duration of the audio file in seconds.\n\n    Args:\n      audio_file: The path to the audio file.\n\n    Returns:\n      The duration of the audio file in seconds.\n    \"\"\"\n\n    with wave.open(audio_file, \"rb\") as f:\n        frames = f.getnframes()\n        sample_rate = f.getframerate()\n        duration = frames / sample_rate\n        rounded_duration = int(round(duration, 0))\n\n    return rounded_duration\n\n\ndef get_model_name(model: str = None):\n    if model is None:\n        model = \"tiny.en.q5\"\n\n    if model in model_names.keys():",
    "prefix": "import os\nimport re\nimport urllib\nimport subprocess\nimport uuid\nimport logging\nimport wave\nimport gdown\nimport ffmpeg\nfrom tqdm import tqdm\nfrom app.models import model_names\nfrom .checks import chack_file_exist\nfrom .contant import NO_OF_THREADS, NO_OF_PROCESSORS\n\n\n\n\ndef transcribe_file(path: str = None, model=\"ggml-model-whisper-tiny.en-q5_1.bin\"):\n    \"\"\"./binary/whisper -m models/ggml-tiny.en.bin -f Rev.mp3 out.wav -nt --output-text out1.txt\"\"\"\n    try:\n        if path is None:\n            raise Exception(\"No path provided\")\n        rand = uuid.uuid4()\n        outputFilePath: str = f\"transcribe/{rand}.txt\"\n        output_audio_path: str = f\"audio/{rand}.wav\"\n        command: str = f\"./binary/whisper -m models/{model} -f {path} {output_audio_path} -nt --output-text {outputFilePath}\"\n        execute_command(command)\n        f = open(outputFilePath, \"r\")\n        data = f.read()\n        f.close()\n        return [data, output_audio_path]\n    except Exception as exc:\n        logging.error(exc)\n        raise Exception(exc.__str__())\n\n\ndef generate_vtt_file(path: str = None, model=\"ggml-tiny.bin\"):\n    \"\"\"./whisper -m models/ggml-tiny.en.bin -f Rev.mp3 out.wav -nt --output-vtt\"\"\"\n    try:\n        if path is None or not chack_file_exist(path):\n            raise Exception(\"PATH Error!\")\n        rand = uuid.uuid4()\n        output_audio_path: str = f\"data/{rand}.wav\"\n        vtt_file_path: str = f\"data/{rand}.wav.vtt\"\n        command: str = f\"./binary/whisper -t {NO_OF_THREADS} -p {NO_OF_PROCESSORS} -m models/{model} -f {path} {output_audio_path} -nt --output-vtt\"\n        execute_command(command)\n        return [rand, output_audio_path, vtt_file_path]\n    except Exception as exc:\n        logging.error(exc)\n        raise Exception(exc.__str__())\n\n\ndef execute_command(command: str) -> str:\n    try:\n        result = subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n        return result.decode(\"utf-8\").strip()\n    except subprocess.CalledProcessError as exc:\n        logging.error(exc.output.decode(\"utf-8\").strip())\n        raise Exception(\"Error while transcribing\")\n\n\ndef save_audio_file(file=None):\n    if file is None:\n        return \"\"\n    path = f\"audio/{uuid.uuid4()}.mp3\"\n    with open(path, \"wb\") as f:\n        f.write(file.file.read())\n    return path\n\n\ndef get_audio_duration(audio_file):\n    \"\"\"Gets the duration of the audio file in seconds.\n\n    Args:\n      audio_file: The path to the audio file.\n\n    Returns:\n      The duration of the audio file in seconds.\n    \"\"\"\n\n    with wave.open(audio_file, \"rb\") as f:\n        frames = f.getnframes()\n        sample_rate = f.getframerate()\n        duration = frames / sample_rate\n        rounded_duration = int(round(duration, 0))\n\n    return rounded_duration\n\n\ndef get_model_name(model: str = None):\n    if model is None:\n        model = \"tiny.en.q5\"\n\n    if model in model_names.keys():",
    "suffix": ""
  },
  {
    "name": "x0rzavi/github-readme-terminal:gifos/utils/convert_ansi_escape.py@986",
    "canonical_solution": "        \"95\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"bright_colors\"), \"magenta\", \"#c58cec\"),",
    "prompt": "from gifos.utils.load_config import ansi_escape_colors, gifos_settings\nfrom gifos.utils.schemas.ansi_escape import AnsiEscape\n# Colorscheme reference: https://github.com/rxyhn/yoru#art--colorscheme\n\n\"\"\"This module contains a class `ConvertAnsiEscape` for converting ANSI escape codes to color values.\"\"\"\n\n\nclass ConvertAnsiEscape:\n    \"\"\"A class for converting ANSI escape codes to color values.\"\"\"\n\n    __color_scheme = gifos_settings.get(\"general\", {}).get(\"color_scheme\")\n\n    @staticmethod\n    def __get_color(color_dict, color_name, def_color):\n        \"\"\"Get the color value from the color dictionary.\n\n        This method takes a color dictionary, a color name, and a default color as\n        input. If the color dictionary is indeed a dictionary and contains the color\n        name, it returns the corresponding color value. Otherwise, it returns the\n        default color.\n\n        :param color_dict: The color dictionary to get the color value from.\n        :type color_dict: dict\n        :param color_name: The name of the color to get.\n        :type color_name: str\n        :param def_color: The default color to return if the color name is not in the\n            color dictionary.\n        :type def_color: str\n        :return: The color value corresponding to the color name if it's in the color\n            dictionary, otherwise the default color.\n        :rtype: str\n        \"\"\"\n        return (\n            color_dict.get(color_name, def_color)\n            if isinstance(color_dict, dict)\n            else def_color\n        )\n\n    # fmt: off\n    ANSI_ESCAPE_MAP_TXT_COLOR = {\n        # normal color mode\n        \"30\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"normal_colors\"), \"black\", \"#232526\"),\n        \"31\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"normal_colors\"), \"red\", \"#df5b61\"),\n        \"32\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"normal_colors\"), \"green\", \"#78b892\"),\n        \"33\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"normal_colors\"), \"yellow\", \"#de8f78\"),\n        \"34\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"normal_colors\"), \"blue\", \"#6791c9\"),\n        \"35\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"normal_colors\"), \"magenta\", \"#bc83e3\"),\n        \"36\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"normal_colors\"), \"cyan\", \"#67afc1\"),\n        \"37\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"normal_colors\"), \"white\", \"#e4e6e7\"),\n        \"39\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"default_colors\"), \"fg\", \"#edeff0\"),\n        # bright color mode\n        \"90\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"bright_colors\"), \"black\", \"#2c2e2f\"),\n        \"91\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"bright_colors\"), \"red\", \"#e8646a\"),\n        \"92\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"bright_colors\"), \"green\", \"#81c19b\"),\n        \"93\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"bright_colors\"), \"yellow\", \"#e79881\"),\n        \"94\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"bright_colors\"), \"blue\", \"#709ad2\"),",
    "prefix": "from gifos.utils.load_config import ansi_escape_colors, gifos_settings\nfrom gifos.utils.schemas.ansi_escape import AnsiEscape\n# Colorscheme reference: https://github.com/rxyhn/yoru#art--colorscheme\n\n\"\"\"This module contains a class `ConvertAnsiEscape` for converting ANSI escape codes to color values.\"\"\"\n\n\nclass ConvertAnsiEscape:\n    \"\"\"A class for converting ANSI escape codes to color values.\"\"\"\n\n    __color_scheme = gifos_settings.get(\"general\", {}).get(\"color_scheme\")\n\n    @staticmethod\n    def __get_color(color_dict, color_name, def_color):\n        \"\"\"Get the color value from the color dictionary.\n\n        This method takes a color dictionary, a color name, and a default color as\n        input. If the color dictionary is indeed a dictionary and contains the color\n        name, it returns the corresponding color value. Otherwise, it returns the\n        default color.\n\n        :param color_dict: The color dictionary to get the color value from.\n        :type color_dict: dict\n        :param color_name: The name of the color to get.\n        :type color_name: str\n        :param def_color: The default color to return if the color name is not in the\n            color dictionary.\n        :type def_color: str\n        :return: The color value corresponding to the color name if it's in the color\n            dictionary, otherwise the default color.\n        :rtype: str\n        \"\"\"\n        return (\n            color_dict.get(color_name, def_color)\n            if isinstance(color_dict, dict)\n            else def_color\n        )\n\n    # fmt: off\n    ANSI_ESCAPE_MAP_TXT_COLOR = {\n        # normal color mode\n        \"30\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"normal_colors\"), \"black\", \"#232526\"),\n        \"31\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"normal_colors\"), \"red\", \"#df5b61\"),\n        \"32\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"normal_colors\"), \"green\", \"#78b892\"),\n        \"33\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"normal_colors\"), \"yellow\", \"#de8f78\"),\n        \"34\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"normal_colors\"), \"blue\", \"#6791c9\"),\n        \"35\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"normal_colors\"), \"magenta\", \"#bc83e3\"),\n        \"36\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"normal_colors\"), \"cyan\", \"#67afc1\"),\n        \"37\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"normal_colors\"), \"white\", \"#e4e6e7\"),\n        \"39\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"default_colors\"), \"fg\", \"#edeff0\"),\n        # bright color mode\n        \"90\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"bright_colors\"), \"black\", \"#2c2e2f\"),\n        \"91\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"bright_colors\"), \"red\", \"#e8646a\"),\n        \"92\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"bright_colors\"), \"green\", \"#81c19b\"),\n        \"93\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"bright_colors\"), \"yellow\", \"#e79881\"),\n        \"94\": __get_color(ansi_escape_colors.get(__color_scheme, {}).get(\"bright_colors\"), \"blue\", \"#709ad2\"),",
    "suffix": ""
  },
  {
    "name": "Zaloog/kanban-python:src/kanban_python/interface.py@1358",
    "canonical_solution": "            else FOOTER[1]",
    "prompt": "import calendar\nfrom datetime import datetime\nfrom itertools import zip_longest\nfrom rich.prompt import Confirm, IntPrompt, Prompt\nfrom rich.table import Table\nfrom .config import cfg\nfrom .constants import (\n    BOARD_CAPTION_STRING,\n    COLOR_DICT,\n    CONFIG_FILE_PATH,\n    FOOTER,\n    REPORT_COLORS,\n)\nfrom .utils import (\n    calculate_days_left_till_due,\n    calculate_time_delta_str,\n    check_due_date_format,\n    console,\n    create_color_mapping,\n    create_dict_for_report_view,\n    create_status_dict_for_rows,\n    current_time_to_str,\n    due_date_date_to_datetime,\n    due_date_datetime_to_date,\n)\n\n\n\n\n# Board\n#####################################################################################\ndef create_table(data: dict) -> Table:\n    status_dict = create_status_dict_for_rows(data=data, vis_cols=cfg.vis_cols)\n\n    table_name = cfg.active_board\n    table = Table(\n        title=f\"[blue]Active Board: {table_name}[/]\",\n        highlight=True,\n        show_header=True,\n        show_footer=True if cfg.show_footer == \"True\" else False,\n        caption=BOARD_CAPTION_STRING,\n    )\n\n    for i, category in enumerate([COLOR_DICT.get(col, col) for col in cfg.vis_cols]):\n        table.add_column(\n            header=category + f\"\\t({len(status_dict[cfg.vis_cols[i]])} Task/s)\",\n            header_style=\"bold\",\n            justify=\"left\",\n            overflow=\"fold\",\n            footer=FOOTER[0]\n            if i == 0",
    "prefix": "import calendar\nfrom datetime import datetime\nfrom itertools import zip_longest\nfrom rich.prompt import Confirm, IntPrompt, Prompt\nfrom rich.table import Table\nfrom .config import cfg\nfrom .constants import (\n    BOARD_CAPTION_STRING,\n    COLOR_DICT,\n    CONFIG_FILE_PATH,\n    FOOTER,\n    REPORT_COLORS,\n)\nfrom .utils import (\n    calculate_days_left_till_due,\n    calculate_time_delta_str,\n    check_due_date_format,\n    console,\n    create_color_mapping,\n    create_dict_for_report_view,\n    create_status_dict_for_rows,\n    current_time_to_str,\n    due_date_date_to_datetime,\n    due_date_datetime_to_date,\n)\n\n\n\n\n# Board\n#####################################################################################\ndef create_table(data: dict) -> Table:\n    status_dict = create_status_dict_for_rows(data=data, vis_cols=cfg.vis_cols)\n\n    table_name = cfg.active_board\n    table = Table(\n        title=f\"[blue]Active Board: {table_name}[/]\",\n        highlight=True,\n        show_header=True,\n        show_footer=True if cfg.show_footer == \"True\" else False,\n        caption=BOARD_CAPTION_STRING,\n    )\n\n    for i, category in enumerate([COLOR_DICT.get(col, col) for col in cfg.vis_cols]):\n        table.add_column(\n            header=category + f\"\\t({len(status_dict[cfg.vis_cols[i]])} Task/s)\",\n            header_style=\"bold\",\n            justify=\"left\",\n            overflow=\"fold\",\n            footer=FOOTER[0]\n            if i == 0",
    "suffix": ""
  },
  {
    "name": "AMAAI-Lab/mustango:diffusers/src/diffusers/utils/testing_utils.py@1129",
    "canonical_solution": "    if not is_torch_available():",
    "prompt": "import inspect\nimport logging\nimport os\nimport random\nimport re\nimport tempfile\nimport unittest\nimport urllib.parse\nimport numpy as np\nimport PIL.Image\nimport PIL.ImageOps\nimport requests\n    import torch\n        import cv2\nfrom distutils.util import strtobool\nfrom io import BytesIO, StringIO\nfrom pathlib import Path\nfrom typing import List, Optional, Union\nfrom packaging import version\nfrom .import_utils import (\n    BACKENDS_MAPPING,\n    is_compel_available,\n    is_flax_available,\n    is_note_seq_available,\n    is_onnx_available,\n    is_opencv_available,\n    is_torch_available,\n    is_torch_version,\n)\nfrom .logging import get_logger\n    from _pytest.config import create_terminal_writer\n\n\n\n\nglobal_rng = random.Random()\n\nlogger = get_logger(__name__)\n\nif is_torch_available():\n\n    if \"DIFFUSERS_TEST_DEVICE\" in os.environ:\n        torch_device = os.environ[\"DIFFUSERS_TEST_DEVICE\"]\n\n        available_backends = [\"cuda\", \"cpu\", \"mps\"]\n        if torch_device not in available_backends:\n            raise ValueError(\n                f\"unknown torch backend for diffusers tests: {torch_device}. Available backends are:\"\n                f\" {available_backends}\"\n            )\n        logger.info(f\"torch_device overrode to {torch_device}\")\n    else:\n        torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        is_torch_higher_equal_than_1_12 = version.parse(\n            version.parse(torch.__version__).base_version\n        ) >= version.parse(\"1.12\")\n\n        if is_torch_higher_equal_than_1_12:\n            # Some builds of torch 1.12 don't have the mps backend registered. See #892 for more details\n            mps_backend_registered = hasattr(torch.backends, \"mps\")\n            torch_device = \"mps\" if (mps_backend_registered and torch.backends.mps.is_available()) else torch_device\n\n\ndef torch_all_close(a, b, *args, **kwargs):",
    "prefix": "import inspect\nimport logging\nimport os\nimport random\nimport re\nimport tempfile\nimport unittest\nimport urllib.parse\nimport numpy as np\nimport PIL.Image\nimport PIL.ImageOps\nimport requests\n    import torch\n        import cv2\nfrom distutils.util import strtobool\nfrom io import BytesIO, StringIO\nfrom pathlib import Path\nfrom typing import List, Optional, Union\nfrom packaging import version\nfrom .import_utils import (\n    BACKENDS_MAPPING,\n    is_compel_available,\n    is_flax_available,\n    is_note_seq_available,\n    is_onnx_available,\n    is_opencv_available,\n    is_torch_available,\n    is_torch_version,\n)\nfrom .logging import get_logger\n    from _pytest.config import create_terminal_writer\n\n\n\n\nglobal_rng = random.Random()\n\nlogger = get_logger(__name__)\n\nif is_torch_available():\n\n    if \"DIFFUSERS_TEST_DEVICE\" in os.environ:\n        torch_device = os.environ[\"DIFFUSERS_TEST_DEVICE\"]\n\n        available_backends = [\"cuda\", \"cpu\", \"mps\"]\n        if torch_device not in available_backends:\n            raise ValueError(\n                f\"unknown torch backend for diffusers tests: {torch_device}. Available backends are:\"\n                f\" {available_backends}\"\n            )\n        logger.info(f\"torch_device overrode to {torch_device}\")\n    else:\n        torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        is_torch_higher_equal_than_1_12 = version.parse(\n            version.parse(torch.__version__).base_version\n        ) >= version.parse(\"1.12\")\n\n        if is_torch_higher_equal_than_1_12:\n            # Some builds of torch 1.12 don't have the mps backend registered. See #892 for more details\n            mps_backend_registered = hasattr(torch.backends, \"mps\")\n            torch_device = \"mps\" if (mps_backend_registered and torch.backends.mps.is_available()) else torch_device\n\n\ndef torch_all_close(a, b, *args, **kwargs):",
    "suffix": ""
  },
  {
    "name": "lxmusics/lx-music-api-server-python:common/log.py@650",
    "canonical_solution": "        if log_file:",
    "prompt": "import logging\nimport colorlog\nimport os\nfrom pygments import highlight\nfrom pygments.lexers import PythonLexer\nfrom pygments.formatters import TerminalFormatter\nfrom .utils import filterFileName, addToGlobalNamespace\nfrom .variable import debug_mode, log_length_limit, log_file\n# ----------------------------------------\n# - mode: python -\n# - author: helloplhm-qwq -\n# - name: log.py -\n# - project: lx-music-api-server -\n# - license: MIT -\n# ----------------------------------------\n# This file is part of the \"lx-music-api-server\" project.\n\n\nif ((not os.path.exists(\"logs\")) and log_file):\n    try:\n        os.mkdir(\"logs\")\n    except:\n        pass\n\n\ndef highlight_error(error):\n    # \u5bf9\u5806\u6808\u8ddf\u8e2a\u8fdb\u884c\u8bed\u6cd5\u9ad8\u4eae\n    highlighted_traceback = highlight(\n        error, PythonLexer(), TerminalFormatter())\n\n    # \u8fd4\u56de\u8bed\u6cd5\u9ad8\u4eae\u540e\u7684\u5806\u6808\u8ddf\u8e2a\u5b57\u7b26\u4e32\n    return str(highlighted_traceback)\n\n\nclass LogHelper(logging.Handler):\n    # \u65e5\u5fd7\u8f6c\u63a5\u5668\n    def __init__(self, custom_logger):\n        super().__init__()\n        self.custom_logger = custom_logger\n\n    def emit(self, record):\n        # print(record)\n        log_message = self.format(record)\n        self.custom_logger.info(log_message)\n\n\nclass log:\n    # \u4e3b\u7c7b\n    def __init__(self, module_name='Not named logger', output_level='INFO', filename=''):\n        self._logger = logging.getLogger(module_name)\n        if not output_level.upper() in dir(logging):\n            raise NameError('Unknown loglevel: '+output_level)\n        if not debug_mode:\n            self._logger.setLevel(getattr(logging, output_level.upper()))\n        else:\n            self._logger.setLevel(logging.DEBUG)\n        formatter = colorlog.ColoredFormatter(\n            '%(log_color)s%(asctime)s|[%(name)s/%(levelname)s]|%(message)s',\n            datefmt='%Y-%m-%d %H:%M:%S',\n            log_colors={\n                'DEBUG': 'cyan',\n                'INFO': 'white',\n                'WARNING': 'yellow',\n                'ERROR': 'red',\n                'CRITICAL': 'red,bg_white',\n            })",
    "prefix": "import logging\nimport colorlog\nimport os\nfrom pygments import highlight\nfrom pygments.lexers import PythonLexer\nfrom pygments.formatters import TerminalFormatter\nfrom .utils import filterFileName, addToGlobalNamespace\nfrom .variable import debug_mode, log_length_limit, log_file\n# ----------------------------------------\n# - mode: python -\n# - author: helloplhm-qwq -\n# - name: log.py -\n# - project: lx-music-api-server -\n# - license: MIT -\n# ----------------------------------------\n# This file is part of the \"lx-music-api-server\" project.\n\n\nif ((not os.path.exists(\"logs\")) and log_file):\n    try:\n        os.mkdir(\"logs\")\n    except:\n        pass\n\n\ndef highlight_error(error):\n    # \u5bf9\u5806\u6808\u8ddf\u8e2a\u8fdb\u884c\u8bed\u6cd5\u9ad8\u4eae\n    highlighted_traceback = highlight(\n        error, PythonLexer(), TerminalFormatter())\n\n    # \u8fd4\u56de\u8bed\u6cd5\u9ad8\u4eae\u540e\u7684\u5806\u6808\u8ddf\u8e2a\u5b57\u7b26\u4e32\n    return str(highlighted_traceback)\n\n\nclass LogHelper(logging.Handler):\n    # \u65e5\u5fd7\u8f6c\u63a5\u5668\n    def __init__(self, custom_logger):\n        super().__init__()\n        self.custom_logger = custom_logger\n\n    def emit(self, record):\n        # print(record)\n        log_message = self.format(record)\n        self.custom_logger.info(log_message)\n\n\nclass log:\n    # \u4e3b\u7c7b\n    def __init__(self, module_name='Not named logger', output_level='INFO', filename=''):\n        self._logger = logging.getLogger(module_name)\n        if not output_level.upper() in dir(logging):\n            raise NameError('Unknown loglevel: '+output_level)\n        if not debug_mode:\n            self._logger.setLevel(getattr(logging, output_level.upper()))\n        else:\n            self._logger.setLevel(logging.DEBUG)\n        formatter = colorlog.ColoredFormatter(\n            '%(log_color)s%(asctime)s|[%(name)s/%(levelname)s]|%(message)s',\n            datefmt='%Y-%m-%d %H:%M:%S',\n            log_colors={\n                'DEBUG': 'cyan',\n                'INFO': 'white',\n                'WARNING': 'yellow',\n                'ERROR': 'red',\n                'CRITICAL': 'red,bg_white',\n            })",
    "suffix": ""
  },
  {
    "name": "spfrommer/torchexplorer:torchexplorer/structure/structure.py@1328",
    "canonical_solution": "            gradfn: GradFn,",
    "prompt": "from torch.nn import Module\nfrom typing import Optional, Union\nfrom dataclasses import dataclass\nfrom loguru import logger\nfrom torchexplorer import utils\nfrom torchexplorer.core import GradFn, InvocationId, ModuleInvocationStructure\nimport re\nimport sys\nfrom __future__ import annotations\n\n\n\n\n\n\n\nconfig = {'handlers': [{ 'sink': sys.stderr, 'format': '{message}', 'level': 'DEBUG' }]}\nlogger.configure(**config)  # type: ignore\nlogger.disable(\"torchexplorer\")\n\n\n\nStructureNode = Union[ModuleInvocationStructure, str]\n\n\n@dataclass\nclass UpstreamStructureNode:\n    node: StructureNode\n    output_index: int\n\n    def __str__(self):\n        node_str = self.node if isinstance(self.node, str) else self.node.str_impl()\n        return f'(Upstream {node_str}):output {self.output_index}'\n\n\n@dataclass\nclass DownstreamStructureNode:\n    node: StructureNode\n    input_index: int\n    gradfn: GradFn\n\n    def __str__(self):\n        node_str = self.node if isinstance(self.node, str) else self.node.str_impl()\n        return f'(Downstream {node_str}):input {self.input_index}'\n\n\ndef extract_structure(\n        module: Module, invocation_id: InvocationId=0\n    ) -> ModuleInvocationStructure:\n    \"\"\"Module must have had hooks already added and one forward pass.\"\"\"\n\n    extractor = StructureExtractor(module, invocation_id)\n    return extractor.extract_structure()\n\n\nclass StructureExtractor:\n    def __init__(self, module: Module, invocation_id: InvocationId):\n        self.module = module\n        self.invocation_id = invocation_id\n        self.structure_id = 0\n        self.log_indent_level = 0\n\n    def log(self, message: str, extract_level: str, class_name: str):\n        ind = '| ' * self.log_indent_level\n        logger.opt(colors=True).debug(\n            f'{ind}<green>{extract_level}</green>:<blue>{class_name}</blue> {message}'\n        )\n\n    def extract_structure(self) -> ModuleInvocationStructure:\n        return self._extract_structure(self.module, self.invocation_id)\n\n    def _extract_structure(\n            self, module: Module, invocation_id: InvocationId\n        ) -> ModuleInvocationStructure:\n\n        self.log_indent_level += 1\n\n        log_args = ['OUTER', module.__class__.__name__]\n        self.log('Start extracting structure', *log_args)\n\n        input_n = len(module.torchexplorer_metadata.input_gradfns[invocation_id])\n        output_n = len(module.torchexplorer_metadata.output_gradfns[invocation_id])\n        structure = ModuleInvocationStructure(\n            module, invocation_id, self.structure_id, input_n, output_n\n        )\n        self.structure_id += 1\n        structure.upstreams_fetched = False\n\n\n        downstreams = []\n        for i, output_gradfn in utils.enum_not_none(_get_output_gradfns(structure)):\n            node_name = f'Output {i}'\n            downstreams.append(DownstreamStructureNode(\n                node=node_name, input_index=i, gradfn=output_gradfn\n            ))\n\n        i = 0\n        while i < len(downstreams):\n            downstream = downstreams[i]\n            i += 1\n\n            self.log(f'Processing downstream {downstream}', *log_args)\n            upstreams = self._inner_recurse(structure, downstream.gradfn)\n            self.log(f'Done inner recurse, got {len(upstreams)} upstreams', *log_args)\n\n            for upstream in upstreams:\n                assert structure.inner_graph.has_node(upstream.node)\n                assert structure.inner_graph.has_node(downstream.node)\n\n                structure.inner_graph.add_edge(\n                    upstream.node,\n                    downstream.node,\n                    upstream_output_index=upstream.output_index,\n                    downstream_input_index=downstream.input_index,\n                )\n\n                if is_input_node(upstream.node):\n                    continue\n\n                assert isinstance(upstream.node, ModuleInvocationStructure)\n                    \n                if not upstream.node.upstreams_fetched:\n                    self.log(f'Q {upstream.node.module.__class__.__name__}', *log_args)\n                    input_gradfns = _get_input_gradfns(upstream.node)\n                    for j, input_gradfn in utils.enum_not_none(input_gradfns):\n                        self.log(f'Queueing upstream {input_gradfn}', *log_args)\n                        downstreams.append(DownstreamStructureNode(\n                            node=upstream.node, input_index=j, gradfn=input_gradfn\n                        ))\n                    \n                    upstream.node.upstreams_fetched = True\n\n        self.log('Done extracting structure', *log_args)\n        self.log_indent_level -= 1\n        return structure\n\n    def _inner_recurse(\n            self,\n            current_struct: ModuleInvocationStructure,",
    "prefix": "from torch.nn import Module\nfrom typing import Optional, Union\nfrom dataclasses import dataclass\nfrom loguru import logger\nfrom torchexplorer import utils\nfrom torchexplorer.core import GradFn, InvocationId, ModuleInvocationStructure\nimport re\nimport sys\nfrom __future__ import annotations\n\n\n\n\n\n\n\nconfig = {'handlers': [{ 'sink': sys.stderr, 'format': '{message}', 'level': 'DEBUG' }]}\nlogger.configure(**config)  # type: ignore\nlogger.disable(\"torchexplorer\")\n\n\n\nStructureNode = Union[ModuleInvocationStructure, str]\n\n\n@dataclass\nclass UpstreamStructureNode:\n    node: StructureNode\n    output_index: int\n\n    def __str__(self):\n        node_str = self.node if isinstance(self.node, str) else self.node.str_impl()\n        return f'(Upstream {node_str}):output {self.output_index}'\n\n\n@dataclass\nclass DownstreamStructureNode:\n    node: StructureNode\n    input_index: int\n    gradfn: GradFn\n\n    def __str__(self):\n        node_str = self.node if isinstance(self.node, str) else self.node.str_impl()\n        return f'(Downstream {node_str}):input {self.input_index}'\n\n\ndef extract_structure(\n        module: Module, invocation_id: InvocationId=0\n    ) -> ModuleInvocationStructure:\n    \"\"\"Module must have had hooks already added and one forward pass.\"\"\"\n\n    extractor = StructureExtractor(module, invocation_id)\n    return extractor.extract_structure()\n\n\nclass StructureExtractor:\n    def __init__(self, module: Module, invocation_id: InvocationId):\n        self.module = module\n        self.invocation_id = invocation_id\n        self.structure_id = 0\n        self.log_indent_level = 0\n\n    def log(self, message: str, extract_level: str, class_name: str):\n        ind = '| ' * self.log_indent_level\n        logger.opt(colors=True).debug(\n            f'{ind}<green>{extract_level}</green>:<blue>{class_name}</blue> {message}'\n        )\n\n    def extract_structure(self) -> ModuleInvocationStructure:\n        return self._extract_structure(self.module, self.invocation_id)\n\n    def _extract_structure(\n            self, module: Module, invocation_id: InvocationId\n        ) -> ModuleInvocationStructure:\n\n        self.log_indent_level += 1\n\n        log_args = ['OUTER', module.__class__.__name__]\n        self.log('Start extracting structure', *log_args)\n\n        input_n = len(module.torchexplorer_metadata.input_gradfns[invocation_id])\n        output_n = len(module.torchexplorer_metadata.output_gradfns[invocation_id])\n        structure = ModuleInvocationStructure(\n            module, invocation_id, self.structure_id, input_n, output_n\n        )\n        self.structure_id += 1\n        structure.upstreams_fetched = False\n\n\n        downstreams = []\n        for i, output_gradfn in utils.enum_not_none(_get_output_gradfns(structure)):\n            node_name = f'Output {i}'\n            downstreams.append(DownstreamStructureNode(\n                node=node_name, input_index=i, gradfn=output_gradfn\n            ))\n\n        i = 0\n        while i < len(downstreams):\n            downstream = downstreams[i]\n            i += 1\n\n            self.log(f'Processing downstream {downstream}', *log_args)\n            upstreams = self._inner_recurse(structure, downstream.gradfn)\n            self.log(f'Done inner recurse, got {len(upstreams)} upstreams', *log_args)\n\n            for upstream in upstreams:\n                assert structure.inner_graph.has_node(upstream.node)\n                assert structure.inner_graph.has_node(downstream.node)\n\n                structure.inner_graph.add_edge(\n                    upstream.node,\n                    downstream.node,\n                    upstream_output_index=upstream.output_index,\n                    downstream_input_index=downstream.input_index,\n                )\n\n                if is_input_node(upstream.node):\n                    continue\n\n                assert isinstance(upstream.node, ModuleInvocationStructure)\n                    \n                if not upstream.node.upstreams_fetched:\n                    self.log(f'Q {upstream.node.module.__class__.__name__}', *log_args)\n                    input_gradfns = _get_input_gradfns(upstream.node)\n                    for j, input_gradfn in utils.enum_not_none(input_gradfns):\n                        self.log(f'Queueing upstream {input_gradfn}', *log_args)\n                        downstreams.append(DownstreamStructureNode(\n                            node=upstream.node, input_index=j, gradfn=input_gradfn\n                        ))\n                    \n                    upstream.node.upstreams_fetched = True\n\n        self.log('Done extracting structure', *log_args)\n        self.log_indent_level -= 1\n        return structure\n\n    def _inner_recurse(\n            self,\n            current_struct: ModuleInvocationStructure,",
    "suffix": ""
  },
  {
    "name": "namin/llm-verified-with-monte-carlo-tree-search:huggingface_generate.py@779",
    "canonical_solution": "            top_k=MODEL_ARG_TOP_K if MODEL_ARG_TOP_K is not None else 50 if num>1 and not SAME_FOR_MANY_SAMPLES else 7,",
    "prompt": "import torch\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\nfrom trl import AutoModelForCausalLMWithValueHead\nfrom peft import PeftModel\nfrom lang_config import STOP_WORD\nfrom model_config import BASE_MODEL_NAME, PEFT_MODEL_PATH, PPO_MODEL_PATH, CUSTOM_STOP, SAME_FOR_MANY_SAMPLES, BEAM_SEARCH, MODEL_ARG_TOP_K, MODEL_ARG_TOP_P, MODEL_ARG_TEMP\nfrom typing import List\n\n\ndef load_model(\n    base_model_name: str = BASE_MODEL_NAME,\n    ppo_model_path: str = PPO_MODEL_PATH,\n    peft_model_path: str = PEFT_MODEL_PATH,\n) -> (AutoModelForCausalLM, PeftModel, AutoTokenizer):\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,\n    )\n    if ppo_model_path is None:\n        base_model = AutoModelForCausalLM.from_pretrained(\n            base_model_name,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            trust_remote_code=True,\n            use_auth_token=True,\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            base_model_name, trust_remote_code=True\n        )\n    else:\n        base_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n            ppo_model_path, quantization_config=bnb_config\n        )\n        tokenizer = AutoTokenizer.from_pretrained(ppo_model_path)\n    tokenizer.pad_token = tokenizer.eos_token\n    model = (\n        PeftModel.from_pretrained(base_model, peft_model_path)\n        if peft_model_path\n        else base_model\n    )\n    return (base_model, model, tokenizer)\n\n\ndef stop_words_ids(tokenizer: AutoTokenizer) -> List[int]:\n    # Hack: we want the stop word as it is encoded glued to another word.\n    stop_word_id = tokenizer.encode(\"hello\" + STOP_WORD, add_special_tokens=False)[-1]\n    quote_word_id = tokenizer.encode(\"```\", add_special_tokens=False)[-1]\n    return [stop_word_id, quote_word_id]\n\n\ndef get_model_generation_token_args(\n        tokenizer: AutoTokenizer, custom_stop: bool = CUSTOM_STOP\n):\n    return dict(\n        min_length=5,\n        max_new_tokens=100,\n        eos_token_id=stop_words_ids(tokenizer)\n        if custom_stop\n        else tokenizer.eos_token_id,\n        pad_token_id=tokenizer.eos_token_id,\n    )\n\ndef get_model_generation_search_args(\n        num: int,\n        beam_search: bool = BEAM_SEARCH\n):\n    if beam_search:\n        return dict(\n            num_beams=num,\n            num_beam_groups=num,\n            diversity_penalty=0.9,\n        )\n    else:\n        return dict(",
    "prefix": "import torch\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\nfrom trl import AutoModelForCausalLMWithValueHead\nfrom peft import PeftModel\nfrom lang_config import STOP_WORD\nfrom model_config import BASE_MODEL_NAME, PEFT_MODEL_PATH, PPO_MODEL_PATH, CUSTOM_STOP, SAME_FOR_MANY_SAMPLES, BEAM_SEARCH, MODEL_ARG_TOP_K, MODEL_ARG_TOP_P, MODEL_ARG_TEMP\nfrom typing import List\n\n\ndef load_model(\n    base_model_name: str = BASE_MODEL_NAME,\n    ppo_model_path: str = PPO_MODEL_PATH,\n    peft_model_path: str = PEFT_MODEL_PATH,\n) -> (AutoModelForCausalLM, PeftModel, AutoTokenizer):\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,\n    )\n    if ppo_model_path is None:\n        base_model = AutoModelForCausalLM.from_pretrained(\n            base_model_name,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            trust_remote_code=True,\n            use_auth_token=True,\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            base_model_name, trust_remote_code=True\n        )\n    else:\n        base_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n            ppo_model_path, quantization_config=bnb_config\n        )\n        tokenizer = AutoTokenizer.from_pretrained(ppo_model_path)\n    tokenizer.pad_token = tokenizer.eos_token\n    model = (\n        PeftModel.from_pretrained(base_model, peft_model_path)\n        if peft_model_path\n        else base_model\n    )\n    return (base_model, model, tokenizer)\n\n\ndef stop_words_ids(tokenizer: AutoTokenizer) -> List[int]:\n    # Hack: we want the stop word as it is encoded glued to another word.\n    stop_word_id = tokenizer.encode(\"hello\" + STOP_WORD, add_special_tokens=False)[-1]\n    quote_word_id = tokenizer.encode(\"```\", add_special_tokens=False)[-1]\n    return [stop_word_id, quote_word_id]\n\n\ndef get_model_generation_token_args(\n        tokenizer: AutoTokenizer, custom_stop: bool = CUSTOM_STOP\n):\n    return dict(\n        min_length=5,\n        max_new_tokens=100,\n        eos_token_id=stop_words_ids(tokenizer)\n        if custom_stop\n        else tokenizer.eos_token_id,\n        pad_token_id=tokenizer.eos_token_id,\n    )\n\ndef get_model_generation_search_args(\n        num: int,\n        beam_search: bool = BEAM_SEARCH\n):\n    if beam_search:\n        return dict(\n            num_beams=num,\n            num_beam_groups=num,\n            diversity_penalty=0.9,\n        )\n    else:\n        return dict(",
    "suffix": ""
  },
  {
    "name": "BraveGroup/Drive-WM:src/diffusers/pipelines/stable_diffusion/pipeline_output.py@1166",
    "canonical_solution": "    class FlaxStableDiffusionPipelineOutput(BaseOutput):",
    "prompt": "from dataclasses import dataclass\nfrom typing import List, Optional, Union\nfrom ...utils import BaseOutput, is_flax_available\nimport numpy as np\nimport PIL.Image\n    import flax\n\n\n\n\n@dataclass\nclass StableDiffusionPipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for Stable Diffusion pipelines.\n\n    Args:\n        images (`List[PIL.Image.Image]` or `np.ndarray`)\n            List of denoised PIL images of length `batch_size` or NumPy array of shape `(batch_size, height, width,\n            num_channels)`.\n        nsfw_content_detected (`List[bool]`)\n            List indicating whether the corresponding generated image contains \"not-safe-for-work\" (nsfw) content or\n            `None` if safety checking could not be performed.\n    \"\"\"\n\n    images: Union[List[PIL.Image.Image], np.ndarray]\n    nsfw_content_detected: Optional[List[bool]]\n\n\nif is_flax_available():\n\n    @flax.struct.dataclass",
    "prefix": "from dataclasses import dataclass\nfrom typing import List, Optional, Union\nfrom ...utils import BaseOutput, is_flax_available\nimport numpy as np\nimport PIL.Image\n    import flax\n\n\n\n\n@dataclass\nclass StableDiffusionPipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for Stable Diffusion pipelines.\n\n    Args:\n        images (`List[PIL.Image.Image]` or `np.ndarray`)\n            List of denoised PIL images of length `batch_size` or NumPy array of shape `(batch_size, height, width,\n            num_channels)`.\n        nsfw_content_detected (`List[bool]`)\n            List indicating whether the corresponding generated image contains \"not-safe-for-work\" (nsfw) content or\n            `None` if safety checking could not be performed.\n    \"\"\"\n\n    images: Union[List[PIL.Image.Image], np.ndarray]\n    nsfw_content_detected: Optional[List[bool]]\n\n\nif is_flax_available():\n\n    @flax.struct.dataclass",
    "suffix": ""
  },
  {
    "name": "basnijholt/unidep:tests/test_utils.py@1520",
    "canonical_solution": "    assert escape_unicode(\"foo\\\\t\") == \"foo\\t\"",
    "prompt": "import sys\nimport pytest\nfrom unittest.mock import patch\nfrom unidep.platform_definitions import Selector\nfrom unidep.utils import (\n    UnsupportedPlatformError,\n    build_pep508_environment_marker,\n    escape_unicode,\n    extract_matching_platforms,\n    identify_current_platform,\n    parse_package_str,\n)\n    from typing import get_args\n    from typing_extensions import get_args\n\"\"\"Tests for the unidep.utils module.\"\"\"\nfrom __future__ import annotations\n\n\n\n\nif sys.version_info >= (3, 8):\nelse:  # pragma: no cover\n\n\ndef test_escape_unicode() -> None:\n    assert escape_unicode(\"foo\\\\n\") == \"foo\\n\"",
    "prefix": "import sys\nimport pytest\nfrom unittest.mock import patch\nfrom unidep.platform_definitions import Selector\nfrom unidep.utils import (\n    UnsupportedPlatformError,\n    build_pep508_environment_marker,\n    escape_unicode,\n    extract_matching_platforms,\n    identify_current_platform,\n    parse_package_str,\n)\n    from typing import get_args\n    from typing_extensions import get_args\n\"\"\"Tests for the unidep.utils module.\"\"\"\nfrom __future__ import annotations\n\n\n\n\nif sys.version_info >= (3, 8):\nelse:  # pragma: no cover\n\n\ndef test_escape_unicode() -> None:\n    assert escape_unicode(\"foo\\\\n\") == \"foo\\n\"",
    "suffix": ""
  },
  {
    "name": "BAAI-DCAI/SegVol:segment_anything_volumetric/modeling/image_encoder.py@1247",
    "canonical_solution": "            LayerNorm2d(out_chans),",
    "prompt": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple, Type\nfrom .common import LayerNorm2d, MLPBlock\nfrom monai.networks.blocks import PatchEmbed\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n\n\n\n# This class and its supporting functions below lightly adapted from the ViTDet backbone available at: https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/vit.py # noqa\nclass ImageEncoderViT(nn.Module):\n    def __init__(\n        self,\n        img_size: int = 1024,\n        patch_size: int = 16,\n        in_chans: int = 1,\n        embed_dim: int = 768,\n        depth: int = 12,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0,\n        out_chans: int = 256,\n        qkv_bias: bool = True,\n        norm_layer: Type[nn.Module] = nn.LayerNorm,\n        act_layer: Type[nn.Module] = nn.GELU,\n        use_abs_pos: bool = True,\n        use_rel_pos: bool = False,\n        rel_pos_zero_init: bool = True,\n        window_size: int = 0,\n        global_attn_indexes: Tuple[int, ...] = (),\n    ) -> None:\n        \"\"\"\n        Args:\n            img_size (int): Input image size.\n            patch_size (int): Patch size.\n            in_chans (int): Number of input image channels.\n            embed_dim (int): Patch embedding dimension.\n            depth (int): Depth of ViT.\n            num_heads (int): Number of attention heads in each ViT block.\n            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n            norm_layer (nn.Module): Normalization layer.\n            act_layer (nn.Module): Activation layer.\n            use_abs_pos (bool): If True, use absolute positional embeddings.\n            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            window_size (int): Window size for window attention blocks.\n            global_attn_indexes (list): Indexes for blocks using global attention.\n        \"\"\"\n        super().__init__()\n        self.img_size = img_size\n\n        # self.patch_embed = PatchEmbed(\n        #     kernel_size=(patch_size, patch_size),\n        #     stride=(patch_size, patch_size),\n        #     in_chans=in_chans,\n        #     embed_dim=embed_dim,\n        # )\n\n        self.patch_embed = PatchEmbed(\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            spatial_dims=3,\n        )\n\n        self.pos_embed: Optional[nn.Parameter] = None\n        if use_abs_pos:\n            # Initialize absolute positional embedding with pretrain image size.\n            self.pos_embed = nn.Parameter(\n                torch.zeros(1, img_size // patch_size, img_size // patch_size, img_size // patch_size, embed_dim)\n            )\n\n        self.blocks = nn.ModuleList()\n        for i in range(depth):\n            block = Block(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n                use_rel_pos=use_rel_pos,\n                rel_pos_zero_init=rel_pos_zero_init,\n                window_size=window_size if i not in global_attn_indexes else 0,\n                input_size=(img_size // patch_size, img_size // patch_size),\n            )\n            self.blocks.append(block)\n\n        self.neck = nn.Sequential(\n            nn.Conv2d(\n                embed_dim,\n                out_chans,\n                kernel_size=1,\n                bias=False,\n            ),\n            LayerNorm2d(out_chans),\n            nn.Conv2d(\n                out_chans,\n                out_chans,\n                kernel_size=3,\n                padding=1,\n                bias=False,\n            ),",
    "prefix": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple, Type\nfrom .common import LayerNorm2d, MLPBlock\nfrom monai.networks.blocks import PatchEmbed\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n\n\n\n# This class and its supporting functions below lightly adapted from the ViTDet backbone available at: https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/vit.py # noqa\nclass ImageEncoderViT(nn.Module):\n    def __init__(\n        self,\n        img_size: int = 1024,\n        patch_size: int = 16,\n        in_chans: int = 1,\n        embed_dim: int = 768,\n        depth: int = 12,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0,\n        out_chans: int = 256,\n        qkv_bias: bool = True,\n        norm_layer: Type[nn.Module] = nn.LayerNorm,\n        act_layer: Type[nn.Module] = nn.GELU,\n        use_abs_pos: bool = True,\n        use_rel_pos: bool = False,\n        rel_pos_zero_init: bool = True,\n        window_size: int = 0,\n        global_attn_indexes: Tuple[int, ...] = (),\n    ) -> None:\n        \"\"\"\n        Args:\n            img_size (int): Input image size.\n            patch_size (int): Patch size.\n            in_chans (int): Number of input image channels.\n            embed_dim (int): Patch embedding dimension.\n            depth (int): Depth of ViT.\n            num_heads (int): Number of attention heads in each ViT block.\n            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n            norm_layer (nn.Module): Normalization layer.\n            act_layer (nn.Module): Activation layer.\n            use_abs_pos (bool): If True, use absolute positional embeddings.\n            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            window_size (int): Window size for window attention blocks.\n            global_attn_indexes (list): Indexes for blocks using global attention.\n        \"\"\"\n        super().__init__()\n        self.img_size = img_size\n\n        # self.patch_embed = PatchEmbed(\n        #     kernel_size=(patch_size, patch_size),\n        #     stride=(patch_size, patch_size),\n        #     in_chans=in_chans,\n        #     embed_dim=embed_dim,\n        # )\n\n        self.patch_embed = PatchEmbed(\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            spatial_dims=3,\n        )\n\n        self.pos_embed: Optional[nn.Parameter] = None\n        if use_abs_pos:\n            # Initialize absolute positional embedding with pretrain image size.\n            self.pos_embed = nn.Parameter(\n                torch.zeros(1, img_size // patch_size, img_size // patch_size, img_size // patch_size, embed_dim)\n            )\n\n        self.blocks = nn.ModuleList()\n        for i in range(depth):\n            block = Block(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n                use_rel_pos=use_rel_pos,\n                rel_pos_zero_init=rel_pos_zero_init,\n                window_size=window_size if i not in global_attn_indexes else 0,\n                input_size=(img_size // patch_size, img_size // patch_size),\n            )\n            self.blocks.append(block)\n\n        self.neck = nn.Sequential(\n            nn.Conv2d(\n                embed_dim,\n                out_chans,\n                kernel_size=1,\n                bias=False,\n            ),\n            LayerNorm2d(out_chans),\n            nn.Conv2d(\n                out_chans,\n                out_chans,\n                kernel_size=3,\n                padding=1,\n                bias=False,\n            ),",
    "suffix": ""
  },
  {
    "name": "fjzzq2002/is-my-problem-new:src/scrapper/codeforces.py@1008",
    "canonical_solution": "dump_json_safe(scrapped_problems, \"problems/codeforces.json\")",
    "prompt": "from ..utils import read_problems, dump_json_safe, get_text\nfrom bs4 import BeautifulSoup\nfrom tqdm.auto import tqdm\nimport json\nimport os\nimport requests\nimport time\n    import random\n\n\nscrapped_problems = []\ntry:\n    scrapped_problems = read_problems(\"problems/codeforces.json\")\n    print(f\"Recalled {len(scrapped_problems)} scrapped problems\")\nexcept:\n    print(\"Cannot find scrapped problems\")\nscrapped_uids = set(p[\"uid\"] for p in scrapped_problems)\n\ncodeforces_endpoint = \"https://codeforces.com/api/problemset.problems\"\n# get list of problems\nlist_problems = requests.get(codeforces_endpoint).json()[\"result\"][\"problems\"]\n# the website is down, read problems.txt instead\n# with open('problems.txt') as f:\n#     list_problems = json.load(f)['result']['problems']\nprint(\"# problems:\", len(list_problems))\n\n\n# a scrapper for codeforces\ndef scrap_problem(contestId, index, rating, tags, uid):\n    url = f\"https://codeforces.com/contest/{contestId}/problem/{index}\"\n    response = requests.get(url, timeout=30)\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    statement = soup.find(class_=\"problem-statement\")\n    try:\n        statement.find(class_=\"header\").decompose()\n    except:\n        pass\n    statement_body = statement.find(\"div\")\n    statement_body = get_text(statement_body)\n    # \\r -> \\n, remove duplicate \\n, strip\n    statement_body = (\n        statement_body.replace(\"\\r\", \"\\n\")\n        .replace(\"\\n\\n\", \"\\n\")\n        .replace(\"$$$\", \"$\")\n        .strip()\n    )\n    problem = {\n        \"uid\": uid,\n        \"url\": url,\n        \"tags\": tags,\n        #        'raw': str(response.content),\n        \"statement\": statement_body,\n        \"contestId\": contestId,\n        \"index\": index,\n        \"rating\": rating,\n    }\n    return problem\n\n\nfor problem in tqdm(list_problems):\n    contestId, index, rating, tags = (\n        problem[\"contestId\"],\n        problem[\"index\"],\n        problem.get(\"rating\", -1),\n        problem[\"tags\"],\n    )\n    uid = f\"Codeforces{contestId}{index}\"\n    if uid in scrapped_uids:\n        continue\n    print(f\"Scrapping {uid}\")\n    result = None\n    try:\n        result = scrap_problem(contestId, index, rating, tags, uid)\n    except Exception as e:\n        print(\"Error while scrapping:\", e)\n    if result is not None:\n        scrapped_problems.append(result)\n    time.sleep(0.1)\n    # save to file every 10 problems\n\n    if random.random() < 0.1:\n        dump_json_safe(scrapped_problems, \"problems/codeforces.json\")\n",
    "prefix": "from ..utils import read_problems, dump_json_safe, get_text\nfrom bs4 import BeautifulSoup\nfrom tqdm.auto import tqdm\nimport json\nimport os\nimport requests\nimport time\n    import random\n\n\nscrapped_problems = []\ntry:\n    scrapped_problems = read_problems(\"problems/codeforces.json\")\n    print(f\"Recalled {len(scrapped_problems)} scrapped problems\")\nexcept:\n    print(\"Cannot find scrapped problems\")\nscrapped_uids = set(p[\"uid\"] for p in scrapped_problems)\n\ncodeforces_endpoint = \"https://codeforces.com/api/problemset.problems\"\n# get list of problems\nlist_problems = requests.get(codeforces_endpoint).json()[\"result\"][\"problems\"]\n# the website is down, read problems.txt instead\n# with open('problems.txt') as f:\n#     list_problems = json.load(f)['result']['problems']\nprint(\"# problems:\", len(list_problems))\n\n\n# a scrapper for codeforces\ndef scrap_problem(contestId, index, rating, tags, uid):\n    url = f\"https://codeforces.com/contest/{contestId}/problem/{index}\"\n    response = requests.get(url, timeout=30)\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    statement = soup.find(class_=\"problem-statement\")\n    try:\n        statement.find(class_=\"header\").decompose()\n    except:\n        pass\n    statement_body = statement.find(\"div\")\n    statement_body = get_text(statement_body)\n    # \\r -> \\n, remove duplicate \\n, strip\n    statement_body = (\n        statement_body.replace(\"\\r\", \"\\n\")\n        .replace(\"\\n\\n\", \"\\n\")\n        .replace(\"$$$\", \"$\")\n        .strip()\n    )\n    problem = {\n        \"uid\": uid,\n        \"url\": url,\n        \"tags\": tags,\n        #        'raw': str(response.content),\n        \"statement\": statement_body,\n        \"contestId\": contestId,\n        \"index\": index,\n        \"rating\": rating,\n    }\n    return problem\n\n\nfor problem in tqdm(list_problems):\n    contestId, index, rating, tags = (\n        problem[\"contestId\"],\n        problem[\"index\"],\n        problem.get(\"rating\", -1),\n        problem[\"tags\"],\n    )\n    uid = f\"Codeforces{contestId}{index}\"\n    if uid in scrapped_uids:\n        continue\n    print(f\"Scrapping {uid}\")\n    result = None\n    try:\n        result = scrap_problem(contestId, index, rating, tags, uid)\n    except Exception as e:\n        print(\"Error while scrapping:\", e)\n    if result is not None:\n        scrapped_problems.append(result)\n    time.sleep(0.1)\n    # save to file every 10 problems\n\n    if random.random() < 0.1:\n        dump_json_safe(scrapped_problems, \"problems/codeforces.json\")\n",
    "suffix": ""
  },
  {
    "name": "p0p4k/pflowtts_pytorch:pflow/hifigan/models.py@1214",
    "canonical_solution": "        self.ups.apply(init_weights)",
    "prompt": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import AvgPool1d, Conv1d, Conv2d, ConvTranspose1d\nfrom torch.nn.utils import remove_weight_norm, spectral_norm, weight_norm\nfrom .xutils import get_padding, init_weights\n\"\"\" from https://github.com/jik876/hifi-gan \"\"\"\n\n\n\nLRELU_SLOPE = 0.1\n\n\nclass ResBlock1(torch.nn.Module):\n    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super().__init__()\n        self.h = h\n        self.convs1 = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[0],\n                        padding=get_padding(kernel_size, dilation[0]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[1],\n                        padding=get_padding(kernel_size, dilation[1]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[2],\n                        padding=get_padding(kernel_size, dilation[2]),\n                    )\n                ),\n            ]\n        )\n        self.convs1.apply(init_weights)\n\n        self.convs2 = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=1,\n                        padding=get_padding(kernel_size, 1),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=1,\n                        padding=get_padding(kernel_size, 1),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=1,\n                        padding=get_padding(kernel_size, 1),\n                    )\n                ),\n            ]\n        )\n        self.convs2.apply(init_weights)\n\n    def forward(self, x):\n        for c1, c2 in zip(self.convs1, self.convs2):\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            xt = c1(xt)\n            xt = F.leaky_relu(xt, LRELU_SLOPE)\n            xt = c2(xt)\n            x = xt + x\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs1:\n            remove_weight_norm(l)\n        for l in self.convs2:\n            remove_weight_norm(l)\n\n\nclass ResBlock2(torch.nn.Module):\n    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3)):\n        super().__init__()\n        self.h = h\n        self.convs = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[0],\n                        padding=get_padding(kernel_size, dilation[0]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[1],\n                        padding=get_padding(kernel_size, dilation[1]),\n                    )\n                ),\n            ]\n        )\n        self.convs.apply(init_weights)\n\n    def forward(self, x):\n        for c in self.convs:\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            xt = c(xt)\n            x = xt + x\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs:\n            remove_weight_norm(l)\n\n\nclass Generator(torch.nn.Module):\n    def __init__(self, h):\n        super().__init__()\n        self.h = h\n        self.num_kernels = len(h.resblock_kernel_sizes)\n        self.num_upsamples = len(h.upsample_rates)\n        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n        resblock = ResBlock1 if h.resblock == \"1\" else ResBlock2\n\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n            self.ups.append(\n                weight_norm(\n                    ConvTranspose1d(\n                        h.upsample_initial_channel // (2**i),\n                        h.upsample_initial_channel // (2 ** (i + 1)),\n                        k,\n                        u,\n                        padding=(k - u) // 2,\n                    )\n                )\n            )\n\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = h.upsample_initial_channel // (2 ** (i + 1))\n            for _, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n                self.resblocks.append(resblock(h, ch, k, d))\n\n        self.conv_post = weight_norm(Conv1d(ch, 1, 7, 1, padding=3))",
    "prefix": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import AvgPool1d, Conv1d, Conv2d, ConvTranspose1d\nfrom torch.nn.utils import remove_weight_norm, spectral_norm, weight_norm\nfrom .xutils import get_padding, init_weights\n\"\"\" from https://github.com/jik876/hifi-gan \"\"\"\n\n\n\nLRELU_SLOPE = 0.1\n\n\nclass ResBlock1(torch.nn.Module):\n    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super().__init__()\n        self.h = h\n        self.convs1 = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[0],\n                        padding=get_padding(kernel_size, dilation[0]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[1],\n                        padding=get_padding(kernel_size, dilation[1]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[2],\n                        padding=get_padding(kernel_size, dilation[2]),\n                    )\n                ),\n            ]\n        )\n        self.convs1.apply(init_weights)\n\n        self.convs2 = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=1,\n                        padding=get_padding(kernel_size, 1),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=1,\n                        padding=get_padding(kernel_size, 1),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=1,\n                        padding=get_padding(kernel_size, 1),\n                    )\n                ),\n            ]\n        )\n        self.convs2.apply(init_weights)\n\n    def forward(self, x):\n        for c1, c2 in zip(self.convs1, self.convs2):\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            xt = c1(xt)\n            xt = F.leaky_relu(xt, LRELU_SLOPE)\n            xt = c2(xt)\n            x = xt + x\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs1:\n            remove_weight_norm(l)\n        for l in self.convs2:\n            remove_weight_norm(l)\n\n\nclass ResBlock2(torch.nn.Module):\n    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3)):\n        super().__init__()\n        self.h = h\n        self.convs = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[0],\n                        padding=get_padding(kernel_size, dilation[0]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[1],\n                        padding=get_padding(kernel_size, dilation[1]),\n                    )\n                ),\n            ]\n        )\n        self.convs.apply(init_weights)\n\n    def forward(self, x):\n        for c in self.convs:\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            xt = c(xt)\n            x = xt + x\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs:\n            remove_weight_norm(l)\n\n\nclass Generator(torch.nn.Module):\n    def __init__(self, h):\n        super().__init__()\n        self.h = h\n        self.num_kernels = len(h.resblock_kernel_sizes)\n        self.num_upsamples = len(h.upsample_rates)\n        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n        resblock = ResBlock1 if h.resblock == \"1\" else ResBlock2\n\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n            self.ups.append(\n                weight_norm(\n                    ConvTranspose1d(\n                        h.upsample_initial_channel // (2**i),\n                        h.upsample_initial_channel // (2 ** (i + 1)),\n                        k,\n                        u,\n                        padding=(k - u) // 2,\n                    )\n                )\n            )\n\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = h.upsample_initial_channel // (2 ** (i + 1))\n            for _, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n                self.resblocks.append(resblock(h, ch, k, d))\n\n        self.conv_post = weight_norm(Conv1d(ch, 1, 7, 1, padding=3))",
    "suffix": ""
  },
  {
    "name": "theroyallab/tabbyAPI:OAI/utils_oai.py@1233",
    "canonical_solution": "        usage=UsageStats(",
    "prompt": "import pathlib\nfrom typing import Optional\nfrom OAI.types.chat_completion import (\n    ChatCompletionMessage,\n    ChatCompletionRespChoice,\n    ChatCompletionStreamChunk,\n    ChatCompletionResponse,\n    ChatCompletionStreamChoice,\n)\nfrom OAI.types.completion import CompletionResponse, CompletionRespChoice\nfrom OAI.types.common import UsageStats\nfrom OAI.types.lora import LoraList, LoraCard\nfrom OAI.types.model import ModelList, ModelCard\nfrom utils import unwrap\n\"\"\" Utility functions for the OpenAI server. \"\"\"\n\n\n\n\ndef create_completion_response(\n    text: str,\n    prompt_tokens: int,\n    completion_tokens: int,\n    model_name: Optional[str],\n):\n    \"\"\"Create a completion response from the provided text.\"\"\"\n    choice = CompletionRespChoice(finish_reason=\"Generated\", text=text)\n\n    response = CompletionResponse(\n        choices=[choice],\n        model=unwrap(model_name, \"\"),\n        usage=UsageStats(\n            prompt_tokens=prompt_tokens,\n            completion_tokens=completion_tokens,\n            total_tokens=prompt_tokens + completion_tokens,\n        ),\n    )\n\n    return response\n\n\ndef create_chat_completion_response(\n    text: str,\n    prompt_tokens: int,\n    completion_tokens: int,\n    model_name: Optional[str],\n):\n    \"\"\"Create a chat completion response from the provided text.\"\"\"\n    message = ChatCompletionMessage(role=\"assistant\", content=text)\n\n    choice = ChatCompletionRespChoice(finish_reason=\"Generated\", message=message)\n\n    response = ChatCompletionResponse(\n        choices=[choice],\n        model=unwrap(model_name, \"\"),",
    "prefix": "import pathlib\nfrom typing import Optional\nfrom OAI.types.chat_completion import (\n    ChatCompletionMessage,\n    ChatCompletionRespChoice,\n    ChatCompletionStreamChunk,\n    ChatCompletionResponse,\n    ChatCompletionStreamChoice,\n)\nfrom OAI.types.completion import CompletionResponse, CompletionRespChoice\nfrom OAI.types.common import UsageStats\nfrom OAI.types.lora import LoraList, LoraCard\nfrom OAI.types.model import ModelList, ModelCard\nfrom utils import unwrap\n\"\"\" Utility functions for the OpenAI server. \"\"\"\n\n\n\n\ndef create_completion_response(\n    text: str,\n    prompt_tokens: int,\n    completion_tokens: int,\n    model_name: Optional[str],\n):\n    \"\"\"Create a completion response from the provided text.\"\"\"\n    choice = CompletionRespChoice(finish_reason=\"Generated\", text=text)\n\n    response = CompletionResponse(\n        choices=[choice],\n        model=unwrap(model_name, \"\"),\n        usage=UsageStats(\n            prompt_tokens=prompt_tokens,\n            completion_tokens=completion_tokens,\n            total_tokens=prompt_tokens + completion_tokens,\n        ),\n    )\n\n    return response\n\n\ndef create_chat_completion_response(\n    text: str,\n    prompt_tokens: int,\n    completion_tokens: int,\n    model_name: Optional[str],\n):\n    \"\"\"Create a chat completion response from the provided text.\"\"\"\n    message = ChatCompletionMessage(role=\"assistant\", content=text)\n\n    choice = ChatCompletionRespChoice(finish_reason=\"Generated\", message=message)\n\n    response = ChatCompletionResponse(\n        choices=[choice],\n        model=unwrap(model_name, \"\"),",
    "suffix": ""
  },
  {
    "name": "zorazrw/filco:replace_context.py@1029",
    "canonical_solution": "        sent_ctx = prefix_format.format(CONTEXT_PREFIX, output_text)",
    "prompt": "import argparse\nfrom typing import Dict, List\nfrom get_inputs import get_answer, prefix_format, CONTEXT_PREFIX\nfrom utils import load_dataset, write_dataset\n\"\"\"Replace the 'context' in the input with 'sent' in the output. \"\"\"\n\n\ndef main():\n    dataset = load_dataset(args.dataset_path)\n    predset = load_dataset(args.predset_path)\n    # assert len(dataset) == len(predset)\n    N = len(predset)\n\n    def get_input_text(input_text: str, output_text: str) -> str: \n        prefix, text = input_text.split(\"\\n\\n\")\n        prefix = prefix.replace(\"['question', 'context']\", \"['context', 'question']\")\n        prefix = prefix.replace(\"most helpful sentence\", \"answer to the question\")\n        question, context = text.split(\"\\ncontext:\")\n        question = question.rstrip()\n        sent_ctx = prefix_format.format(CONTEXT_PREFIX, output_text)\n        return prefix + '\\n\\n' + sent_ctx + '\\n' + question\n\n    def get_input_text_wow(input_text: str, output_text: str) -> str:\n        prefix, text = input_text.split(\"\\n\\n\")\n        prefix = prefix.replace(\"question\", \"query\")\n        prefix = prefix.replace(\"['query', 'context']\", \"['context', 'query']\")\n        prefix = prefix.replace(\"most helpful sentence\", \"response to the conversation\")\n        question, context = text.split(\"context:\")\n        question = question.rstrip()\n        context = \"context:\" + context",
    "prefix": "import argparse\nfrom typing import Dict, List\nfrom get_inputs import get_answer, prefix_format, CONTEXT_PREFIX\nfrom utils import load_dataset, write_dataset\n\"\"\"Replace the 'context' in the input with 'sent' in the output. \"\"\"\n\n\ndef main():\n    dataset = load_dataset(args.dataset_path)\n    predset = load_dataset(args.predset_path)\n    # assert len(dataset) == len(predset)\n    N = len(predset)\n\n    def get_input_text(input_text: str, output_text: str) -> str: \n        prefix, text = input_text.split(\"\\n\\n\")\n        prefix = prefix.replace(\"['question', 'context']\", \"['context', 'question']\")\n        prefix = prefix.replace(\"most helpful sentence\", \"answer to the question\")\n        question, context = text.split(\"\\ncontext:\")\n        question = question.rstrip()\n        sent_ctx = prefix_format.format(CONTEXT_PREFIX, output_text)\n        return prefix + '\\n\\n' + sent_ctx + '\\n' + question\n\n    def get_input_text_wow(input_text: str, output_text: str) -> str:\n        prefix, text = input_text.split(\"\\n\\n\")\n        prefix = prefix.replace(\"question\", \"query\")\n        prefix = prefix.replace(\"['query', 'context']\", \"['context', 'query']\")\n        prefix = prefix.replace(\"most helpful sentence\", \"response to the conversation\")\n        question, context = text.split(\"context:\")\n        question = question.rstrip()\n        context = \"context:\" + context",
    "suffix": ""
  },
  {
    "name": "OliverMao/FlaskAutoApiBuilder:demo/utils/file.py@643",
    "canonical_solution": "    return utils.upload(file=file, app=app, path=path)",
    "prompt": "from Faab.Faab import faab\nfrom Faab import utils\nfrom werkzeug.datastructures.file_storage import FileStorage\n\n\ndef compress_image(file: FileStorage, app: faab, path: str = None, quality: int = 100, format_: str = None) -> str:\n    \"\"\"\n    Compresses an image during upload.\n\n    Args:\n        file (str): The image file to be uploaded.\n        app (str): The application name.\n        path (str, optional): The path where the compressed image should be stored. If not provided, the image will be stored in the 'static' folder of the Faab startup folder.\n        format_ (str, optional): The desired format of the compressed image. Valid values are 'PNG', 'JPEG', and 'WEBP'.\n        quality (int, optional): The quality of the compressed image. Range is 1-100, with higher values indicating higher quality. Defaults to 100.\n\n    Returns:\n        str: The URL of the compressed image.\n\n    \"\"\"\n    return utils.faab_compress_image(file=file, path=path, app=app, format_=format_, quality=quality)\n\n\ndef upload(file: FileStorage, app: faab, path: str = None) -> str:\n    \"\"\"\n    Uploads a file to a specified path.\n\n    Args:\n        file: The file to be uploaded.\n        path (str, optional): The path where the file should be saved. If not provided, the file will be saved in the 'static' folder.\n        app (object, optional): The application object. If not provided, the 'app' parameter must be set.\n\n    Returns:\n        str: The file path of the uploaded file.\n\n    Raises:\n        ValueError: If the file is not selected or if 'app' is not delivered.\n\n    \"\"\"",
    "prefix": "from Faab.Faab import faab\nfrom Faab import utils\nfrom werkzeug.datastructures.file_storage import FileStorage\n\n\ndef compress_image(file: FileStorage, app: faab, path: str = None, quality: int = 100, format_: str = None) -> str:\n    \"\"\"\n    Compresses an image during upload.\n\n    Args:\n        file (str): The image file to be uploaded.\n        app (str): The application name.\n        path (str, optional): The path where the compressed image should be stored. If not provided, the image will be stored in the 'static' folder of the Faab startup folder.\n        format_ (str, optional): The desired format of the compressed image. Valid values are 'PNG', 'JPEG', and 'WEBP'.\n        quality (int, optional): The quality of the compressed image. Range is 1-100, with higher values indicating higher quality. Defaults to 100.\n\n    Returns:\n        str: The URL of the compressed image.\n\n    \"\"\"\n    return utils.faab_compress_image(file=file, path=path, app=app, format_=format_, quality=quality)\n\n\ndef upload(file: FileStorage, app: faab, path: str = None) -> str:\n    \"\"\"\n    Uploads a file to a specified path.\n\n    Args:\n        file: The file to be uploaded.\n        path (str, optional): The path where the file should be saved. If not provided, the file will be saved in the 'static' folder.\n        app (object, optional): The application object. If not provided, the 'app' parameter must be set.\n\n    Returns:\n        str: The file path of the uploaded file.\n\n    Raises:\n        ValueError: If the file is not selected or if 'app' is not delivered.\n\n    \"\"\"",
    "suffix": ""
  },
  {
    "name": "leeyuentuen/polestar_api:custom_components/polestar_api/pypolestar/auth.py@1129",
    "canonical_solution": "        raise PolestarAuthException(\"Error getting resume path \", result.status_code)",
    "prompt": "from datetime import datetime, timedelta\nfrom .const import HTTPX_TIMEOUT\nfrom .exception import PolestarAuthException\nimport json\nimport logging\nimport httpx\n\n\n\n_LOGGER = logging.getLogger(__name__)\n\n\nclass PolestarAuth:\n    \"\"\"base class for Polestar authentication.\"\"\"\n\n    def __init__(self, username: str, password: str) -> None:\n        \"\"\"Initialize the Polestar authentication.\"\"\"\n        self.username = username\n        self.password = password\n        self.access_token = None\n        self.refresh_token = None\n        self.token_expiry = None\n        self.latest_call_code = None\n        self._client_session = httpx.AsyncClient()\n\n    async def get_token(self, refresh=False) -> None:\n        \"\"\"Get the token from Polestar.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        operationName = \"getAuthToken\"\n        if not refresh:\n            code = await self._get_code()\n            if code is None:\n                return\n            params = {\n                \"query\": \"query getAuthToken($code: String!) { getAuthToken(code: $code) { id_token access_token refresh_token expires_in }}\",\n                \"operationName\": operationName,\n                \"variables\": json.dumps({\"code\": code}),\n            }\n        else:\n            if self.refresh_token is None:\n                return\n            token = self.refresh_token\n            operationName = \"refreshAuthToken\"\n            headers[\"Authorization\"] = f\"Bearer {self.access_token}\"\n\n            params = {\n                \"query\": \"query refreshAuthToken($token: String!) { refreshAuthToken(token: $token) { id_token access_token refresh_token expires_in }}\",\n                \"operationName\": operationName,\n                \"variables\": json.dumps({\"token\": token}),\n            }\n        result = await self._client_session.get(\"https://pc-api.polestar.com/eu-north-1/auth/\", params=params, headers=headers, timeout=HTTPX_TIMEOUT)\n        self.latest_call_code = result.status_code\n        resultData = result.json()\n        if result.status_code != 200 or (\"errors\" in resultData and len(resultData[\"errors\"])):\n            _LOGGER.error(result)\n            raise PolestarAuthException(\"Error getting token\", result.status_code)\n        _LOGGER.debug(resultData)\n\n        if resultData['data']:\n            self.access_token = resultData['data'][operationName]['access_token']\n            self.refresh_token = resultData['data'][operationName]['refresh_token']\n            self.token_expiry = datetime.now(\n            ) + timedelta(seconds=resultData['data'][operationName]['expires_in'])\n            # ID Token\n\n        _LOGGER.debug(f\"Response {self.access_token}\")\n\n    async def _get_code(self) -> None:\n        query_params = await self._get_resume_path()\n\n        # check if code is in query_params\n        if query_params.get('code'):\n            return query_params.get('code')\n\n        # get the resumePath\n        if query_params.get('resumePath'):\n            resumePath = query_params.get('resumePath')\n\n        if resumePath is None:\n            return\n\n        params = {\n            'client_id': 'polmystar'\n        }\n        data = {\n            'pf.username': self.username,\n            'pf.pass': self.password\n        }\n        result = await self._client_session.post(\n            f\"https://polestarid.eu.polestar.com/as/{resumePath}/resume/as/authorization.ping\",\n            params=params,\n            data=data\n        )\n        self.latest_call_code = result.status_code\n        if result.status_code != 302:\n            raise PolestarAuthException(\"Error getting code\", result.status_code)\n\n        # get the realUrl\n        url = result.url\n        code = result.next_request.url.params.get('code')\n\n        # sign-in-callback\n        result = await self._client_session.get(result.next_request.url, timeout=HTTPX_TIMEOUT)\n        self.latest_call_code = result.status_code\n\n        if result.status_code != 200:\n            _LOGGER.error(result)\n            raise PolestarAuthException(\"Error getting code callback\", result.status_code)\n\n        # url encode the code\n        result = await self._client_session.get(url)\n        self.latest_call_code = result.status_code\n\n        return code\n\n    async def _get_resume_path(self):\n        \"\"\"Get Resume Path from Polestar.\"\"\"\n        params = {\n            \"response_type\": \"code\",\n            \"client_id\": \"polmystar\",\n            \"redirect_uri\": \"https://www.polestar.com/sign-in-callback\"\n        }\n        result = await self._client_session.get(\"https://polestarid.eu.polestar.com/as/authorization.oauth2\", params=params, timeout=HTTPX_TIMEOUT)\n        if result.status_code in (303, 302):\n            return result.next_request.url.params\n\n        _LOGGER.error(result.text)",
    "prefix": "from datetime import datetime, timedelta\nfrom .const import HTTPX_TIMEOUT\nfrom .exception import PolestarAuthException\nimport json\nimport logging\nimport httpx\n\n\n\n_LOGGER = logging.getLogger(__name__)\n\n\nclass PolestarAuth:\n    \"\"\"base class for Polestar authentication.\"\"\"\n\n    def __init__(self, username: str, password: str) -> None:\n        \"\"\"Initialize the Polestar authentication.\"\"\"\n        self.username = username\n        self.password = password\n        self.access_token = None\n        self.refresh_token = None\n        self.token_expiry = None\n        self.latest_call_code = None\n        self._client_session = httpx.AsyncClient()\n\n    async def get_token(self, refresh=False) -> None:\n        \"\"\"Get the token from Polestar.\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        operationName = \"getAuthToken\"\n        if not refresh:\n            code = await self._get_code()\n            if code is None:\n                return\n            params = {\n                \"query\": \"query getAuthToken($code: String!) { getAuthToken(code: $code) { id_token access_token refresh_token expires_in }}\",\n                \"operationName\": operationName,\n                \"variables\": json.dumps({\"code\": code}),\n            }\n        else:\n            if self.refresh_token is None:\n                return\n            token = self.refresh_token\n            operationName = \"refreshAuthToken\"\n            headers[\"Authorization\"] = f\"Bearer {self.access_token}\"\n\n            params = {\n                \"query\": \"query refreshAuthToken($token: String!) { refreshAuthToken(token: $token) { id_token access_token refresh_token expires_in }}\",\n                \"operationName\": operationName,\n                \"variables\": json.dumps({\"token\": token}),\n            }\n        result = await self._client_session.get(\"https://pc-api.polestar.com/eu-north-1/auth/\", params=params, headers=headers, timeout=HTTPX_TIMEOUT)\n        self.latest_call_code = result.status_code\n        resultData = result.json()\n        if result.status_code != 200 or (\"errors\" in resultData and len(resultData[\"errors\"])):\n            _LOGGER.error(result)\n            raise PolestarAuthException(\"Error getting token\", result.status_code)\n        _LOGGER.debug(resultData)\n\n        if resultData['data']:\n            self.access_token = resultData['data'][operationName]['access_token']\n            self.refresh_token = resultData['data'][operationName]['refresh_token']\n            self.token_expiry = datetime.now(\n            ) + timedelta(seconds=resultData['data'][operationName]['expires_in'])\n            # ID Token\n\n        _LOGGER.debug(f\"Response {self.access_token}\")\n\n    async def _get_code(self) -> None:\n        query_params = await self._get_resume_path()\n\n        # check if code is in query_params\n        if query_params.get('code'):\n            return query_params.get('code')\n\n        # get the resumePath\n        if query_params.get('resumePath'):\n            resumePath = query_params.get('resumePath')\n\n        if resumePath is None:\n            return\n\n        params = {\n            'client_id': 'polmystar'\n        }\n        data = {\n            'pf.username': self.username,\n            'pf.pass': self.password\n        }\n        result = await self._client_session.post(\n            f\"https://polestarid.eu.polestar.com/as/{resumePath}/resume/as/authorization.ping\",\n            params=params,\n            data=data\n        )\n        self.latest_call_code = result.status_code\n        if result.status_code != 302:\n            raise PolestarAuthException(\"Error getting code\", result.status_code)\n\n        # get the realUrl\n        url = result.url\n        code = result.next_request.url.params.get('code')\n\n        # sign-in-callback\n        result = await self._client_session.get(result.next_request.url, timeout=HTTPX_TIMEOUT)\n        self.latest_call_code = result.status_code\n\n        if result.status_code != 200:\n            _LOGGER.error(result)\n            raise PolestarAuthException(\"Error getting code callback\", result.status_code)\n\n        # url encode the code\n        result = await self._client_session.get(url)\n        self.latest_call_code = result.status_code\n\n        return code\n\n    async def _get_resume_path(self):\n        \"\"\"Get Resume Path from Polestar.\"\"\"\n        params = {\n            \"response_type\": \"code\",\n            \"client_id\": \"polmystar\",\n            \"redirect_uri\": \"https://www.polestar.com/sign-in-callback\"\n        }\n        result = await self._client_session.get(\"https://polestarid.eu.polestar.com/as/authorization.oauth2\", params=params, timeout=HTTPX_TIMEOUT)\n        if result.status_code in (303, 302):\n            return result.next_request.url.params\n\n        _LOGGER.error(result.text)",
    "suffix": ""
  },
  {
    "name": "dubverse-ai/MahaTTS:maha_tts/models/autoregressive.py@994",
    "canonical_solution": "            self.text_embed = nn.Embedding(len(text_labels_en),self.n_embed)",
    "prompt": "import os,sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport functools\nfrom typing import Any\nfrom torch.utils.data import Dataset,DataLoader\nfrom transformers import GPT2Tokenizer,GPT2Config, GPT2Model, GPT2LMHeadModel\nfrom tqdm import tqdm\nfrom maha_tts.config import config\nfrom maha_tts.text.symbols import labels,code_labels,text_labels,text_labels_en\nfrom maha_tts.models.modules import GST\n'''\nInspiration taken from https://github.com/neonbjb/tortoise-tts/blob/main/tortoise/models/autoregressive.py\n'''\n\n\ndef null_position_embeddings(range, dim):\n    return torch.zeros((range.shape[0], range.shape[1], dim), device=range.device)\n\nclass TS_model(nn.Module):\n    def __init__(self,n_embed = 512, n_layer = 16, n_head = 8, n_positions = 2048, name='Smolie-in'):\n        super(TS_model,self).__init__()\n\n        self.vocab_size=len(labels)\n        self.n_positions=n_positions\n        self.n_embed=n_embed\n        self.n_layer=n_layer\n        self.n_head=n_head\n        self.name=name\n\n        self.config = GPT2Config(vocab_size=self.vocab_size,n_positions=self.n_positions,n_embd=self.n_embed,n_layer=self.n_layer,n_head=self.n_head)\n        self.gpt = GPT2Model(self.config)\n        del self.gpt.wpe\n        self.gpt.wpe = functools.partial(null_position_embeddings, dim=self.n_embed)\n        # Built-in token embeddings are unused.\n        del self.gpt.wte\n        self.GST = GST(model_channels=self.n_embed,num_heads=self.n_head,in_channels=config.n_mel_channels,k=1)\n        if self.name == 'Smolie-en':\n            self.text_head = nn.Linear(self.n_embed,len(text_labels_en))\n        else:\n            self.text_head = nn.Linear(self.n_embed,len(text_labels))\n\n        self.code_head = nn.Linear(self.n_embed,len(code_labels))\n        self.text_positional_embed = LearnedPositionEmbeddings(self.n_positions,self.n_embed)\n        self.code_positional_embed = LearnedPositionEmbeddings(self.n_positions,self.n_embed)\n        \n        if self.name == 'Smolie-en':",
    "prefix": "import os,sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport functools\nfrom typing import Any\nfrom torch.utils.data import Dataset,DataLoader\nfrom transformers import GPT2Tokenizer,GPT2Config, GPT2Model, GPT2LMHeadModel\nfrom tqdm import tqdm\nfrom maha_tts.config import config\nfrom maha_tts.text.symbols import labels,code_labels,text_labels,text_labels_en\nfrom maha_tts.models.modules import GST\n'''\nInspiration taken from https://github.com/neonbjb/tortoise-tts/blob/main/tortoise/models/autoregressive.py\n'''\n\n\ndef null_position_embeddings(range, dim):\n    return torch.zeros((range.shape[0], range.shape[1], dim), device=range.device)\n\nclass TS_model(nn.Module):\n    def __init__(self,n_embed = 512, n_layer = 16, n_head = 8, n_positions = 2048, name='Smolie-in'):\n        super(TS_model,self).__init__()\n\n        self.vocab_size=len(labels)\n        self.n_positions=n_positions\n        self.n_embed=n_embed\n        self.n_layer=n_layer\n        self.n_head=n_head\n        self.name=name\n\n        self.config = GPT2Config(vocab_size=self.vocab_size,n_positions=self.n_positions,n_embd=self.n_embed,n_layer=self.n_layer,n_head=self.n_head)\n        self.gpt = GPT2Model(self.config)\n        del self.gpt.wpe\n        self.gpt.wpe = functools.partial(null_position_embeddings, dim=self.n_embed)\n        # Built-in token embeddings are unused.\n        del self.gpt.wte\n        self.GST = GST(model_channels=self.n_embed,num_heads=self.n_head,in_channels=config.n_mel_channels,k=1)\n        if self.name == 'Smolie-en':\n            self.text_head = nn.Linear(self.n_embed,len(text_labels_en))\n        else:\n            self.text_head = nn.Linear(self.n_embed,len(text_labels))\n\n        self.code_head = nn.Linear(self.n_embed,len(code_labels))\n        self.text_positional_embed = LearnedPositionEmbeddings(self.n_positions,self.n_embed)\n        self.code_positional_embed = LearnedPositionEmbeddings(self.n_positions,self.n_embed)\n        \n        if self.name == 'Smolie-en':",
    "suffix": ""
  },
  {
    "name": "WCGKING/KINGUSERBOT:Branded/plugins/pmguard.py@1433",
    "canonical_solution": "        await approve(uid)",
    "prompt": "import asyncio\nfrom pyrogram import Client, filters\nfrom pyrogram.enums import ChatType\nfrom pyrogram.types import *\nfrom .. import *\nfrom ..modules.data import approve, disapprove, is_approved\n\n\n\nMSG_PERMIT = \"\"\"\nPM_SECURITY BRANDED-USERBOT\n\n{}\nawait message.reply_photo=\"https://te.legra.ph/file/11cfa74175b590014bd16.jpg\"\n\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\n\u235f You have {}/{} warning!!!\n\"\"\"\n\nDEFAULT = \"\"\"\nWELCOME....\n\n\u029c\u026a, \u1d1b\u029c\u026a\ua731 \u026a\ua731 \u1d1b\u029c\u1d07 \u1d0b\u1d07\u1d07\u1d18\u1d07\u0280 \u1d0f\ua730 \u1d18\u0280\u026a\u1d20\u1d00\u1d1b\u1d07 \u1d0d\u1d07\ua731\ua731\u1d00\u0262\u1d07\ua731. \u1d05\u1d0f\u0274'\u1d1b \ua731\u1d18\u1d00\u1d0d \u028f\u1d00 \u1d0f\u0280 \u026a'\u029f\u029f \u0299\u029f\u1d0f\u1d04\u1d0b \u028f\u1d0f\u1d1c. \u1d21\u1d00\u026a\u1d1b \u1d1c\u0274\u1d1b\u026a\u029f \u1d0d\u028f \u1d0d\u1d00\ua731\u1d1b\u1d07\u0280 \u0280\u1d07\u1d04\u1d07\u026a\u1d20\u1d07\ua731 \u028f\u1d0f\u1d1c\u0280 \u1d0d\u1d07\ua731\ua731\u1d00\u0262\u1d07.\u026a \u1d00\u1d0d \u1d00\u0274 \u1d00\u1d05\u1d20\u1d00\u0274\u1d04\u1d07\u1d05 \u1d00\u0274\u1d05 s\u1d1c\u1d18\u1d07\u0280\u0493\u1d00s\u1d1b \u1d1c\ua731\u1d07\u0280\u0299\u1d0f\u1d1b  \u1d21\u026a\u1d1b\u029c 24x7 \u1d00\u1d04\u1d1b\u026a\u1d20\u1d07 \u00bb \u0493\u1d0f\u0280 \u1d1b\u1d07\u029f\u1d07\u0262\u0280\u1d00\u1d0d \u026a\u1d05 \n\"\"\"\n\n@app.on_message(\n    (\n        filters.private\n        & filters.incoming\n        & ~filters.service\n        & ~filters.me\n        & ~filters.bot\n        & ~filters.via_bot\n    )\n)\nasync def pmpermit_func(client: Client, message: Message):\n    user_ = message.from_user\n    approved = await is_approved()\n    pmper = var.PMPERMIT\n    if pmper == str(False):\n        return True\n    if user_.is_bot:\n        return\n    if user_.is_self:\n        return\n    if user_.is_contact:\n        return\n    if user_.is_verified:\n        return\n    if user_.is_scam:\n        await message.reply_text(\"Imposter Detected!\\nAutomatic Blocking!!!\")\n        await client.block_user(user_.id)\n        return\n    if user_.is_support:\n        return\n    if user_.id in approved:\n        return\n    limits = var.PERMIT_LIMIT\n    async for m in client.get_chat_history(user_.id, limit=limits):\n        if m.reply_markup:\n            await m.delete()\n    if str(user_.id) in flood:\n        flood[str(user_.id)] += 1\n    else:\n        flood[str(user_.id)] = 1\n    if flood[str(user_.id)] > limits:\n        await message.reply_text(\"Spammer Detected!\\nAutomatic Blocking User!!!\")\n        if str(user_.id) in OLD_MSG:\n            OLD_MSG.pop(str(user_.id))\n            flood.update({user_.id: 0})\n        return await client.block_user(user_.id)\n    getmsg = Config.PERMIT_MSG\n    pm_message = DEFAULT if not getmsg else getmsg\n    msg_dlt = await client.send_message(\n        user_.id,\n        MSG_PERMIT.format(pm_message, flood[str(user_.id)], limits),\n    )\n    if str(user_.id) in OLD_MSG:\n        try:\n            await OLD_MSG[str(user_.id)].delete()\n        except BaseException:\n            pass\n    OLD_MSG[str(user_.id)] = msg_dlt\n\n\n@app.on_message(commandx([\"approve\", \"a\"]))\nasync def pm_approve(client: Client, message: Message):\n    permit = await is_approved()\n    if message.reply_to_message:\n        reply = message.reply_to_message\n        replied_user = reply.from_user\n        if replied_user.is_self:\n            await message.edit(\"You can't do that to yourself.\")\n            return\n        uid = replied_user.id\n        if uid in permit:\n            return await message.reply(\"This user already exists in the database.\")\n        await approve(uid)\n        xnxx = await message.reply(\"Your message was received.\")\n        if str(uid) in OLD_MSG and str(uid) in flood:\n            await OLD_MSG[str(uid)].delete()\n            flood[str(uid)] = 0\n        await asyncio.sleep(3)\n        await xnxx.delete()\n    else:\n        aname = message.chat\n        if not aname.type == ChatType.PRIVATE:\n            await message.reply(\n                \"You're not currently in PM and you haven't replied to someone's messages.\"\n            )\n            return\n        uid = aname.id\n        if uid in permit:\n            return await message.reply(\"This user already exists in the database\")",
    "prefix": "import asyncio\nfrom pyrogram import Client, filters\nfrom pyrogram.enums import ChatType\nfrom pyrogram.types import *\nfrom .. import *\nfrom ..modules.data import approve, disapprove, is_approved\n\n\n\nMSG_PERMIT = \"\"\"\nPM_SECURITY BRANDED-USERBOT\n\n{}\nawait message.reply_photo=\"https://te.legra.ph/file/11cfa74175b590014bd16.jpg\"\n\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\n\u235f You have {}/{} warning!!!\n\"\"\"\n\nDEFAULT = \"\"\"\nWELCOME....\n\n\u029c\u026a, \u1d1b\u029c\u026a\ua731 \u026a\ua731 \u1d1b\u029c\u1d07 \u1d0b\u1d07\u1d07\u1d18\u1d07\u0280 \u1d0f\ua730 \u1d18\u0280\u026a\u1d20\u1d00\u1d1b\u1d07 \u1d0d\u1d07\ua731\ua731\u1d00\u0262\u1d07\ua731. \u1d05\u1d0f\u0274'\u1d1b \ua731\u1d18\u1d00\u1d0d \u028f\u1d00 \u1d0f\u0280 \u026a'\u029f\u029f \u0299\u029f\u1d0f\u1d04\u1d0b \u028f\u1d0f\u1d1c. \u1d21\u1d00\u026a\u1d1b \u1d1c\u0274\u1d1b\u026a\u029f \u1d0d\u028f \u1d0d\u1d00\ua731\u1d1b\u1d07\u0280 \u0280\u1d07\u1d04\u1d07\u026a\u1d20\u1d07\ua731 \u028f\u1d0f\u1d1c\u0280 \u1d0d\u1d07\ua731\ua731\u1d00\u0262\u1d07.\u026a \u1d00\u1d0d \u1d00\u0274 \u1d00\u1d05\u1d20\u1d00\u0274\u1d04\u1d07\u1d05 \u1d00\u0274\u1d05 s\u1d1c\u1d18\u1d07\u0280\u0493\u1d00s\u1d1b \u1d1c\ua731\u1d07\u0280\u0299\u1d0f\u1d1b  \u1d21\u026a\u1d1b\u029c 24x7 \u1d00\u1d04\u1d1b\u026a\u1d20\u1d07 \u00bb \u0493\u1d0f\u0280 \u1d1b\u1d07\u029f\u1d07\u0262\u0280\u1d00\u1d0d \u026a\u1d05 \n\"\"\"\n\n@app.on_message(\n    (\n        filters.private\n        & filters.incoming\n        & ~filters.service\n        & ~filters.me\n        & ~filters.bot\n        & ~filters.via_bot\n    )\n)\nasync def pmpermit_func(client: Client, message: Message):\n    user_ = message.from_user\n    approved = await is_approved()\n    pmper = var.PMPERMIT\n    if pmper == str(False):\n        return True\n    if user_.is_bot:\n        return\n    if user_.is_self:\n        return\n    if user_.is_contact:\n        return\n    if user_.is_verified:\n        return\n    if user_.is_scam:\n        await message.reply_text(\"Imposter Detected!\\nAutomatic Blocking!!!\")\n        await client.block_user(user_.id)\n        return\n    if user_.is_support:\n        return\n    if user_.id in approved:\n        return\n    limits = var.PERMIT_LIMIT\n    async for m in client.get_chat_history(user_.id, limit=limits):\n        if m.reply_markup:\n            await m.delete()\n    if str(user_.id) in flood:\n        flood[str(user_.id)] += 1\n    else:\n        flood[str(user_.id)] = 1\n    if flood[str(user_.id)] > limits:\n        await message.reply_text(\"Spammer Detected!\\nAutomatic Blocking User!!!\")\n        if str(user_.id) in OLD_MSG:\n            OLD_MSG.pop(str(user_.id))\n            flood.update({user_.id: 0})\n        return await client.block_user(user_.id)\n    getmsg = Config.PERMIT_MSG\n    pm_message = DEFAULT if not getmsg else getmsg\n    msg_dlt = await client.send_message(\n        user_.id,\n        MSG_PERMIT.format(pm_message, flood[str(user_.id)], limits),\n    )\n    if str(user_.id) in OLD_MSG:\n        try:\n            await OLD_MSG[str(user_.id)].delete()\n        except BaseException:\n            pass\n    OLD_MSG[str(user_.id)] = msg_dlt\n\n\n@app.on_message(commandx([\"approve\", \"a\"]))\nasync def pm_approve(client: Client, message: Message):\n    permit = await is_approved()\n    if message.reply_to_message:\n        reply = message.reply_to_message\n        replied_user = reply.from_user\n        if replied_user.is_self:\n            await message.edit(\"You can't do that to yourself.\")\n            return\n        uid = replied_user.id\n        if uid in permit:\n            return await message.reply(\"This user already exists in the database.\")\n        await approve(uid)\n        xnxx = await message.reply(\"Your message was received.\")\n        if str(uid) in OLD_MSG and str(uid) in flood:\n            await OLD_MSG[str(uid)].delete()\n            flood[str(uid)] = 0\n        await asyncio.sleep(3)\n        await xnxx.delete()\n    else:\n        aname = message.chat\n        if not aname.type == ChatType.PRIVATE:\n            await message.reply(\n                \"You're not currently in PM and you haven't replied to someone's messages.\"\n            )\n            return\n        uid = aname.id\n        if uid in permit:\n            return await message.reply(\"This user already exists in the database\")",
    "suffix": ""
  },
  {
    "name": "kudelskisecurity/fuzzomatic:fuzzomatic/approaches/common.py@1561",
    "canonical_solution": "        fuzz_target_path = write_fuzz_target(code_snippet, codebase_dir, target_name)",
    "prompt": "import fuzzomatic.tools.utils\nfrom fuzzomatic.tools import llm, prompts\nfrom fuzzomatic.tools.utils import (\n    write_fuzz_target,\n    build_target,\n    remove_fuzz_dependency,\n    add_fuzz_dependency,\n)\n\n\ndef llm_attempt(\n    codebase_dir, prompt, target_name, remaining_attempts=2, additional_code=None\n):\n    # read the example and feed it to the LLM\n    response = llm.ask_llm(prompt)\n    code_snippet = llm.extract_fuzz_target(response, codebase_dir)\n\n    # append additional code (used in unit tests with additional code approach)\n    if additional_code is not None and code_snippet is not None:\n        if additional_code not in code_snippet:\n            code_snippet += \"\\n\\n\"\n            code_snippet += additional_code\n\n    print(\"Extracted code snippet\")\n    print(\"======\")\n    print(code_snippet)\n    fix = True\n\n    if code_snippet is not None and \"library_function(\" in code_snippet:\n        print(\"Generated call to library_function(). Moving on...\")\n        return False, None\n\n    if code_snippet is not None:\n        fuzz_target_path = write_fuzz_target(code_snippet, codebase_dir, target_name)\n        # try to build the target\n        build_success, error, built_code = build_target(codebase_dir, target_name)\n    else:\n        build_success = False\n        fix = False\n    if build_success:\n        return build_success, fuzz_target_path\n    else:\n        # try to fix the code using the error message\n        if fix:\n            fix_success, error = llm_attempt_fix_error(\n                codebase_dir, target_name, built_code, error, remaining_attempts=2\n            )\n        else:\n            fix_success = False\n\n        if fix_success:\n            return fix_success, fuzz_target_path\n        elif remaining_attempts > 0:\n            return llm_attempt(\n                codebase_dir,\n                prompt,\n                target_name,\n                remaining_attempts - 1,\n                additional_code=additional_code,\n            )\n        else:\n            # no more remaining attempts\n            return False, None\n\n\ndef llm_attempt_fix_error(\n    codebase_dir, target_name, code_snippet, error, remaining_attempts=2\n):\n    # try to fix missing cargo dependencies deterministically\n    build_success, error, code_snippet = add_missing_cargo_dependencies(\n        codebase_dir, error, code_snippet, target_name\n    )\n    if build_success:\n        fuzz_target_path = write_fuzz_target(code_snippet, codebase_dir, target_name)\n        return True, fuzz_target_path\n    else:\n        print(\"Failed to fix cargo dependencies. Resuming...\")\n\n    fix_prompt = prompts.fix_prompt(code_snippet, error)\n    print(\"Asking LLM to fix the code...\")\n    response = llm.ask_llm(fix_prompt)\n    print(\"Response:\")\n    print(response)\n    code_snippet = llm.extract_fuzz_target(response, codebase_dir)\n    print(\"Extracted code snippet\")\n    print(\"======\")\n    print(code_snippet)\n    fix = True\n\n    if code_snippet is not None:",
    "prefix": "import fuzzomatic.tools.utils\nfrom fuzzomatic.tools import llm, prompts\nfrom fuzzomatic.tools.utils import (\n    write_fuzz_target,\n    build_target,\n    remove_fuzz_dependency,\n    add_fuzz_dependency,\n)\n\n\ndef llm_attempt(\n    codebase_dir, prompt, target_name, remaining_attempts=2, additional_code=None\n):\n    # read the example and feed it to the LLM\n    response = llm.ask_llm(prompt)\n    code_snippet = llm.extract_fuzz_target(response, codebase_dir)\n\n    # append additional code (used in unit tests with additional code approach)\n    if additional_code is not None and code_snippet is not None:\n        if additional_code not in code_snippet:\n            code_snippet += \"\\n\\n\"\n            code_snippet += additional_code\n\n    print(\"Extracted code snippet\")\n    print(\"======\")\n    print(code_snippet)\n    fix = True\n\n    if code_snippet is not None and \"library_function(\" in code_snippet:\n        print(\"Generated call to library_function(). Moving on...\")\n        return False, None\n\n    if code_snippet is not None:\n        fuzz_target_path = write_fuzz_target(code_snippet, codebase_dir, target_name)\n        # try to build the target\n        build_success, error, built_code = build_target(codebase_dir, target_name)\n    else:\n        build_success = False\n        fix = False\n    if build_success:\n        return build_success, fuzz_target_path\n    else:\n        # try to fix the code using the error message\n        if fix:\n            fix_success, error = llm_attempt_fix_error(\n                codebase_dir, target_name, built_code, error, remaining_attempts=2\n            )\n        else:\n            fix_success = False\n\n        if fix_success:\n            return fix_success, fuzz_target_path\n        elif remaining_attempts > 0:\n            return llm_attempt(\n                codebase_dir,\n                prompt,\n                target_name,\n                remaining_attempts - 1,\n                additional_code=additional_code,\n            )\n        else:\n            # no more remaining attempts\n            return False, None\n\n\ndef llm_attempt_fix_error(\n    codebase_dir, target_name, code_snippet, error, remaining_attempts=2\n):\n    # try to fix missing cargo dependencies deterministically\n    build_success, error, code_snippet = add_missing_cargo_dependencies(\n        codebase_dir, error, code_snippet, target_name\n    )\n    if build_success:\n        fuzz_target_path = write_fuzz_target(code_snippet, codebase_dir, target_name)\n        return True, fuzz_target_path\n    else:\n        print(\"Failed to fix cargo dependencies. Resuming...\")\n\n    fix_prompt = prompts.fix_prompt(code_snippet, error)\n    print(\"Asking LLM to fix the code...\")\n    response = llm.ask_llm(fix_prompt)\n    print(\"Response:\")\n    print(response)\n    code_snippet = llm.extract_fuzz_target(response, codebase_dir)\n    print(\"Extracted code snippet\")\n    print(\"======\")\n    print(code_snippet)\n    fix = True\n\n    if code_snippet is not None:",
    "suffix": ""
  },
  {
    "name": "muyuworks/myla:tests/myla/assistants_test.py@884",
    "canonical_solution": "        asst_read = assistants.get(id=asst_created.id, user_id=user_id, session=self.session)",
    "prompt": "import unittest\nfrom myla import assistants, persistence\n\n\n\nclass TestUsers(unittest.TestCase):\n\n    def setUp(self) -> None:\n        self.db = persistence.Persistence(database_url=\"sqlite://\")\n        self.db.initialize_database()\n        self.session = self.db.create_session()\n\n    def tearDown(self) -> None:\n        self.session.close()\n\n    def test_create_get_list_assistant(self, user_id=None):\n        asst_created = assistants.create(assistant=assistants.AssistantCreate(\n            name='name',\n            description='desc',\n            model='model',\n            instructions='instruction',\n            tools=[{'type': 'retrieval'}],\n            file_ids=['1'],\n            metadata={'k': 'v'}\n            ), user_id=user_id, session=self.session)\n        self.assertIsNotNone(asst_created.id)\n\n        asst_read = assistants.get(id=asst_created.id, user_id=user_id, session=self.session)\n        self.assertIsNotNone(asst_read)\n        self.assertEqual(asst_created.id, asst_read.id)\n        self.assertEqual(asst_read.name, 'name')\n        self.assertEqual(asst_read.description, 'desc')\n        self.assertEqual(asst_read.instructions, 'instruction')\n        self.assertEqual(asst_read.tools, [{'type': 'retrieval'}])\n        self.assertEqual(asst_read.file_ids, ['1'])\n        self.assertEqual(asst_read.model, 'model')\n        self.assertEqual(asst_read.metadata, {'k': 'v'})\n\n        asst_list = assistants.list(user_id=user_id, session=self.session)\n        self.assertEqual(len(asst_list.data), 1)\n        asst_read = asst_list.data[0]\n        self.assertEqual(asst_created.id, asst_read.id)\n        self.assertEqual(asst_read.name, 'name')\n        self.assertEqual(asst_read.description, 'desc')\n        self.assertEqual(asst_read.instructions, 'instruction')\n        self.assertEqual(asst_read.tools, [{'type': 'retrieval'}])\n        self.assertEqual(asst_read.file_ids, ['1'])\n        self.assertEqual(asst_read.model, 'model')\n        self.assertEqual(asst_read.metadata, {'k': 'v'})\n\n        asst_update = assistants.modify(id=asst_read.id, assistant=assistants.AssistantModify(name='newname', model='newmodel', metadata={'k': 'v1'}), user_id=user_id, session=self.session)\n        self.assertIsNotNone(asst_update)\n        self.assertEqual(asst_update.id, asst_created.id)\n        self.assertEqual(asst_update.name, 'newname')",
    "prefix": "import unittest\nfrom myla import assistants, persistence\n\n\n\nclass TestUsers(unittest.TestCase):\n\n    def setUp(self) -> None:\n        self.db = persistence.Persistence(database_url=\"sqlite://\")\n        self.db.initialize_database()\n        self.session = self.db.create_session()\n\n    def tearDown(self) -> None:\n        self.session.close()\n\n    def test_create_get_list_assistant(self, user_id=None):\n        asst_created = assistants.create(assistant=assistants.AssistantCreate(\n            name='name',\n            description='desc',\n            model='model',\n            instructions='instruction',\n            tools=[{'type': 'retrieval'}],\n            file_ids=['1'],\n            metadata={'k': 'v'}\n            ), user_id=user_id, session=self.session)\n        self.assertIsNotNone(asst_created.id)\n\n        asst_read = assistants.get(id=asst_created.id, user_id=user_id, session=self.session)\n        self.assertIsNotNone(asst_read)\n        self.assertEqual(asst_created.id, asst_read.id)\n        self.assertEqual(asst_read.name, 'name')\n        self.assertEqual(asst_read.description, 'desc')\n        self.assertEqual(asst_read.instructions, 'instruction')\n        self.assertEqual(asst_read.tools, [{'type': 'retrieval'}])\n        self.assertEqual(asst_read.file_ids, ['1'])\n        self.assertEqual(asst_read.model, 'model')\n        self.assertEqual(asst_read.metadata, {'k': 'v'})\n\n        asst_list = assistants.list(user_id=user_id, session=self.session)\n        self.assertEqual(len(asst_list.data), 1)\n        asst_read = asst_list.data[0]\n        self.assertEqual(asst_created.id, asst_read.id)\n        self.assertEqual(asst_read.name, 'name')\n        self.assertEqual(asst_read.description, 'desc')\n        self.assertEqual(asst_read.instructions, 'instruction')\n        self.assertEqual(asst_read.tools, [{'type': 'retrieval'}])\n        self.assertEqual(asst_read.file_ids, ['1'])\n        self.assertEqual(asst_read.model, 'model')\n        self.assertEqual(asst_read.metadata, {'k': 'v'})\n\n        asst_update = assistants.modify(id=asst_read.id, assistant=assistants.AssistantModify(name='newname', model='newmodel', metadata={'k': 'v1'}), user_id=user_id, session=self.session)\n        self.assertIsNotNone(asst_update)\n        self.assertEqual(asst_update.id, asst_created.id)\n        self.assertEqual(asst_update.name, 'newname')",
    "suffix": ""
  },
  {
    "name": "OSU-NLP-Group/TableLlama:inference_ent_link.py@696",
    "canonical_solution": "    return PROMPT_DICT[\"prompt_no_input\"].format(instruction=instruction)",
    "prompt": "import os\nimport json\nimport sys\nimport math\nimport torch\nimport argparse\nimport transformers\nfrom peft import PeftModel\nfrom transformers import GenerationConfig\nfrom llama_attn_replace import replace_llama_attn\nfrom supervised_fine_tune import PROMPT_DICT\nfrom tqdm import tqdm\n# import textwrap\n# from queue import Queue\n# from threading import Thread\n# import gradio as gr\n\ndef parse_config():\n    parser = argparse.ArgumentParser(description='arg parser')\n    parser.add_argument('--base_model', type=str, default=\"/data1/pretrained-models/llama-7b-hf\")\n    parser.add_argument('--cache_dir', type=str, default=\"./cache\")\n    parser.add_argument('--context_size', type=int, default=-1, help='context size during fine-tuning')\n    parser.add_argument('--flash_attn', type=bool, default=False, help='')\n    parser.add_argument('--temperature', type=float, default=0.6, help='')\n    parser.add_argument('--top_p', type=float, default=0.9, help='')\n    parser.add_argument('--max_gen_len', type=int, default=512, help='')\n    parser.add_argument('--input_data_file', type=str, default='input_data/', help='')\n    parser.add_argument('--output_data_file', type=str, default='output_data/', help='')\n    args = parser.parse_args()\n    return args\n\ndef generate_prompt(instruction, question, input_seg=None):\n  if input:\n    return PROMPT_DICT[\"prompt_input\"].format(instruction=instruction, input_seg=input_seg, question=question)\n  else:",
    "prefix": "import os\nimport json\nimport sys\nimport math\nimport torch\nimport argparse\nimport transformers\nfrom peft import PeftModel\nfrom transformers import GenerationConfig\nfrom llama_attn_replace import replace_llama_attn\nfrom supervised_fine_tune import PROMPT_DICT\nfrom tqdm import tqdm\n# import textwrap\n# from queue import Queue\n# from threading import Thread\n# import gradio as gr\n\ndef parse_config():\n    parser = argparse.ArgumentParser(description='arg parser')\n    parser.add_argument('--base_model', type=str, default=\"/data1/pretrained-models/llama-7b-hf\")\n    parser.add_argument('--cache_dir', type=str, default=\"./cache\")\n    parser.add_argument('--context_size', type=int, default=-1, help='context size during fine-tuning')\n    parser.add_argument('--flash_attn', type=bool, default=False, help='')\n    parser.add_argument('--temperature', type=float, default=0.6, help='')\n    parser.add_argument('--top_p', type=float, default=0.9, help='')\n    parser.add_argument('--max_gen_len', type=int, default=512, help='')\n    parser.add_argument('--input_data_file', type=str, default='input_data/', help='')\n    parser.add_argument('--output_data_file', type=str, default='output_data/', help='')\n    args = parser.parse_args()\n    return args\n\ndef generate_prompt(instruction, question, input_seg=None):\n  if input:\n    return PROMPT_DICT[\"prompt_input\"].format(instruction=instruction, input_seg=input_seg, question=question)\n  else:",
    "suffix": ""
  },
  {
    "name": "jianchang512/vocal-separate:start.py@1175",
    "canonical_solution": "        p=subprocess.run(['ffprobe','-v','error','-show_entries',\"format=duration\",'-of', \"default=noprint_wrappers=1:nokey=1\", wav_file], capture_output=True)      ",
    "prompt": "import logging\nimport threading\nimport sys\nimport os\nimport subprocess\nfrom flask import Flask, request, render_template, jsonify, send_from_directory\nfrom gevent.pywsgi import WSGIServer, WSGIHandler,LoggingLogAdapter\nfrom logging.handlers import RotatingFileHandler\nfrom vocal import cfg, tool\nfrom vocal.cfg import ROOT_DIR\nfrom spleeter.separator import Separator\n\nclass CustomRequestHandler(WSGIHandler):\n    def log_request(self):\n        pass\n\n# \u7981\u7528 Werkzeug \u9ed8\u8ba4\u7684\u65e5\u5fd7\u5904\u7406\u5668\nlog = logging.getLogger('werkzeug')\nlog.handlers[:] = []\nlog.setLevel(logging.WARNING)\n\napp = Flask(__name__, static_folder=os.path.join(ROOT_DIR, 'static'), static_url_path='/static',\n            template_folder=os.path.join(ROOT_DIR, 'templates'))\nroot_log = logging.getLogger()  # Flask\u7684\u6839\u65e5\u5fd7\u8bb0\u5f55\u5668\nroot_log.handlers = []\nroot_log.setLevel(logging.WARNING)\n\n# \u914d\u7f6e\u65e5\u5fd7\napp.logger.setLevel(logging.WARNING)  # \u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b\u4e3a INFO\n# \u521b\u5efa RotatingFileHandler \u5bf9\u8c61\uff0c\u8bbe\u7f6e\u5199\u5165\u7684\u6587\u4ef6\u8def\u5f84\u548c\u5927\u5c0f\u9650\u5236\nfile_handler = RotatingFileHandler(os.path.join(ROOT_DIR, 'vocal.log'), maxBytes=1024 * 1024, backupCount=5)\n# \u521b\u5efa\u65e5\u5fd7\u7684\u683c\u5f0f\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n# \u8bbe\u7f6e\u6587\u4ef6\u5904\u7406\u5668\u7684\u7ea7\u522b\u548c\u683c\u5f0f\nfile_handler.setLevel(logging.WARNING)\nfile_handler.setFormatter(formatter)\n# \u5c06\u6587\u4ef6\u5904\u7406\u5668\u6dfb\u52a0\u5230\u65e5\u5fd7\u8bb0\u5f55\u5668\u4e2d\napp.logger.addHandler(file_handler)\n\n\n@app.route('/static/<path:filename>')\ndef static_files(filename):\n    return send_from_directory(app.config['STATIC_FOLDER'], filename)\n\n@app.route('/')\ndef index():\n    return render_template(\"index.html\",cuda=cfg.cuda, language=cfg.LANG,root_dir=ROOT_DIR.replace('\\\\', '/'))\n\n\n# \u4e0a\u4f20\u97f3\u9891\n@app.route('/upload', methods=['POST'])\ndef upload():\n    try:\n        # \u83b7\u53d6\u4e0a\u4f20\u7684\u6587\u4ef6\n        audio_file = request.files['audio']\n        # \u5982\u679c\u662fmp4\n        noextname, ext = os.path.splitext(audio_file.filename)\n        ext = ext.lower()\n        # \u5982\u679c\u662f\u89c6\u9891\uff0c\u5148\u5206\u79bb\n        wav_file = os.path.join(cfg.TMP_DIR, f'{noextname}.wav')\n        if os.path.exists(wav_file) and os.path.getsize(wav_file) > 0:\n            return jsonify({'code': 0, 'msg': cfg.transobj['lang1'], \"data\": os.path.basename(wav_file)})\n        msg=\"\"\n        if ext in ['.mp4', '.mov', '.avi', '.mkv', '.mpeg', '.mp3', '.flac']:\n            video_file = os.path.join(cfg.TMP_DIR, f'{noextname}{ext}')\n            audio_file.save(video_file)\n            params = [\n                \"-i\",\n                video_file,\n            ]\n            if ext not in ['.mp3', '.flac']:\n                params.append('-vn')\n            params.append(wav_file)\n            rs = tool.runffmpeg(params)\n            if rs != 'ok':\n                return jsonify({\"code\": 1, \"msg\": rs})\n            msg=\",\"+cfg.transobj['lang9']\n        elif ext == '.wav':\n            audio_file.save(wav_file)\n        else:\n            return jsonify({\"code\": 1, \"msg\": f\"{cfg.transobj['lang3']} {ext}\"})\n        \n        # \u8fd4\u56de\u6210\u529f\u7684\u54cd\u5e94\n        return jsonify({'code': 0, 'msg': cfg.transobj['lang1']+msg, \"data\": os.path.basename(wav_file)})\n    except Exception as e:\n        app.logger.error(f'[upload]error: {e}')\n        return jsonify({'code': 2, 'msg': cfg.transobj['lang2']})\n\n\n# \u6839\u636e\u6587\u672c\u8fd4\u56detts\u7ed3\u679c\uff0c\u8fd4\u56de name=\u6587\u4ef6\u540d\u5b57\uff0cfilename=\u6587\u4ef6\u7edd\u5bf9\u8def\u5f84\n# \u8bf7\u6c42\u7aef\u6839\u636e\u9700\u8981\u81ea\u884c\u9009\u62e9\u4f7f\u7528\u54ea\u4e2a\n# params\n# wav_name:tmp\u4e0b\u7684wav\u6587\u4ef6\n# model \u6a21\u578b\u540d\u79f0\n@app.route('/process', methods=['GET', 'POST'])\ndef process():\n    # \u539f\u59cb\u5b57\u7b26\u4e32\n    wav_name = request.form.get(\"wav_name\").strip()\n    model = request.form.get(\"model\")\n    wav_file = os.path.join(cfg.TMP_DIR, wav_name)\n    noextname = wav_name[:-4]\n    if not os.path.exists(wav_file):\n        return jsonify({\"code\": 1, \"msg\": f\"{wav_file} {cfg.langlist['lang5']}\"})\n    if not os.path.exists(os.path.join(cfg.MODEL_DIR, model, 'model.meta')):\n        return jsonify({\"code\": 1, \"msg\": f\"{model} {cfg.transobj['lang4']}\"})\n    try:",
    "prefix": "import logging\nimport threading\nimport sys\nimport os\nimport subprocess\nfrom flask import Flask, request, render_template, jsonify, send_from_directory\nfrom gevent.pywsgi import WSGIServer, WSGIHandler,LoggingLogAdapter\nfrom logging.handlers import RotatingFileHandler\nfrom vocal import cfg, tool\nfrom vocal.cfg import ROOT_DIR\nfrom spleeter.separator import Separator\n\nclass CustomRequestHandler(WSGIHandler):\n    def log_request(self):\n        pass\n\n# \u7981\u7528 Werkzeug \u9ed8\u8ba4\u7684\u65e5\u5fd7\u5904\u7406\u5668\nlog = logging.getLogger('werkzeug')\nlog.handlers[:] = []\nlog.setLevel(logging.WARNING)\n\napp = Flask(__name__, static_folder=os.path.join(ROOT_DIR, 'static'), static_url_path='/static',\n            template_folder=os.path.join(ROOT_DIR, 'templates'))\nroot_log = logging.getLogger()  # Flask\u7684\u6839\u65e5\u5fd7\u8bb0\u5f55\u5668\nroot_log.handlers = []\nroot_log.setLevel(logging.WARNING)\n\n# \u914d\u7f6e\u65e5\u5fd7\napp.logger.setLevel(logging.WARNING)  # \u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b\u4e3a INFO\n# \u521b\u5efa RotatingFileHandler \u5bf9\u8c61\uff0c\u8bbe\u7f6e\u5199\u5165\u7684\u6587\u4ef6\u8def\u5f84\u548c\u5927\u5c0f\u9650\u5236\nfile_handler = RotatingFileHandler(os.path.join(ROOT_DIR, 'vocal.log'), maxBytes=1024 * 1024, backupCount=5)\n# \u521b\u5efa\u65e5\u5fd7\u7684\u683c\u5f0f\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n# \u8bbe\u7f6e\u6587\u4ef6\u5904\u7406\u5668\u7684\u7ea7\u522b\u548c\u683c\u5f0f\nfile_handler.setLevel(logging.WARNING)\nfile_handler.setFormatter(formatter)\n# \u5c06\u6587\u4ef6\u5904\u7406\u5668\u6dfb\u52a0\u5230\u65e5\u5fd7\u8bb0\u5f55\u5668\u4e2d\napp.logger.addHandler(file_handler)\n\n\n@app.route('/static/<path:filename>')\ndef static_files(filename):\n    return send_from_directory(app.config['STATIC_FOLDER'], filename)\n\n@app.route('/')\ndef index():\n    return render_template(\"index.html\",cuda=cfg.cuda, language=cfg.LANG,root_dir=ROOT_DIR.replace('\\\\', '/'))\n\n\n# \u4e0a\u4f20\u97f3\u9891\n@app.route('/upload', methods=['POST'])\ndef upload():\n    try:\n        # \u83b7\u53d6\u4e0a\u4f20\u7684\u6587\u4ef6\n        audio_file = request.files['audio']\n        # \u5982\u679c\u662fmp4\n        noextname, ext = os.path.splitext(audio_file.filename)\n        ext = ext.lower()\n        # \u5982\u679c\u662f\u89c6\u9891\uff0c\u5148\u5206\u79bb\n        wav_file = os.path.join(cfg.TMP_DIR, f'{noextname}.wav')\n        if os.path.exists(wav_file) and os.path.getsize(wav_file) > 0:\n            return jsonify({'code': 0, 'msg': cfg.transobj['lang1'], \"data\": os.path.basename(wav_file)})\n        msg=\"\"\n        if ext in ['.mp4', '.mov', '.avi', '.mkv', '.mpeg', '.mp3', '.flac']:\n            video_file = os.path.join(cfg.TMP_DIR, f'{noextname}{ext}')\n            audio_file.save(video_file)\n            params = [\n                \"-i\",\n                video_file,\n            ]\n            if ext not in ['.mp3', '.flac']:\n                params.append('-vn')\n            params.append(wav_file)\n            rs = tool.runffmpeg(params)\n            if rs != 'ok':\n                return jsonify({\"code\": 1, \"msg\": rs})\n            msg=\",\"+cfg.transobj['lang9']\n        elif ext == '.wav':\n            audio_file.save(wav_file)\n        else:\n            return jsonify({\"code\": 1, \"msg\": f\"{cfg.transobj['lang3']} {ext}\"})\n        \n        # \u8fd4\u56de\u6210\u529f\u7684\u54cd\u5e94\n        return jsonify({'code': 0, 'msg': cfg.transobj['lang1']+msg, \"data\": os.path.basename(wav_file)})\n    except Exception as e:\n        app.logger.error(f'[upload]error: {e}')\n        return jsonify({'code': 2, 'msg': cfg.transobj['lang2']})\n\n\n# \u6839\u636e\u6587\u672c\u8fd4\u56detts\u7ed3\u679c\uff0c\u8fd4\u56de name=\u6587\u4ef6\u540d\u5b57\uff0cfilename=\u6587\u4ef6\u7edd\u5bf9\u8def\u5f84\n# \u8bf7\u6c42\u7aef\u6839\u636e\u9700\u8981\u81ea\u884c\u9009\u62e9\u4f7f\u7528\u54ea\u4e2a\n# params\n# wav_name:tmp\u4e0b\u7684wav\u6587\u4ef6\n# model \u6a21\u578b\u540d\u79f0\n@app.route('/process', methods=['GET', 'POST'])\ndef process():\n    # \u539f\u59cb\u5b57\u7b26\u4e32\n    wav_name = request.form.get(\"wav_name\").strip()\n    model = request.form.get(\"model\")\n    wav_file = os.path.join(cfg.TMP_DIR, wav_name)\n    noextname = wav_name[:-4]\n    if not os.path.exists(wav_file):\n        return jsonify({\"code\": 1, \"msg\": f\"{wav_file} {cfg.langlist['lang5']}\"})\n    if not os.path.exists(os.path.join(cfg.MODEL_DIR, model, 'model.meta')):\n        return jsonify({\"code\": 1, \"msg\": f\"{model} {cfg.transobj['lang4']}\"})\n    try:",
    "suffix": ""
  },
  {
    "name": "ali-vilab/dreamtalk:generators/face_model.py@1527",
    "canonical_solution": "        self, ",
    "prompt": "import functools\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport generators.flow_util as flow_util\nfrom generators.base_function import LayerNorm2d, ADAINHourglass, FineEncoder, FineDecoder\n\n\n\nclass FaceGenerator(nn.Module):\n    def __init__(\n        self, \n        mapping_net, \n        warpping_net, \n        editing_net, \n        common\n        ):  \n        super(FaceGenerator, self).__init__()\n        self.mapping_net = MappingNet(**mapping_net)\n        self.warpping_net = WarpingNet(**warpping_net, **common)\n        self.editing_net = EditingNet(**editing_net, **common)\n \n    def forward(\n        self, \n        input_image, \n        driving_source, \n        stage=None\n        ):\n        if stage == 'warp':\n            descriptor = self.mapping_net(driving_source)\n            output = self.warpping_net(input_image, descriptor)\n        else:\n            descriptor = self.mapping_net(driving_source)\n            output = self.warpping_net(input_image, descriptor)\n            output['fake_image'] = self.editing_net(input_image, output['warp_image'], descriptor)\n        return output\n\nclass MappingNet(nn.Module):\n    def __init__(self, coeff_nc, descriptor_nc, layer):\n        super( MappingNet, self).__init__()\n\n        self.layer = layer\n        nonlinearity = nn.LeakyReLU(0.1)\n\n        self.first = nn.Sequential(\n            torch.nn.Conv1d(coeff_nc, descriptor_nc, kernel_size=7, padding=0, bias=True))\n\n        for i in range(layer):\n            net = nn.Sequential(nonlinearity,\n                torch.nn.Conv1d(descriptor_nc, descriptor_nc, kernel_size=3, padding=0, dilation=3))\n            setattr(self, 'encoder' + str(i), net)   \n\n        self.pooling = nn.AdaptiveAvgPool1d(1)\n        self.output_nc = descriptor_nc\n\n    def forward(self, input_3dmm):\n        out = self.first(input_3dmm)\n        for i in range(self.layer):\n            model = getattr(self, 'encoder' + str(i))\n            out = model(out) + out[:,:,3:-3]\n        out = self.pooling(out)\n        return out   \n\nclass WarpingNet(nn.Module):\n    def __init__(",
    "prefix": "import functools\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport generators.flow_util as flow_util\nfrom generators.base_function import LayerNorm2d, ADAINHourglass, FineEncoder, FineDecoder\n\n\n\nclass FaceGenerator(nn.Module):\n    def __init__(\n        self, \n        mapping_net, \n        warpping_net, \n        editing_net, \n        common\n        ):  \n        super(FaceGenerator, self).__init__()\n        self.mapping_net = MappingNet(**mapping_net)\n        self.warpping_net = WarpingNet(**warpping_net, **common)\n        self.editing_net = EditingNet(**editing_net, **common)\n \n    def forward(\n        self, \n        input_image, \n        driving_source, \n        stage=None\n        ):\n        if stage == 'warp':\n            descriptor = self.mapping_net(driving_source)\n            output = self.warpping_net(input_image, descriptor)\n        else:\n            descriptor = self.mapping_net(driving_source)\n            output = self.warpping_net(input_image, descriptor)\n            output['fake_image'] = self.editing_net(input_image, output['warp_image'], descriptor)\n        return output\n\nclass MappingNet(nn.Module):\n    def __init__(self, coeff_nc, descriptor_nc, layer):\n        super( MappingNet, self).__init__()\n\n        self.layer = layer\n        nonlinearity = nn.LeakyReLU(0.1)\n\n        self.first = nn.Sequential(\n            torch.nn.Conv1d(coeff_nc, descriptor_nc, kernel_size=7, padding=0, bias=True))\n\n        for i in range(layer):\n            net = nn.Sequential(nonlinearity,\n                torch.nn.Conv1d(descriptor_nc, descriptor_nc, kernel_size=3, padding=0, dilation=3))\n            setattr(self, 'encoder' + str(i), net)   \n\n        self.pooling = nn.AdaptiveAvgPool1d(1)\n        self.output_nc = descriptor_nc\n\n    def forward(self, input_3dmm):\n        out = self.first(input_3dmm)\n        for i in range(self.layer):\n            model = getattr(self, 'encoder' + str(i))\n            out = model(out) + out[:,:,3:-3]\n        out = self.pooling(out)\n        return out   \n\nclass WarpingNet(nn.Module):\n    def __init__(",
    "suffix": ""
  },
  {
    "name": "jiawei-ren/dreamgaussian4d:diffusers/src/diffusers/dependency_versions_check.py@1072",
    "canonical_solution": "    else:",
    "prompt": "from .dependency_versions_table import deps\nfrom .utils.versions import require_version, require_version_core\n# Copyright 2023 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\n# define which module versions we always want to check at run time\n# (usually the ones defined in `install_requires` in setup.py)\n#\n# order specific notes:\n# - tqdm must be checked before tokenizers\n\npkgs_to_check_at_runtime = \"python requests filelock numpy\".split()\nfor pkg in pkgs_to_check_at_runtime:\n    if pkg in deps:\n        require_version_core(deps[pkg])",
    "prefix": "from .dependency_versions_table import deps\nfrom .utils.versions import require_version, require_version_core\n# Copyright 2023 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\n# define which module versions we always want to check at run time\n# (usually the ones defined in `install_requires` in setup.py)\n#\n# order specific notes:\n# - tqdm must be checked before tokenizers\n\npkgs_to_check_at_runtime = \"python requests filelock numpy\".split()\nfor pkg in pkgs_to_check_at_runtime:\n    if pkg in deps:\n        require_version_core(deps[pkg])",
    "suffix": ""
  },
  {
    "name": "oppo-us-research/SpacetimeGaussians:script/pre_n3d.py@1421",
    "canonical_solution": "    ctr = 0",
    "prompt": "import os \nimport cv2 \nimport glob \nimport tqdm \nimport numpy as np \nimport shutil\nimport pickle\nimport sys \nimport argparse\nfrom thirdparty.gaussian_splatting.utils.my_utils import posetow2c_matrcs, rotmat2qvec\nfrom thirdparty.colmap.pre_colmap import * \nfrom thirdparty.gaussian_splatting.helper3dg import getcolmapsinglen3d\n# MIT License\n\n# Copyright (c) 2023 OPPO\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nsys.path.append(\".\")\n\n\n\n\ndef extractframes(videopath):\n    cam = cv2.VideoCapture(videopath)\n    ctr = 0\n    sucess = True\n    for i in range(300):\n        if os.path.exists(os.path.join(videopath.replace(\".mp4\", \"\"), str(i) + \".png\")):\n            ctr += 1\n    if ctr == 300 or ctr == 150: # 150 for 04_truck \n        print(\"already extracted all the frames, skip extracting\")\n        return",
    "prefix": "import os \nimport cv2 \nimport glob \nimport tqdm \nimport numpy as np \nimport shutil\nimport pickle\nimport sys \nimport argparse\nfrom thirdparty.gaussian_splatting.utils.my_utils import posetow2c_matrcs, rotmat2qvec\nfrom thirdparty.colmap.pre_colmap import * \nfrom thirdparty.gaussian_splatting.helper3dg import getcolmapsinglen3d\n# MIT License\n\n# Copyright (c) 2023 OPPO\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nsys.path.append(\".\")\n\n\n\n\ndef extractframes(videopath):\n    cam = cv2.VideoCapture(videopath)\n    ctr = 0\n    sucess = True\n    for i in range(300):\n        if os.path.exists(os.path.join(videopath.replace(\".mp4\", \"\"), str(i) + \".png\")):\n            ctr += 1\n    if ctr == 300 or ctr == 150: # 150 for 04_truck \n        print(\"already extracted all the frames, skip extracting\")\n        return",
    "suffix": ""
  },
  {
    "name": "Meituan-AutoML/MobileVLM:mobilevlm/model/mobilevlm.py@1501",
    "canonical_solution": "                cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids[:half_len])",
    "prompt": "import torch\nimport torch.nn as nn\nfrom abc import ABC, abstractmethod\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\nfrom mobilevlm.model.vision_encoder import build_vision_tower\nfrom mobilevlm.model.vision_projector import build_vision_projector\nfrom mobilevlm.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, \\\n    DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n    from mobilevlm.model.mobilellama import MobileLlamaForCausalLM\n\n\nclass MobileVLMMetaModel:\n\n    def __init__(self, config):\n        super(MobileVLMMetaModel, self).__init__(config)\n        if hasattr(config, \"mm_vision_tower\"):  \n            self.vision_tower = build_vision_tower(config, delay_load=False)\n            self.mm_projector = build_vision_projector(config)\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, 'vision_tower', None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None):\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n        self.config.mm_vision_tower = model_args.vision_tower\n        self.config.use_mm_proj = True\n        self.config.mm_projector_type = getattr(model_args, 'mm_projector_type', 'linear')\n        self.config.mm_vision_select_layer = mm_vision_select_layer\n        self.config.mm_vision_select_feature = mm_vision_select_feature\n        # Build VisionTower\n        vision_tower = build_vision_tower(model_args)\n        if fsdp is not None and len(fsdp) > 0:\n            self.vision_tower = [vision_tower]\n        else:\n            self.vision_tower = vision_tower\n        self.config.mm_hidden_size = vision_tower.hidden_size\n        # Build Vision-Projector\n        self.mm_projector = build_vision_projector(self.config)\n        if pretrain_mm_mlp_adapter is not None:\n            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\n            def get_w(weights, keyword):\n                return {k.split(keyword + '.')[1]: v for k, v in weights.items() if keyword in k}\n            self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))\n\n\nclass MobileVLMMetaForCausalLM(ABC):\n\n    @abstractmethod\n    def get_model(self):\n        pass\n\n    def get_vision_tower(self):\n        return self.get_model().get_vision_tower()\n\n    def encode_images(self, images):\n        image_features = self.get_model().get_vision_tower()(images)\n        image_features = self.get_model().mm_projector(image_features)\n        return image_features\n\n    def prepare_inputs_labels_for_multimodal(\n        self, input_ids, attention_mask, past_key_values, labels, images\n    ):\n        vision_tower = self.get_vision_tower()\n        if vision_tower is None or images is None or input_ids.shape[1] == 1:\n            if past_key_values is not None and vision_tower is not None and images is not None and input_ids.shape[1] == 1:\n                attention_mask = torch.ones((attention_mask.shape[0], past_key_values[-1][-1].shape[-2] + 1), dtype=attention_mask.dtype, device=attention_mask.device)\n            return input_ids, attention_mask, past_key_values, None, labels\n\n        if type(images) is list or images.ndim == 5:\n            concat_images = torch.cat([image for image in images], dim=0)\n            image_features = self.encode_images(concat_images)\n            split_sizes = [image.shape[0] for image in images]\n            image_features = torch.split(image_features, split_sizes, dim=0)\n            image_features = [x.flatten(0, 1) for x in image_features]\n        else:\n            image_features = self.encode_images(images)\n\n        new_input_embeds = []\n        new_labels = [] if labels is not None else None\n        cur_image_idx = 0\n        for batch_idx, cur_input_ids in enumerate(input_ids):\n            if (cur_input_ids == IMAGE_TOKEN_INDEX).sum() == 0:\n                # multimodal LLM, but the current sample is not multimodal\n                # FIXME: this is a hacky fix, for deepspeed zero3 to work\n                half_len = cur_input_ids.shape[0] // 2\n                cur_image_features = image_features[cur_image_idx]",
    "prefix": "import torch\nimport torch.nn as nn\nfrom abc import ABC, abstractmethod\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\nfrom mobilevlm.model.vision_encoder import build_vision_tower\nfrom mobilevlm.model.vision_projector import build_vision_projector\nfrom mobilevlm.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, \\\n    DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n    from mobilevlm.model.mobilellama import MobileLlamaForCausalLM\n\n\nclass MobileVLMMetaModel:\n\n    def __init__(self, config):\n        super(MobileVLMMetaModel, self).__init__(config)\n        if hasattr(config, \"mm_vision_tower\"):  \n            self.vision_tower = build_vision_tower(config, delay_load=False)\n            self.mm_projector = build_vision_projector(config)\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, 'vision_tower', None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None):\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n        self.config.mm_vision_tower = model_args.vision_tower\n        self.config.use_mm_proj = True\n        self.config.mm_projector_type = getattr(model_args, 'mm_projector_type', 'linear')\n        self.config.mm_vision_select_layer = mm_vision_select_layer\n        self.config.mm_vision_select_feature = mm_vision_select_feature\n        # Build VisionTower\n        vision_tower = build_vision_tower(model_args)\n        if fsdp is not None and len(fsdp) > 0:\n            self.vision_tower = [vision_tower]\n        else:\n            self.vision_tower = vision_tower\n        self.config.mm_hidden_size = vision_tower.hidden_size\n        # Build Vision-Projector\n        self.mm_projector = build_vision_projector(self.config)\n        if pretrain_mm_mlp_adapter is not None:\n            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\n            def get_w(weights, keyword):\n                return {k.split(keyword + '.')[1]: v for k, v in weights.items() if keyword in k}\n            self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))\n\n\nclass MobileVLMMetaForCausalLM(ABC):\n\n    @abstractmethod\n    def get_model(self):\n        pass\n\n    def get_vision_tower(self):\n        return self.get_model().get_vision_tower()\n\n    def encode_images(self, images):\n        image_features = self.get_model().get_vision_tower()(images)\n        image_features = self.get_model().mm_projector(image_features)\n        return image_features\n\n    def prepare_inputs_labels_for_multimodal(\n        self, input_ids, attention_mask, past_key_values, labels, images\n    ):\n        vision_tower = self.get_vision_tower()\n        if vision_tower is None or images is None or input_ids.shape[1] == 1:\n            if past_key_values is not None and vision_tower is not None and images is not None and input_ids.shape[1] == 1:\n                attention_mask = torch.ones((attention_mask.shape[0], past_key_values[-1][-1].shape[-2] + 1), dtype=attention_mask.dtype, device=attention_mask.device)\n            return input_ids, attention_mask, past_key_values, None, labels\n\n        if type(images) is list or images.ndim == 5:\n            concat_images = torch.cat([image for image in images], dim=0)\n            image_features = self.encode_images(concat_images)\n            split_sizes = [image.shape[0] for image in images]\n            image_features = torch.split(image_features, split_sizes, dim=0)\n            image_features = [x.flatten(0, 1) for x in image_features]\n        else:\n            image_features = self.encode_images(images)\n\n        new_input_embeds = []\n        new_labels = [] if labels is not None else None\n        cur_image_idx = 0\n        for batch_idx, cur_input_ids in enumerate(input_ids):\n            if (cur_input_ids == IMAGE_TOKEN_INDEX).sum() == 0:\n                # multimodal LLM, but the current sample is not multimodal\n                # FIXME: this is a hacky fix, for deepspeed zero3 to work\n                half_len = cur_input_ids.shape[0] // 2\n                cur_image_features = image_features[cur_image_idx]",
    "suffix": ""
  },
  {
    "name": "kinggongzilla/ai-clone-whatsapp:utils/config_utils.py@1459",
    "canonical_solution": "                        print(f\"Warning: {config_name} does not accept parameter: {k}\")",
    "prompt": "import inspect\nimport torch.distributed as dist\nfrom dataclasses import asdict\nfrom torch.utils.data import DistributedSampler\nfrom peft import (\n    LoraConfig,\n    AdaptionPromptConfig,\n    PrefixTuningConfig,\n)\nfrom transformers import default_data_collator\nfrom transformers.data import DataCollatorForSeq2Seq\nfrom configs import datasets, lora_config, llama_adapter_config, prefix_config, train_config\nfrom data.sampler import LengthBasedBatchSampler, DistributedLengthBasedBatchSampler\nfrom utils.dataset_utils import DATASET_PREPROC\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\n\n\n\n\ndef update_config(config, **kwargs):\n    if isinstance(config, (tuple, list)):\n        for c in config:\n            update_config(c, **kwargs)\n    else:\n        for k, v in kwargs.items():\n            if hasattr(config, k):\n                setattr(config, k, v)\n            elif \".\" in k:\n                # allow --some_config.some_param=True\n                config_name, param_name = k.split(\".\")\n                if type(config).__name__ == config_name:\n                    if hasattr(config, param_name):\n                        setattr(config, param_name, v)\n                    else:\n                        # In case of specialized config we can warm user",
    "prefix": "import inspect\nimport torch.distributed as dist\nfrom dataclasses import asdict\nfrom torch.utils.data import DistributedSampler\nfrom peft import (\n    LoraConfig,\n    AdaptionPromptConfig,\n    PrefixTuningConfig,\n)\nfrom transformers import default_data_collator\nfrom transformers.data import DataCollatorForSeq2Seq\nfrom configs import datasets, lora_config, llama_adapter_config, prefix_config, train_config\nfrom data.sampler import LengthBasedBatchSampler, DistributedLengthBasedBatchSampler\nfrom utils.dataset_utils import DATASET_PREPROC\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\n\n\n\n\ndef update_config(config, **kwargs):\n    if isinstance(config, (tuple, list)):\n        for c in config:\n            update_config(c, **kwargs)\n    else:\n        for k, v in kwargs.items():\n            if hasattr(config, k):\n                setattr(config, k, v)\n            elif \".\" in k:\n                # allow --some_config.some_param=True\n                config_name, param_name = k.split(\".\")\n                if type(config).__name__ == config_name:\n                    if hasattr(config, param_name):\n                        setattr(config, param_name, v)\n                    else:\n                        # In case of specialized config we can warm user",
    "suffix": ""
  },
  {
    "name": "FoundationVision/UniRef:detectron2/evaluation/sem_seg_evaluation.py@1256",
    "canonical_solution": ") -> np.ndarray:",
    "prompt": "import itertools\nimport json\nimport logging\nimport os\nimport numpy as np\nimport pycocotools.mask as mask_util\nimport torch\nfrom collections import OrderedDict\nfrom detectron2.data import DatasetCatalog, MetadataCatalog\nfrom detectron2.utils.comm import all_gather, is_main_process, synchronize\nfrom detectron2.utils.file_io import PathManager\nfrom .evaluator import DatasetEvaluator\nfrom PIL import Image\nfrom typing import Union, Optional\n# Copyright (c) Facebook, Inc. and its affiliates.\n\n\n\n\ndef load_image_into_numpy_array(\n    filename: str,\n    copy: bool = False,\n    dtype: Optional[Union[np.dtype, str]] = None,",
    "prefix": "import itertools\nimport json\nimport logging\nimport os\nimport numpy as np\nimport pycocotools.mask as mask_util\nimport torch\nfrom collections import OrderedDict\nfrom detectron2.data import DatasetCatalog, MetadataCatalog\nfrom detectron2.utils.comm import all_gather, is_main_process, synchronize\nfrom detectron2.utils.file_io import PathManager\nfrom .evaluator import DatasetEvaluator\nfrom PIL import Image\nfrom typing import Union, Optional\n# Copyright (c) Facebook, Inc. and its affiliates.\n\n\n\n\ndef load_image_into_numpy_array(\n    filename: str,\n    copy: bool = False,\n    dtype: Optional[Union[np.dtype, str]] = None,",
    "suffix": ""
  },
  {
    "name": "xhuangcv/humannorm:threestudio/models/materials/no_material.py@1064",
    "canonical_solution": "    cfg: Config",
    "prompt": "import random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport threestudio\nfrom dataclasses import dataclass, field\nfrom threestudio.models.materials.base import BaseMaterial\nfrom threestudio.models.networks import get_encoding, get_mlp\nfrom threestudio.utils.ops import dot, get_activation\nfrom threestudio.utils.typing import *\n\n\n\n\n@threestudio.register(\"no-material\")\nclass NoMaterial(BaseMaterial):\n    @dataclass\n    class Config(BaseMaterial.Config):\n        n_output_dims: int = 3\n        color_activation: str = \"sigmoid\"\n        input_feature_dims: Optional[int] = None\n        mlp_network_config: Optional[dict] = None\n",
    "prefix": "import random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport threestudio\nfrom dataclasses import dataclass, field\nfrom threestudio.models.materials.base import BaseMaterial\nfrom threestudio.models.networks import get_encoding, get_mlp\nfrom threestudio.utils.ops import dot, get_activation\nfrom threestudio.utils.typing import *\n\n\n\n\n@threestudio.register(\"no-material\")\nclass NoMaterial(BaseMaterial):\n    @dataclass\n    class Config(BaseMaterial.Config):\n        n_output_dims: int = 3\n        color_activation: str = \"sigmoid\"\n        input_feature_dims: Optional[int] = None\n        mlp_network_config: Optional[dict] = None\n",
    "suffix": ""
  },
  {
    "name": "jianchang512/stt:start.py@1410",
    "canonical_solution": "            text = segment.text.strip().replace('&#39;', \"'\")",
    "prompt": "import logging\nimport re\nimport threading\nimport sys\nimport torch\nimport os\nfrom flask import Flask, request, render_template, jsonify, send_from_directory\nfrom gevent.pywsgi import WSGIServer, WSGIHandler, LoggingLogAdapter\nfrom logging.handlers import RotatingFileHandler\nfrom stslib import cfg, tool\nfrom stslib.cfg import ROOT_DIR\nfrom faster_whisper import WhisperModel\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass CustomRequestHandler(WSGIHandler):\n    def log_request(self):\n        pass\n\n\n# \u914d\u7f6e\u65e5\u5fd7\n# \u7981\u7528 Werkzeug \u9ed8\u8ba4\u7684\u65e5\u5fd7\u5904\u7406\u5668\nlog = logging.getLogger('werkzeug')\nlog.handlers[:] = []\nlog.setLevel(logging.WARNING)\napp = Flask(__name__, static_folder=os.path.join(ROOT_DIR, 'static'), static_url_path='/static',\n            template_folder=os.path.join(ROOT_DIR, 'templates'))\nroot_log = logging.getLogger()  # Flask\u7684\u6839\u65e5\u5fd7\u8bb0\u5f55\u5668\nroot_log.handlers = []\nroot_log.setLevel(logging.WARNING)\n\n# \u914d\u7f6e\u65e5\u5fd7\napp.logger.setLevel(logging.WARNING)  # \u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b\u4e3a INFO\n# \u521b\u5efa RotatingFileHandler \u5bf9\u8c61\uff0c\u8bbe\u7f6e\u5199\u5165\u7684\u6587\u4ef6\u8def\u5f84\u548c\u5927\u5c0f\u9650\u5236\nfile_handler = RotatingFileHandler(os.path.join(ROOT_DIR, 'sts.log'), maxBytes=1024 * 1024, backupCount=5)\n# \u521b\u5efa\u65e5\u5fd7\u7684\u683c\u5f0f\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n# \u8bbe\u7f6e\u6587\u4ef6\u5904\u7406\u5668\u7684\u7ea7\u522b\u548c\u683c\u5f0f\nfile_handler.setLevel(logging.WARNING)\nfile_handler.setFormatter(formatter)\n# \u5c06\u6587\u4ef6\u5904\u7406\u5668\u6dfb\u52a0\u5230\u65e5\u5fd7\u8bb0\u5f55\u5668\u4e2d\napp.logger.addHandler(file_handler)\n\n\n@app.route('/static/<path:filename>')\ndef static_files(filename):\n    return send_from_directory(app.config['STATIC_FOLDER'], filename)\n\n\n@app.route('/')\ndef index():\n    return render_template(\"index.html\",\n                           cuda=cfg.cuda,\n                           lang_code=cfg.lang_code,\n                           language=cfg.LANG,\n                           root_dir=ROOT_DIR.replace('\\\\', '/'))\n\n\n# \u4e0a\u4f20\u97f3\u9891\n@app.route('/upload', methods=['POST'])\ndef upload():\n    try:\n        # \u83b7\u53d6\u4e0a\u4f20\u7684\u6587\u4ef6\n        audio_file = request.files['audio']\n        # \u5982\u679c\u662fmp4\n        noextname, ext = os.path.splitext(audio_file.filename)\n        ext = ext.lower()\n        # \u5982\u679c\u662f\u89c6\u9891\uff0c\u5148\u5206\u79bb\n        wav_file = os.path.join(cfg.TMP_DIR, f'{noextname}.wav')\n        if os.path.exists(wav_file) and os.path.getsize(wav_file) > 0:\n            return jsonify({'code': 0, 'msg': cfg.transobj['lang1'], \"data\": os.path.basename(wav_file)})\n        msg = \"\"\n        if ext in ['.mp4', '.mov', '.avi', '.mkv', '.mpeg', '.mp3', '.flac']:\n            video_file = os.path.join(cfg.TMP_DIR, f'{noextname}{ext}')\n            audio_file.save(video_file)\n            params = [\n                \"-i\",\n                video_file,\n            ]\n            if ext not in ['.mp3', '.flac']:\n                params.append('-vn')\n            params.append(wav_file)\n            rs = tool.runffmpeg(params)\n            if rs != 'ok':\n                return jsonify({\"code\": 1, \"msg\": rs})\n            msg = \",\" + cfg.transobj['lang9']\n        elif ext == '.wav':\n            audio_file.save(wav_file)\n        else:\n            return jsonify({\"code\": 1, \"msg\": f\"{cfg.transobj['lang3']} {ext}\"})\n\n        # \u8fd4\u56de\u6210\u529f\u7684\u54cd\u5e94\n        return jsonify({'code': 0, 'msg': cfg.transobj['lang1'] + msg, \"data\": os.path.basename(wav_file)})\n    except Exception as e:\n        app.logger.error(f'[upload]error: {e}')\n        return jsonify({'code': 2, 'msg': cfg.transobj['lang2']})\n\n\n# \u6839\u636e\u6587\u672c\u8fd4\u56detts\u7ed3\u679c\uff0c\u8fd4\u56de name=\u6587\u4ef6\u540d\u5b57\uff0cfilename=\u6587\u4ef6\u7edd\u5bf9\u8def\u5f84\n# \u8bf7\u6c42\u7aef\u6839\u636e\u9700\u8981\u81ea\u884c\u9009\u62e9\u4f7f\u7528\u54ea\u4e2a\n# params\n# wav_name:tmp\u4e0b\u7684wav\u6587\u4ef6\n# model \u6a21\u578b\u540d\u79f0\n@app.route('/process', methods=['GET', 'POST'])\ndef process():\n    # \u539f\u59cb\u5b57\u7b26\u4e32\n    wav_name = request.form.get(\"wav_name\").strip()\n    model = request.form.get(\"model\")\n    # \u8bed\u8a00\n    language = request.form.get(\"language\")\n    # \u8fd4\u56de\u683c\u5f0f json txt srt\n    data_type = request.form.get(\"data_type\")\n    wav_file = os.path.join(cfg.TMP_DIR, wav_name)\n    if not os.path.exists(wav_file):\n        return jsonify({\"code\": 1, \"msg\": f\"{wav_file} {cfg.langlist['lang5']}\"})\n    if not os.path.exists(os.path.join(cfg.MODEL_DIR, f'models--Systran--faster-whisper-{model}/snapshots/')):\n        return jsonify({\"code\": 1, \"msg\": f\"{model} {cfg.transobj['lang4']}\"})\n\n    try:\n        model = WhisperModel(model, device=device, compute_type=\"int8\", download_root=cfg.ROOT_DIR + \"/models\")\n        #model = whisper.load_model(model, download_root=cfg.ROOT_DIR + \"/models\")\n        segments,_ = model.transcribe(wav_file, beam_size=5,  vad_filter=True,\n    vad_parameters=dict(min_silence_duration_ms=500),language=language)\n        #segments = transcribe\n        raw_subtitles = []\n        #sidx=0\n        for segment in segments:\n            start = int(segment.start * 1000)\n            end = int(segment.end * 1000)\n            startTime = tool.ms_to_time_string(ms=start)\n            endTime = tool.ms_to_time_string(ms=end)",
    "prefix": "import logging\nimport re\nimport threading\nimport sys\nimport torch\nimport os\nfrom flask import Flask, request, render_template, jsonify, send_from_directory\nfrom gevent.pywsgi import WSGIServer, WSGIHandler, LoggingLogAdapter\nfrom logging.handlers import RotatingFileHandler\nfrom stslib import cfg, tool\nfrom stslib.cfg import ROOT_DIR\nfrom faster_whisper import WhisperModel\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass CustomRequestHandler(WSGIHandler):\n    def log_request(self):\n        pass\n\n\n# \u914d\u7f6e\u65e5\u5fd7\n# \u7981\u7528 Werkzeug \u9ed8\u8ba4\u7684\u65e5\u5fd7\u5904\u7406\u5668\nlog = logging.getLogger('werkzeug')\nlog.handlers[:] = []\nlog.setLevel(logging.WARNING)\napp = Flask(__name__, static_folder=os.path.join(ROOT_DIR, 'static'), static_url_path='/static',\n            template_folder=os.path.join(ROOT_DIR, 'templates'))\nroot_log = logging.getLogger()  # Flask\u7684\u6839\u65e5\u5fd7\u8bb0\u5f55\u5668\nroot_log.handlers = []\nroot_log.setLevel(logging.WARNING)\n\n# \u914d\u7f6e\u65e5\u5fd7\napp.logger.setLevel(logging.WARNING)  # \u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b\u4e3a INFO\n# \u521b\u5efa RotatingFileHandler \u5bf9\u8c61\uff0c\u8bbe\u7f6e\u5199\u5165\u7684\u6587\u4ef6\u8def\u5f84\u548c\u5927\u5c0f\u9650\u5236\nfile_handler = RotatingFileHandler(os.path.join(ROOT_DIR, 'sts.log'), maxBytes=1024 * 1024, backupCount=5)\n# \u521b\u5efa\u65e5\u5fd7\u7684\u683c\u5f0f\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n# \u8bbe\u7f6e\u6587\u4ef6\u5904\u7406\u5668\u7684\u7ea7\u522b\u548c\u683c\u5f0f\nfile_handler.setLevel(logging.WARNING)\nfile_handler.setFormatter(formatter)\n# \u5c06\u6587\u4ef6\u5904\u7406\u5668\u6dfb\u52a0\u5230\u65e5\u5fd7\u8bb0\u5f55\u5668\u4e2d\napp.logger.addHandler(file_handler)\n\n\n@app.route('/static/<path:filename>')\ndef static_files(filename):\n    return send_from_directory(app.config['STATIC_FOLDER'], filename)\n\n\n@app.route('/')\ndef index():\n    return render_template(\"index.html\",\n                           cuda=cfg.cuda,\n                           lang_code=cfg.lang_code,\n                           language=cfg.LANG,\n                           root_dir=ROOT_DIR.replace('\\\\', '/'))\n\n\n# \u4e0a\u4f20\u97f3\u9891\n@app.route('/upload', methods=['POST'])\ndef upload():\n    try:\n        # \u83b7\u53d6\u4e0a\u4f20\u7684\u6587\u4ef6\n        audio_file = request.files['audio']\n        # \u5982\u679c\u662fmp4\n        noextname, ext = os.path.splitext(audio_file.filename)\n        ext = ext.lower()\n        # \u5982\u679c\u662f\u89c6\u9891\uff0c\u5148\u5206\u79bb\n        wav_file = os.path.join(cfg.TMP_DIR, f'{noextname}.wav')\n        if os.path.exists(wav_file) and os.path.getsize(wav_file) > 0:\n            return jsonify({'code': 0, 'msg': cfg.transobj['lang1'], \"data\": os.path.basename(wav_file)})\n        msg = \"\"\n        if ext in ['.mp4', '.mov', '.avi', '.mkv', '.mpeg', '.mp3', '.flac']:\n            video_file = os.path.join(cfg.TMP_DIR, f'{noextname}{ext}')\n            audio_file.save(video_file)\n            params = [\n                \"-i\",\n                video_file,\n            ]\n            if ext not in ['.mp3', '.flac']:\n                params.append('-vn')\n            params.append(wav_file)\n            rs = tool.runffmpeg(params)\n            if rs != 'ok':\n                return jsonify({\"code\": 1, \"msg\": rs})\n            msg = \",\" + cfg.transobj['lang9']\n        elif ext == '.wav':\n            audio_file.save(wav_file)\n        else:\n            return jsonify({\"code\": 1, \"msg\": f\"{cfg.transobj['lang3']} {ext}\"})\n\n        # \u8fd4\u56de\u6210\u529f\u7684\u54cd\u5e94\n        return jsonify({'code': 0, 'msg': cfg.transobj['lang1'] + msg, \"data\": os.path.basename(wav_file)})\n    except Exception as e:\n        app.logger.error(f'[upload]error: {e}')\n        return jsonify({'code': 2, 'msg': cfg.transobj['lang2']})\n\n\n# \u6839\u636e\u6587\u672c\u8fd4\u56detts\u7ed3\u679c\uff0c\u8fd4\u56de name=\u6587\u4ef6\u540d\u5b57\uff0cfilename=\u6587\u4ef6\u7edd\u5bf9\u8def\u5f84\n# \u8bf7\u6c42\u7aef\u6839\u636e\u9700\u8981\u81ea\u884c\u9009\u62e9\u4f7f\u7528\u54ea\u4e2a\n# params\n# wav_name:tmp\u4e0b\u7684wav\u6587\u4ef6\n# model \u6a21\u578b\u540d\u79f0\n@app.route('/process', methods=['GET', 'POST'])\ndef process():\n    # \u539f\u59cb\u5b57\u7b26\u4e32\n    wav_name = request.form.get(\"wav_name\").strip()\n    model = request.form.get(\"model\")\n    # \u8bed\u8a00\n    language = request.form.get(\"language\")\n    # \u8fd4\u56de\u683c\u5f0f json txt srt\n    data_type = request.form.get(\"data_type\")\n    wav_file = os.path.join(cfg.TMP_DIR, wav_name)\n    if not os.path.exists(wav_file):\n        return jsonify({\"code\": 1, \"msg\": f\"{wav_file} {cfg.langlist['lang5']}\"})\n    if not os.path.exists(os.path.join(cfg.MODEL_DIR, f'models--Systran--faster-whisper-{model}/snapshots/')):\n        return jsonify({\"code\": 1, \"msg\": f\"{model} {cfg.transobj['lang4']}\"})\n\n    try:\n        model = WhisperModel(model, device=device, compute_type=\"int8\", download_root=cfg.ROOT_DIR + \"/models\")\n        #model = whisper.load_model(model, download_root=cfg.ROOT_DIR + \"/models\")\n        segments,_ = model.transcribe(wav_file, beam_size=5,  vad_filter=True,\n    vad_parameters=dict(min_silence_duration_ms=500),language=language)\n        #segments = transcribe\n        raw_subtitles = []\n        #sidx=0\n        for segment in segments:\n            start = int(segment.start * 1000)\n            end = int(segment.end * 1000)\n            startTime = tool.ms_to_time_string(ms=start)\n            endTime = tool.ms_to_time_string(ms=end)",
    "suffix": ""
  },
  {
    "name": "neobundy/MLX-Stable-Diffusion-WebUI:model_inspector.py@799",
    "canonical_solution": "def write_to_file(*args, **kwargs):",
    "prompt": "from stable_diffusion.config import PathConfig\nfrom stable_diffusion.model_io import preload_models_from_safetensor_weights\nfrom utils import _state_dict\nfrom utils import get_state_dict_from_safetensor\n\n\nINSPECTION_FILE = \"model_inspection.txt\"\nNUM_ITEMS = 100\n\nMODEL_FILE = \"./models/v2-1_512-ema-pruned.safetensors\"\nMODEL_FILE1 = \"./unet/diffusion_pytorch_model_test.safetensors\"\nMODEL_FILE2 = \"./unet/xxmix9realistic_v40.safetensors\"\n\n\n\n# Recreate the inspection file at every execution of the script\nwith open(INSPECTION_FILE, 'w') as f:\n    pass\n",
    "prefix": "from stable_diffusion.config import PathConfig\nfrom stable_diffusion.model_io import preload_models_from_safetensor_weights\nfrom utils import _state_dict\nfrom utils import get_state_dict_from_safetensor\n\n\nINSPECTION_FILE = \"model_inspection.txt\"\nNUM_ITEMS = 100\n\nMODEL_FILE = \"./models/v2-1_512-ema-pruned.safetensors\"\nMODEL_FILE1 = \"./unet/diffusion_pytorch_model_test.safetensors\"\nMODEL_FILE2 = \"./unet/xxmix9realistic_v40.safetensors\"\n\n\n\n# Recreate the inspection file at every execution of the script\nwith open(INSPECTION_FILE, 'w') as f:\n    pass\n",
    "suffix": ""
  },
  {
    "name": "ffmemes/ff-backend:src/storage/ocr/mystic.py@690",
    "canonical_solution": "    print(f\"OCR result from Mystic: {ocr_result}\")",
    "prompt": "import uuid\nimport httpx\nfrom typing import Any\nfrom src.config import settings\nfrom src.storage.schemas import OcrResult\n\n\n# URL for the API endpoint\nurl = 'https://www.mystic.ai/v3/runs'\n\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": f\"Bearer {settings.MYSTIC_TOKEN}\"\n}\n\nPIPELINE_ID = \"uriel/easyocr-r:v30\"\n\n\nasync def load_file_to_mystic(file_content: bytes) -> str:\n    file_name = f\"{uuid.uuid4()}.jpg\"\n    files = { \"pfile\": (file_name, file_content, \"image/jpeg\") }\n\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            \"https://www.mystic.ai/v3/pipeline_files\",\n            files=files,\n            headers=headers,\n        )\n        response.raise_for_status()\n        return response.json()[\"path\"]\n    \n\nasync def ocr_mystic_file_path(\n    mystic_file_path: str,\n    pipeline_id: str = PIPELINE_ID,\n    language: str = \"Russian\",\n) -> dict[str, Any]:\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            url,\n            json={\n                \"pipeline_id_or_pointer\": pipeline_id,\n                \"async_run\": False,\n                \"input_data\": [\n                    {\n                        \"type\": \"file\",\n                        \"value\": \"\",\n                        \"file_path\": mystic_file_path,\n                    },\n                    {\n                        \"type\": \"string\",\n                        \"value\": language\n                    }\n                ]\n            },\n            headers=headers,\n        )\n        response.raise_for_status()\n        return response.json()\n\n\nasync def ocr_content(\n    content: bytes,  # ??\n) -> OcrResult | None:\n    try:\n        mystic_file_path = await load_file_to_mystic(content)\n        ocr_result = await ocr_mystic_file_path(mystic_file_path)\n    except Exception as e:\n        print(f\"Mystic OCR error: {e}\")\n        return None",
    "prefix": "import uuid\nimport httpx\nfrom typing import Any\nfrom src.config import settings\nfrom src.storage.schemas import OcrResult\n\n\n# URL for the API endpoint\nurl = 'https://www.mystic.ai/v3/runs'\n\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": f\"Bearer {settings.MYSTIC_TOKEN}\"\n}\n\nPIPELINE_ID = \"uriel/easyocr-r:v30\"\n\n\nasync def load_file_to_mystic(file_content: bytes) -> str:\n    file_name = f\"{uuid.uuid4()}.jpg\"\n    files = { \"pfile\": (file_name, file_content, \"image/jpeg\") }\n\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            \"https://www.mystic.ai/v3/pipeline_files\",\n            files=files,\n            headers=headers,\n        )\n        response.raise_for_status()\n        return response.json()[\"path\"]\n    \n\nasync def ocr_mystic_file_path(\n    mystic_file_path: str,\n    pipeline_id: str = PIPELINE_ID,\n    language: str = \"Russian\",\n) -> dict[str, Any]:\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            url,\n            json={\n                \"pipeline_id_or_pointer\": pipeline_id,\n                \"async_run\": False,\n                \"input_data\": [\n                    {\n                        \"type\": \"file\",\n                        \"value\": \"\",\n                        \"file_path\": mystic_file_path,\n                    },\n                    {\n                        \"type\": \"string\",\n                        \"value\": language\n                    }\n                ]\n            },\n            headers=headers,\n        )\n        response.raise_for_status()\n        return response.json()\n\n\nasync def ocr_content(\n    content: bytes,  # ??\n) -> OcrResult | None:\n    try:\n        mystic_file_path = await load_file_to_mystic(content)\n        ocr_result = await ocr_mystic_file_path(mystic_file_path)\n    except Exception as e:\n        print(f\"Mystic OCR error: {e}\")\n        return None",
    "suffix": ""
  },
  {
    "name": "Con6924/SPM:src/evaluation/clip_evaluator.py@1337",
    "canonical_solution": "                    concept,",
    "prompt": "import json\nimport os\nimport random\nfrom argparse import ArgumentParser\nfrom prettytable import PrettyTable\nfrom tqdm import tqdm\nfrom src.configs.generation_config import GenerationConfig\nfrom ..misc.clip_templates import anchor_templates, imagenet_templates\nfrom .eval_util import clip_eval_by_image\nfrom .evaluator import Evaluator, GenerationDataset\n\n\n\n\nclass ClipTemplateDataset(GenerationDataset):\n    def __init__(\n        self,\n        concepts: list[str],\n        save_folder: str = \"benchmark/generated_imgs/\",\n        base_cfg: GenerationConfig = GenerationConfig(),\n        num_templates: int = 80,\n        num_images_per_template: int = 10,\n        **kwargs\n    ):\n        assert 1 <= num_templates <= 80, \"num_templates should be in range(1, 81).\"\n        meta = {}\n        self.data = []\n        for concept in concepts:\n            meta[concept] = {}\n            sampled_template_indices = random.sample(range(80), num_templates)\n            for template_idx in sampled_template_indices:\n                # construct cfg\n                cfg = base_cfg.copy()\n                cfg.prompts = [imagenet_templates[template_idx].format(concept)]\n                cfg.generate_num = num_images_per_template\n                cfg.save_path = os.path.join(\n                    save_folder,",
    "prefix": "import json\nimport os\nimport random\nfrom argparse import ArgumentParser\nfrom prettytable import PrettyTable\nfrom tqdm import tqdm\nfrom src.configs.generation_config import GenerationConfig\nfrom ..misc.clip_templates import anchor_templates, imagenet_templates\nfrom .eval_util import clip_eval_by_image\nfrom .evaluator import Evaluator, GenerationDataset\n\n\n\n\nclass ClipTemplateDataset(GenerationDataset):\n    def __init__(\n        self,\n        concepts: list[str],\n        save_folder: str = \"benchmark/generated_imgs/\",\n        base_cfg: GenerationConfig = GenerationConfig(),\n        num_templates: int = 80,\n        num_images_per_template: int = 10,\n        **kwargs\n    ):\n        assert 1 <= num_templates <= 80, \"num_templates should be in range(1, 81).\"\n        meta = {}\n        self.data = []\n        for concept in concepts:\n            meta[concept] = {}\n            sampled_template_indices = random.sample(range(80), num_templates)\n            for template_idx in sampled_template_indices:\n                # construct cfg\n                cfg = base_cfg.copy()\n                cfg.prompts = [imagenet_templates[template_idx].format(concept)]\n                cfg.generate_num = num_images_per_template\n                cfg.save_path = os.path.join(\n                    save_folder,",
    "suffix": ""
  },
  {
    "name": "theOneAndOnlyOne/BeReel:main.py@1139",
    "canonical_solution": "    else:",
    "prompt": "import os\nimport requests\nfrom flask import Flask, render_template, request, jsonify\nfrom combineImages import create_images\nfrom generateSlideshow import buildSlideshow\nfrom recap import butidRecap\nfrom datetime import datetime\n\napp = Flask(__name__, template_folder=\"templates\")\n\n\n# Acquire Phone Number from User\ndef send_code(phone):\n    print(\"> Entered phone number is \", phone)\n    # First Post to send out OTP session and code\n    url_send_code = \"https://berealapi.fly.dev/login/send-code\"\n\n    # IMPORTANT: Format must be +##########\n    payload = {\"phone\": phone}\n\n    print(\"-- Sending OTP Session Request --\")\n    response = requests.post(url_send_code, json=payload)\n    otp_session = \"n/a\"\n\n    if response.status_code == 201:\n        print(\"> Request successful!\")\n        print(\"Response:\", response.json())\n        response_json = response.json()\n        if \"data\" in response_json and \"otpSession\" in response_json[\"data\"]:\n            otp_session = response_json[\"data\"][\"otpSession\"]\n            print(\"OTP Session:\", otp_session)\n        else:\n            print(\"No 'otpSession' found in the response.\")\n    else:\n        print(\"Request failed with status code:\", response.status_code)\n        print(response.json())\n\n    return otp_session\n\n\n# Verify Session using otp_session code and user entered otp_code recieved from phone notification\ndef verify(otp_session, otp_code):\n    # print(\"please enter OTP code\")\n    # otp_code = input()\n    print(\"> OTP: \", otp_code)\n\n    # Second POST request to verify base don user input\n    url_verify = \"https://berealapi.fly.dev/login/verify\"\n\n    payload_verify = {\"code\": otp_code, \"otpSession\": otp_session}\n\n    print(\"-- Sending Verify Request --\")\n    response_verify = requests.post(url_verify, json=payload_verify)\n    tokenObj = \"n/a\"\n\n    if response_verify.status_code == 201:\n        print(\"> Verification request successful!\")\n        print(\"Response:\", response_verify.json())\n        # Process the verification response if needed\n        response_json = response_verify.json()\n        if \"data\" in response_json and \"token\" in response_json[\"data\"]:\n            tokenObj = response_json[\"data\"][\"token\"]\n            print(\"tokenObj:\", tokenObj)\n        else:\n            print(\"No 'tokenObj' found in the response.\")\n            exit()\n    else:\n        print(\n            \"> Verification request failed with status code:\",\n            response_verify.status_code,\n        )\n        print(response_verify.json())\n        exit()\n\n    return tokenObj\n\n\n# Fetch user memories. Skip to this stage if we already acquired reusable token\ndef get_memories(tokenObj, start_date_range, end_date_range):\n    url_mem_feed = \"https://berealapi.fly.dev/friends/mem-feed\"\n    headers = {\"token\": tokenObj}\n\n    # Create a folder named 'primary' if it doesn't exist\n    folder_name = \"primary\"\n    if not os.path.exists(folder_name):\n        os.makedirs(folder_name)\n\n    # Create a folder named 'secondary' if it doesn't exist\n    secondary_folder_name = \"secondary\"\n    if not os.path.exists(secondary_folder_name):\n        os.makedirs(secondary_folder_name)\n\n    print(\"-- Sending Get Memories Request --\")\n    response_mem_feed = requests.get(url_mem_feed, headers=headers)\n    data_array = []\n\n    if response_mem_feed.status_code == 200:\n        print(\"> GET request successful!\")\n        # Process the response from mem-feed endpoint\n        print(\"Response:\", response_mem_feed.json())\n        print(\"we did it yay\")\n        response_data = response_mem_feed.json().get(\"data\", {})\n        data_array = response_data.get(\"data\", [])\n",
    "prefix": "import os\nimport requests\nfrom flask import Flask, render_template, request, jsonify\nfrom combineImages import create_images\nfrom generateSlideshow import buildSlideshow\nfrom recap import butidRecap\nfrom datetime import datetime\n\napp = Flask(__name__, template_folder=\"templates\")\n\n\n# Acquire Phone Number from User\ndef send_code(phone):\n    print(\"> Entered phone number is \", phone)\n    # First Post to send out OTP session and code\n    url_send_code = \"https://berealapi.fly.dev/login/send-code\"\n\n    # IMPORTANT: Format must be +##########\n    payload = {\"phone\": phone}\n\n    print(\"-- Sending OTP Session Request --\")\n    response = requests.post(url_send_code, json=payload)\n    otp_session = \"n/a\"\n\n    if response.status_code == 201:\n        print(\"> Request successful!\")\n        print(\"Response:\", response.json())\n        response_json = response.json()\n        if \"data\" in response_json and \"otpSession\" in response_json[\"data\"]:\n            otp_session = response_json[\"data\"][\"otpSession\"]\n            print(\"OTP Session:\", otp_session)\n        else:\n            print(\"No 'otpSession' found in the response.\")\n    else:\n        print(\"Request failed with status code:\", response.status_code)\n        print(response.json())\n\n    return otp_session\n\n\n# Verify Session using otp_session code and user entered otp_code recieved from phone notification\ndef verify(otp_session, otp_code):\n    # print(\"please enter OTP code\")\n    # otp_code = input()\n    print(\"> OTP: \", otp_code)\n\n    # Second POST request to verify base don user input\n    url_verify = \"https://berealapi.fly.dev/login/verify\"\n\n    payload_verify = {\"code\": otp_code, \"otpSession\": otp_session}\n\n    print(\"-- Sending Verify Request --\")\n    response_verify = requests.post(url_verify, json=payload_verify)\n    tokenObj = \"n/a\"\n\n    if response_verify.status_code == 201:\n        print(\"> Verification request successful!\")\n        print(\"Response:\", response_verify.json())\n        # Process the verification response if needed\n        response_json = response_verify.json()\n        if \"data\" in response_json and \"token\" in response_json[\"data\"]:\n            tokenObj = response_json[\"data\"][\"token\"]\n            print(\"tokenObj:\", tokenObj)\n        else:\n            print(\"No 'tokenObj' found in the response.\")\n            exit()\n    else:\n        print(\n            \"> Verification request failed with status code:\",\n            response_verify.status_code,\n        )\n        print(response_verify.json())\n        exit()\n\n    return tokenObj\n\n\n# Fetch user memories. Skip to this stage if we already acquired reusable token\ndef get_memories(tokenObj, start_date_range, end_date_range):\n    url_mem_feed = \"https://berealapi.fly.dev/friends/mem-feed\"\n    headers = {\"token\": tokenObj}\n\n    # Create a folder named 'primary' if it doesn't exist\n    folder_name = \"primary\"\n    if not os.path.exists(folder_name):\n        os.makedirs(folder_name)\n\n    # Create a folder named 'secondary' if it doesn't exist\n    secondary_folder_name = \"secondary\"\n    if not os.path.exists(secondary_folder_name):\n        os.makedirs(secondary_folder_name)\n\n    print(\"-- Sending Get Memories Request --\")\n    response_mem_feed = requests.get(url_mem_feed, headers=headers)\n    data_array = []\n\n    if response_mem_feed.status_code == 200:\n        print(\"> GET request successful!\")\n        # Process the response from mem-feed endpoint\n        print(\"Response:\", response_mem_feed.json())\n        print(\"we did it yay\")\n        response_data = response_mem_feed.json().get(\"data\", {})\n        data_array = response_data.get(\"data\", [])\n",
    "suffix": ""
  },
  {
    "name": "dakpinaroglu/Frame2seq:frame2seq/utils/score.py@1216",
    "canonical_solution": "        sequence=str_form,",
    "prompt": "import os\nimport torch\nfrom tqdm import tqdm\nfrom frame2seq.utils import residue_constants\nfrom frame2seq.utils.util import get_neg_pll, read_fasta_file\nfrom frame2seq.utils.pdb2input import get_inference_inputs\nfrom frame2seq.utils.pred2output import output_csv, output_indiv_csv\n\n\n\ndef score(self, pdb_file, chain_id, fasta_file, save_indiv_neg_pll):\n    temperature = 1.0\n    seq_mask, aatype, X = get_inference_inputs(pdb_file, chain_id)\n    seq_mask = seq_mask.to(self.device)\n    aatype = aatype.to(self.device)\n    X = X.to(self.device)\n    str_form = [residue_constants.ID_TO_AA[int(i)] for i in aatype[0]]\n    input_aatype_onehot = residue_constants.sequence_to_onehot(",
    "prefix": "import os\nimport torch\nfrom tqdm import tqdm\nfrom frame2seq.utils import residue_constants\nfrom frame2seq.utils.util import get_neg_pll, read_fasta_file\nfrom frame2seq.utils.pdb2input import get_inference_inputs\nfrom frame2seq.utils.pred2output import output_csv, output_indiv_csv\n\n\n\ndef score(self, pdb_file, chain_id, fasta_file, save_indiv_neg_pll):\n    temperature = 1.0\n    seq_mask, aatype, X = get_inference_inputs(pdb_file, chain_id)\n    seq_mask = seq_mask.to(self.device)\n    aatype = aatype.to(self.device)\n    X = X.to(self.device)\n    str_form = [residue_constants.ID_TO_AA[int(i)] for i in aatype[0]]\n    input_aatype_onehot = residue_constants.sequence_to_onehot(",
    "suffix": ""
  },
  {
    "name": "Maximilian-Winter/llama-cpp-agent:src/llama_cpp_agent/function_calling.py@1214",
    "canonical_solution": "        self.gbnf_grammar = gbnf_grammar",
    "prompt": "import json\nfrom typing import Type\nfrom llama_cpp import LlamaGrammar\nfrom pydantic import BaseModel\nfrom .output_parser import parse_json_response_with_markdown_code_block, parse_json_response\nfrom .gbnf_grammar_generator.gbnf_grammar_from_pydantic_models import format_model_and_field_name, \\\n    generate_gbnf_grammar_and_documentation\n\n\n\n\nclass LlamaCppFunctionTool:\n    def __init__(self, pydantic_model: Type[BaseModel], has_markdown_code_block=False, has_triple_quoted_string=False,\n                 **additional_parameters):\n        self.model = pydantic_model\n        self.look_for_field_string = has_markdown_code_block or has_triple_quoted_string\n        self.has_markdown_code_block = has_markdown_code_block\n        self.has_triple_quoted_string = has_triple_quoted_string\n        self.additional_parameters = additional_parameters if additional_parameters else {}\n\n    def __call__(self, *args, **kwargs):\n        return self.model(**kwargs)\n\n\nclass LlamaCppFunctionToolRegistry:\n    def __init__(self):\n        self.tool_root = \"function\"\n        self.tool_rule_content = \"function-parameters\"\n        self.model_prefix = \"Function\"\n        self.fields_prefix = \"Function Parameters\"\n        self.function_tools = {}\n        self.function_tools_containing_field_string = {}\n        self.grammar = None\n        self.grammar_documentation = None\n        self.gbnf_grammar = None\n\n    def register_function_tool(self, function_tool: LlamaCppFunctionTool):\n        function_name = format_model_and_field_name(function_tool.model.__name__)\n        if function_tool.look_for_field_string:\n            self.function_tools_containing_field_string[function_name] = function_tool\n        else:\n            self.function_tools[function_name] = function_tool\n\n    def get_function_tool(self, function_name: str):\n        if function_name in self.function_tools:\n            return self.function_tools[function_name]\n        elif function_name in self.function_tools_containing_field_string:\n            return self.function_tools_containing_field_string[function_name]\n        else:\n            return None\n\n    def finalize(self):\n        pydantic_function_models = []\n        look_markdown_code_block = False\n        for function_tool in self.function_tools.values():\n            pydantic_function_models.append(function_tool.model)\n            if function_tool.look_for_field_string:\n                look_markdown_code_block = True\n        for function_tool in self.function_tools_containing_field_string.values():\n            pydantic_function_models.append(function_tool.model)\n            if function_tool.look_for_field_string:\n                look_markdown_code_block = True\n        gbnf_grammar, documentation = generate_gbnf_grammar_and_documentation(\n            pydantic_function_models, look_markdown_code_block, look_markdown_code_block, self.tool_root,\n            self.tool_rule_content, self.model_prefix,\n            self.fields_prefix)\n\n        self.grammar = LlamaGrammar.from_string(gbnf_grammar, verbose=False)\n        self.grammar_documentation = documentation",
    "prefix": "import json\nfrom typing import Type\nfrom llama_cpp import LlamaGrammar\nfrom pydantic import BaseModel\nfrom .output_parser import parse_json_response_with_markdown_code_block, parse_json_response\nfrom .gbnf_grammar_generator.gbnf_grammar_from_pydantic_models import format_model_and_field_name, \\\n    generate_gbnf_grammar_and_documentation\n\n\n\n\nclass LlamaCppFunctionTool:\n    def __init__(self, pydantic_model: Type[BaseModel], has_markdown_code_block=False, has_triple_quoted_string=False,\n                 **additional_parameters):\n        self.model = pydantic_model\n        self.look_for_field_string = has_markdown_code_block or has_triple_quoted_string\n        self.has_markdown_code_block = has_markdown_code_block\n        self.has_triple_quoted_string = has_triple_quoted_string\n        self.additional_parameters = additional_parameters if additional_parameters else {}\n\n    def __call__(self, *args, **kwargs):\n        return self.model(**kwargs)\n\n\nclass LlamaCppFunctionToolRegistry:\n    def __init__(self):\n        self.tool_root = \"function\"\n        self.tool_rule_content = \"function-parameters\"\n        self.model_prefix = \"Function\"\n        self.fields_prefix = \"Function Parameters\"\n        self.function_tools = {}\n        self.function_tools_containing_field_string = {}\n        self.grammar = None\n        self.grammar_documentation = None\n        self.gbnf_grammar = None\n\n    def register_function_tool(self, function_tool: LlamaCppFunctionTool):\n        function_name = format_model_and_field_name(function_tool.model.__name__)\n        if function_tool.look_for_field_string:\n            self.function_tools_containing_field_string[function_name] = function_tool\n        else:\n            self.function_tools[function_name] = function_tool\n\n    def get_function_tool(self, function_name: str):\n        if function_name in self.function_tools:\n            return self.function_tools[function_name]\n        elif function_name in self.function_tools_containing_field_string:\n            return self.function_tools_containing_field_string[function_name]\n        else:\n            return None\n\n    def finalize(self):\n        pydantic_function_models = []\n        look_markdown_code_block = False\n        for function_tool in self.function_tools.values():\n            pydantic_function_models.append(function_tool.model)\n            if function_tool.look_for_field_string:\n                look_markdown_code_block = True\n        for function_tool in self.function_tools_containing_field_string.values():\n            pydantic_function_models.append(function_tool.model)\n            if function_tool.look_for_field_string:\n                look_markdown_code_block = True\n        gbnf_grammar, documentation = generate_gbnf_grammar_and_documentation(\n            pydantic_function_models, look_markdown_code_block, look_markdown_code_block, self.tool_root,\n            self.tool_rule_content, self.model_prefix,\n            self.fields_prefix)\n\n        self.grammar = LlamaGrammar.from_string(gbnf_grammar, verbose=False)\n        self.grammar_documentation = documentation",
    "suffix": ""
  },
  {
    "name": "usail-hkust/LLMTSCS:run_advanced_maxpressure.py@916",
    "canonical_solution": "        road_net = \"4_4\"",
    "prompt": "from utils.utils import oneline_wrapper\nfrom utils import error\nfrom multiprocessing import Process\nimport os\nimport time\nimport argparse\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--memo\", type=str, default='AdvancedMaxPressure')\n    parser.add_argument(\"--model\", type=str, default=\"AdvancedMaxPressure\")\n    parser.add_argument(\"--proj_name\", type=str, default=\"chatgpt-TSCS\")\n    parser.add_argument(\"--eightphase\", action=\"store_true\", default=False)\n    parser.add_argument(\"--multi_process\", action=\"store_true\", default=True)\n    parser.add_argument(\"--workers\", type=int, default=1)\n    parser.add_argument(\"--dataset\", type=str, default=\"template\")\n    parser.add_argument(\"--traffic_file\", type=str, default=\"flow_main_stream.json\")\n\n    return parser.parse_args()\n\n\ndef main(in_args):\n    traffic_file_list = []\n\n    if in_args.dataset == 'jinan':\n        count = 3600\n        road_net = \"3_4\"\n        traffic_file_list = [\"anon_3_4_jinan_real.json\", \"anon_3_4_jinan_real_2000.json\", \"anon_3_4_jinan_real_2500.json\"]\n        template = \"Jinan\"\n    elif in_args.dataset == 'hangzhou':\n        count = 3600",
    "prefix": "from utils.utils import oneline_wrapper\nfrom utils import error\nfrom multiprocessing import Process\nimport os\nimport time\nimport argparse\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--memo\", type=str, default='AdvancedMaxPressure')\n    parser.add_argument(\"--model\", type=str, default=\"AdvancedMaxPressure\")\n    parser.add_argument(\"--proj_name\", type=str, default=\"chatgpt-TSCS\")\n    parser.add_argument(\"--eightphase\", action=\"store_true\", default=False)\n    parser.add_argument(\"--multi_process\", action=\"store_true\", default=True)\n    parser.add_argument(\"--workers\", type=int, default=1)\n    parser.add_argument(\"--dataset\", type=str, default=\"template\")\n    parser.add_argument(\"--traffic_file\", type=str, default=\"flow_main_stream.json\")\n\n    return parser.parse_args()\n\n\ndef main(in_args):\n    traffic_file_list = []\n\n    if in_args.dataset == 'jinan':\n        count = 3600\n        road_net = \"3_4\"\n        traffic_file_list = [\"anon_3_4_jinan_real.json\", \"anon_3_4_jinan_real_2000.json\", \"anon_3_4_jinan_real_2500.json\"]\n        template = \"Jinan\"\n    elif in_args.dataset == 'hangzhou':\n        count = 3600",
    "suffix": ""
  },
  {
    "name": "ohadmata/shmessy:src/shmessy/types/boolean.py@682",
    "canonical_solution": "            return column.apply(",
    "prompt": "from typing import Optional, Tuple\nfrom numpy import ndarray\nfrom pandas import Series\nfrom ..schema import InferredField, ValidatorTypes\nfrom .base import BaseType\n\n\n\n\nclass BooleanType(BaseType):\n    weight = 1\n    validator_types = (ValidatorTypes.STRING, ValidatorTypes.NUMERIC)\n    patterns: list[Tuple] = [  # The first member should be the true value\n        (\"YES\", \"NO\"),\n        (\"TRUE\", \"FALSE\"),\n        (\"T\", \"F\"),\n        (\"Y\", \"N\"),\n        (1, 0),\n    ]\n\n    @staticmethod\n    def _validate_value_pattern(data: ndarray, pattern: Tuple) -> bool:\n        for value in data:\n            if isinstance(value, str):\n                value = value.lower()\n            if isinstance(pattern[0], str):\n                pattern = (pattern[0].lower(), pattern[1].lower())\n            if value not in (pattern[0], pattern[1]):\n                return False\n        return True\n\n    def validate(self, data: ndarray) -> Optional[InferredField]:\n        if not self.is_validator_type_valid(dtype=data.dtype):\n            return None\n\n        for pattern in self.patterns:\n            if self._validate_value_pattern(data, pattern):\n                return InferredField(inferred_type=self.name, inferred_pattern=pattern)\n\n    def fix(self, column: Series, inferred_field: InferredField) -> Series:\n        if isinstance(inferred_field.inferred_pattern[0], str):",
    "prefix": "from typing import Optional, Tuple\nfrom numpy import ndarray\nfrom pandas import Series\nfrom ..schema import InferredField, ValidatorTypes\nfrom .base import BaseType\n\n\n\n\nclass BooleanType(BaseType):\n    weight = 1\n    validator_types = (ValidatorTypes.STRING, ValidatorTypes.NUMERIC)\n    patterns: list[Tuple] = [  # The first member should be the true value\n        (\"YES\", \"NO\"),\n        (\"TRUE\", \"FALSE\"),\n        (\"T\", \"F\"),\n        (\"Y\", \"N\"),\n        (1, 0),\n    ]\n\n    @staticmethod\n    def _validate_value_pattern(data: ndarray, pattern: Tuple) -> bool:\n        for value in data:\n            if isinstance(value, str):\n                value = value.lower()\n            if isinstance(pattern[0], str):\n                pattern = (pattern[0].lower(), pattern[1].lower())\n            if value not in (pattern[0], pattern[1]):\n                return False\n        return True\n\n    def validate(self, data: ndarray) -> Optional[InferredField]:\n        if not self.is_validator_type_valid(dtype=data.dtype):\n            return None\n\n        for pattern in self.patterns:\n            if self._validate_value_pattern(data, pattern):\n                return InferredField(inferred_type=self.name, inferred_pattern=pattern)\n\n    def fix(self, column: Series, inferred_field: InferredField) -> Series:\n        if isinstance(inferred_field.inferred_pattern[0], str):",
    "suffix": ""
  },
  {
    "name": "kokiez/solana-sniper:alreadyBought.py@1466",
    "canonical_solution": "    data[desired_token_address] = settings\r",
    "prompt": "import os, sys\r\nimport json\r\nfrom webhook import sendWebhook\r\nfrom birdeye import getSymbol\r\n\r\ndef write_token_to_file(token_address):\r\n    file_path = os.path.join(sys.path[0], 'data', 'alreadyBoughtTokens.json')\r\n\r\n    # Load the JSON file\r\n    with open(file_path, 'r') as file:\r\n        data = json.load(file)\r\n\r\n    # Check if the 'tokens' key exists in the JSON object\r\n    if 'tokens' in data:\r\n        # If it exists, append the new tokens to the existing list\r\n        data['tokens'].extend([token_address])\r\n    else:\r\n        # If it doesn't exist, create a new list with the new tokens\r\n        data['tokens'] = [token_address]\r\n\r\n    # Write the updated data back to the file\r\n    with open(file_path, 'w') as file:\r\n        json.dump(data, file, indent=4)\r\n\r\n    print(f\"Token address [{token_address}] saved in 'alreadyBoughtTokens.json'.\")\r\n\r\n# def check_if_already_bought(token_address):\r\ndef check_token_existence(token_to_check):\r\n    token_symbol, SOl_Symbol = getSymbol(token_to_check)\r\n\r\n    file_path = os.path.join(sys.path[0], 'data', 'alreadyBoughtTokens.json')\r\n\r\n    # Open the JSON file in read mode ('r')\r\n    with open(file_path, 'r') as file:\r\n        # Load the JSON data into a dictionary\r\n        token_data = json.load(file)\r\n\r\n    # Check if the token exists in the JSON data\r\n    if token_to_check in token_data['tokens']:\r\n        print(f\"[{token_to_check}] already exists in 'alreadyBoughtTokens.json'.\")\r\n        sendWebhook(f\"a|Token SAVE {token_symbol}\", f\"[{token_to_check}] already exists in 'alreadyBoughtTokens.json'.\")\r\n        return True\r\n    return False\r\n\r\n\r\ndef storeSettings(amm,\r\n                  desired_token_address,\r\n                  txB,\r\n                  execution_time,\r\n                  limit_order_sell_Bool,\r\n                  take_profit_ratio,\r\n                  trailing_stop_Bool,\r\n                  trailing_stop_ratio,\r\n                  Limit_and_Trailing_Stop_Bool,\r\n                  bought_token_price):\r\n    \r\n    token_symbol, SOl_Symbol = getSymbol(desired_token_address)\r\n\r\n    file_path = os.path.join(sys.path[0], 'data', 'previousSELLBUYINFO.json')\r\n\r\n    # Define the settings\r\n    settings = {\r\n         'amm': amm,\r\n            'txB': str(txB),\r\n            'execution_time': execution_time,\r\n            'limit_order_sell_Bool': limit_order_sell_Bool,\r\n            'take_profit_ratio': take_profit_ratio,\r\n            'trailing_stop_Bool': trailing_stop_Bool,\r\n            'trailing_stop_ratio': trailing_stop_ratio,\r\n            'Limit_and_Trailing_Stop_Bool': Limit_and_Trailing_Stop_Bool,\r\n            'bought_token_price': bought_token_price\r\n    }\r\n\r\n    # Load the JSON file\r\n    with open(file_path, 'r') as file:\r\n        data = json.load(file)\r\n\r\n    # Append the settings to the JSON object\r",
    "prefix": "import os, sys\r\nimport json\r\nfrom webhook import sendWebhook\r\nfrom birdeye import getSymbol\r\n\r\ndef write_token_to_file(token_address):\r\n    file_path = os.path.join(sys.path[0], 'data', 'alreadyBoughtTokens.json')\r\n\r\n    # Load the JSON file\r\n    with open(file_path, 'r') as file:\r\n        data = json.load(file)\r\n\r\n    # Check if the 'tokens' key exists in the JSON object\r\n    if 'tokens' in data:\r\n        # If it exists, append the new tokens to the existing list\r\n        data['tokens'].extend([token_address])\r\n    else:\r\n        # If it doesn't exist, create a new list with the new tokens\r\n        data['tokens'] = [token_address]\r\n\r\n    # Write the updated data back to the file\r\n    with open(file_path, 'w') as file:\r\n        json.dump(data, file, indent=4)\r\n\r\n    print(f\"Token address [{token_address}] saved in 'alreadyBoughtTokens.json'.\")\r\n\r\n# def check_if_already_bought(token_address):\r\ndef check_token_existence(token_to_check):\r\n    token_symbol, SOl_Symbol = getSymbol(token_to_check)\r\n\r\n    file_path = os.path.join(sys.path[0], 'data', 'alreadyBoughtTokens.json')\r\n\r\n    # Open the JSON file in read mode ('r')\r\n    with open(file_path, 'r') as file:\r\n        # Load the JSON data into a dictionary\r\n        token_data = json.load(file)\r\n\r\n    # Check if the token exists in the JSON data\r\n    if token_to_check in token_data['tokens']:\r\n        print(f\"[{token_to_check}] already exists in 'alreadyBoughtTokens.json'.\")\r\n        sendWebhook(f\"a|Token SAVE {token_symbol}\", f\"[{token_to_check}] already exists in 'alreadyBoughtTokens.json'.\")\r\n        return True\r\n    return False\r\n\r\n\r\ndef storeSettings(amm,\r\n                  desired_token_address,\r\n                  txB,\r\n                  execution_time,\r\n                  limit_order_sell_Bool,\r\n                  take_profit_ratio,\r\n                  trailing_stop_Bool,\r\n                  trailing_stop_ratio,\r\n                  Limit_and_Trailing_Stop_Bool,\r\n                  bought_token_price):\r\n    \r\n    token_symbol, SOl_Symbol = getSymbol(desired_token_address)\r\n\r\n    file_path = os.path.join(sys.path[0], 'data', 'previousSELLBUYINFO.json')\r\n\r\n    # Define the settings\r\n    settings = {\r\n         'amm': amm,\r\n            'txB': str(txB),\r\n            'execution_time': execution_time,\r\n            'limit_order_sell_Bool': limit_order_sell_Bool,\r\n            'take_profit_ratio': take_profit_ratio,\r\n            'trailing_stop_Bool': trailing_stop_Bool,\r\n            'trailing_stop_ratio': trailing_stop_ratio,\r\n            'Limit_and_Trailing_Stop_Bool': Limit_and_Trailing_Stop_Bool,\r\n            'bought_token_price': bought_token_price\r\n    }\r\n\r\n    # Load the JSON file\r\n    with open(file_path, 'r') as file:\r\n        data = json.load(file)\r\n\r\n    # Append the settings to the JSON object\r",
    "suffix": ""
  },
  {
    "name": "zy7y/dfs-generate:main.py@944",
    "canonical_solution": "    request: Request, table_name: str = Query(..., alias=\"tableName\")",
    "prompt": "from fastapi import FastAPI, Query\nfrom fastapi.requests import Request\nfrom fastapi.responses import FileResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom entity import CodeGen, Conf, DBConf, R, RList, Table\nfrom generate.main import generate_code\n    import uvicorn\n\n\napp = FastAPI(\n    title=\"dfs-generate\", description=\"FastAPI SQLModel \u9006\u5411\u751f\u6210\u4ee3\u7801\", docs_url=None\n)\n\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n\n@app.get(\"/\", include_in_schema=False)\ndef index():\n    return FileResponse(\"static/index.html\")\n\n\n@app.get(\"/tables\", response_model=RList[Table])\ndef query_by_table_name_limit(\n    request: Request, table_name: str = Query(\"\", alias=\"tableName\")\n):\n    total = []\n    try:\n        uri, metadata = Conf.get_last_uri_with_metadata()\n        request.app.state.uri = uri\n        request.app.state.metadata = metadata\n        for _, table in metadata.tables.items():\n            if table_name in table.name:\n                total.append(dict(table_name=table.name, table_comment=table.comment))\n        return RList.success(data=total)\n\n    except Exception as e:\n        print(e)\n    return RList.error(msg=\"\u8bf7\u5148\u53bb\u914d\u7f6e\u6570\u636e\u5e93\")\n\n\n@app.get(\"/codegen\", response_model=RList[CodeGen])\ndef get_codegen_by_table_name(",
    "prefix": "from fastapi import FastAPI, Query\nfrom fastapi.requests import Request\nfrom fastapi.responses import FileResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom entity import CodeGen, Conf, DBConf, R, RList, Table\nfrom generate.main import generate_code\n    import uvicorn\n\n\napp = FastAPI(\n    title=\"dfs-generate\", description=\"FastAPI SQLModel \u9006\u5411\u751f\u6210\u4ee3\u7801\", docs_url=None\n)\n\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n\n@app.get(\"/\", include_in_schema=False)\ndef index():\n    return FileResponse(\"static/index.html\")\n\n\n@app.get(\"/tables\", response_model=RList[Table])\ndef query_by_table_name_limit(\n    request: Request, table_name: str = Query(\"\", alias=\"tableName\")\n):\n    total = []\n    try:\n        uri, metadata = Conf.get_last_uri_with_metadata()\n        request.app.state.uri = uri\n        request.app.state.metadata = metadata\n        for _, table in metadata.tables.items():\n            if table_name in table.name:\n                total.append(dict(table_name=table.name, table_comment=table.comment))\n        return RList.success(data=total)\n\n    except Exception as e:\n        print(e)\n    return RList.error(msg=\"\u8bf7\u5148\u53bb\u914d\u7f6e\u6570\u636e\u5e93\")\n\n\n@app.get(\"/codegen\", response_model=RList[CodeGen])\ndef get_codegen_by_table_name(",
    "suffix": ""
  },
  {
    "name": "CrawlScript/Torch-MGDCF:torch_mgdcf/datasets.py@646",
    "canonical_solution": "    return num_users, num_items, user_item_edges, train_index, test_index, train_user_items_dict, test_user_items_dict",
    "prompt": "from torch_mgdcf.utils import download_file, extract_zip\nimport os\nimport pickle\nimport numpy as np\nimport torch\n\n\ndef _read_edge_info(file_path):\n    edge_dict = {}\n    edges = []\n\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for l in f.readlines():\n            if len(l) > 0:\n                try:\n                    l = l.strip('\\n').split(' ')\n                    items = []\n                    uid = int(l[0])\n                    for i in l[1:]:\n                        i = int(i)\n                        items.append(i)\n                        edges.append([uid, i])\n                    if uid not in edge_dict:\n                        edge_dict[uid] = set(items)\n                    else:\n                        item_set = edge_dict[uid]\n                        edge_dict[uid] = set(items).union(item_set)\n                except Exception:\n                    continue\n\n    edges = np.array(edges)\n    return edge_dict, edges\n\ndef _process(dataset_unzip_path):\n\n    train_file = os.path.join(dataset_unzip_path, 'train.txt')\n    test_file = os.path.join(dataset_unzip_path, 'test.txt')\n\n    # print(train_file)\n    # asdfasdf\n\n    train_user_items_dict, train_user_item_edges = _read_edge_info(train_file)\n    test_user_items_dict, test_user_item_edges = _read_edge_info(test_file)\n\n    user_item_edges = np.concatenate([train_user_item_edges, test_user_item_edges], axis=0)\n    index = np.arange(user_item_edges.shape[0])\n    num_train_edges = train_user_item_edges.shape[0]\n    train_index, test_index = index[:num_train_edges], index[num_train_edges:]\n    num_users, num_items = user_item_edges.max(axis=0) + 1\n\n",
    "prefix": "from torch_mgdcf.utils import download_file, extract_zip\nimport os\nimport pickle\nimport numpy as np\nimport torch\n\n\ndef _read_edge_info(file_path):\n    edge_dict = {}\n    edges = []\n\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for l in f.readlines():\n            if len(l) > 0:\n                try:\n                    l = l.strip('\\n').split(' ')\n                    items = []\n                    uid = int(l[0])\n                    for i in l[1:]:\n                        i = int(i)\n                        items.append(i)\n                        edges.append([uid, i])\n                    if uid not in edge_dict:\n                        edge_dict[uid] = set(items)\n                    else:\n                        item_set = edge_dict[uid]\n                        edge_dict[uid] = set(items).union(item_set)\n                except Exception:\n                    continue\n\n    edges = np.array(edges)\n    return edge_dict, edges\n\ndef _process(dataset_unzip_path):\n\n    train_file = os.path.join(dataset_unzip_path, 'train.txt')\n    test_file = os.path.join(dataset_unzip_path, 'test.txt')\n\n    # print(train_file)\n    # asdfasdf\n\n    train_user_items_dict, train_user_item_edges = _read_edge_info(train_file)\n    test_user_items_dict, test_user_item_edges = _read_edge_info(test_file)\n\n    user_item_edges = np.concatenate([train_user_item_edges, test_user_item_edges], axis=0)\n    index = np.arange(user_item_edges.shape[0])\n    num_train_edges = train_user_item_edges.shape[0]\n    train_index, test_index = index[:num_train_edges], index[num_train_edges:]\n    num_users, num_items = user_item_edges.max(axis=0) + 1\n\n",
    "suffix": ""
  },
  {
    "name": "KyanChen/TTP:mmdet/models/losses/cross_entropy_loss.py@1059",
    "canonical_solution": "                  class_weight=None,",
    "prompt": "import warnings\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmdet.registry import MODELS\nfrom .accuracy import accuracy\nfrom .utils import weight_reduce_loss\n# Copyright (c) OpenMMLab. All rights reserved.\n\n\n\n\ndef cross_entropy(pred,\n                  label,\n                  weight=None,\n                  reduction='mean',\n                  avg_factor=None,",
    "prefix": "import warnings\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmdet.registry import MODELS\nfrom .accuracy import accuracy\nfrom .utils import weight_reduce_loss\n# Copyright (c) OpenMMLab. All rights reserved.\n\n\n\n\ndef cross_entropy(pred,\n                  label,\n                  weight=None,\n                  reduction='mean',\n                  avg_factor=None,",
    "suffix": ""
  },
  {
    "name": "dan-r/HomeAssistant-Ohme:custom_components/ohme/switch.py@1097",
    "canonical_solution": "            self._attr_is_on = bool(self.coordinator.data[\"mode\"] == \"STOPPED\")",
    "prompt": "import logging\nimport asyncio\nfrom homeassistant.core import callback, HomeAssistant\nfrom homeassistant.helpers.entity import generate_entity_id\nfrom homeassistant.helpers.update_coordinator import (\n    CoordinatorEntity\n)\nfrom homeassistant.components.switch import SwitchEntity\nfrom homeassistant.util.dt import (utcnow)\nfrom .const import DOMAIN, DATA_CLIENT, DATA_COORDINATORS, COORDINATOR_CHARGESESSIONS, COORDINATOR_ACCOUNTINFO\nfrom .coordinator import OhmeChargeSessionsCoordinator, OhmeAccountInfoCoordinator\nfrom __future__ import annotations\n\n\n\n\n_LOGGER = logging.getLogger(__name__)\n\n\nasync def async_setup_entry(\n    hass: HomeAssistant,\n    config_entry: config_entries.ConfigEntry,\n    async_add_entities\n):\n    \"\"\"Setup switches and configure coordinator.\"\"\"\n    coordinators = hass.data[DOMAIN][DATA_COORDINATORS]\n\n    coordinator = coordinators[COORDINATOR_CHARGESESSIONS]\n    accountinfo_coordinator = coordinators[COORDINATOR_ACCOUNTINFO]\n    client = hass.data[DOMAIN][DATA_CLIENT]\n\n    switches = [OhmePauseChargeSwitch(coordinator, hass, client),\n                OhmeMaxChargeSwitch(coordinator, hass, client)]\n\n    if client.is_capable(\"buttonsLockable\"):\n        switches.append(\n            OhmeConfigurationSwitch(\n                accountinfo_coordinator, hass, client, \"Lock Buttons\", \"lock\", \"buttonsLocked\")\n        )\n    if client.is_capable(\"pluginsRequireApprovalMode\"):\n        switches.append(\n            OhmeConfigurationSwitch(accountinfo_coordinator, hass, client,\n                                    \"Require Approval\", \"check-decagram\", \"pluginsRequireApproval\")\n        )\n    if client.is_capable(\"stealth\"):\n        switches.append(\n            OhmeConfigurationSwitch(accountinfo_coordinator, hass, client,\n                                    \"Sleep When Inactive\", \"power-sleep\", \"stealthEnabled\")\n        )\n\n    async_add_entities(switches, update_before_add=True)\n\n\nclass OhmePauseChargeSwitch(CoordinatorEntity[OhmeChargeSessionsCoordinator], SwitchEntity):\n    \"\"\"Switch for pausing a charge.\"\"\"\n    _attr_name = \"Pause Charge\"\n\n    def __init__(self, coordinator, hass: HomeAssistant, client):\n        super().__init__(coordinator=coordinator)\n\n        self._client = client\n\n        self._state = False\n        self._last_updated = None\n        self._attributes = {}\n\n        self.entity_id = generate_entity_id(\n            \"switch.{}\", \"ohme_pause_charge\", hass=hass)\n\n        self._attr_device_info = client.get_device_info()\n\n    @property\n    def unique_id(self):\n        \"\"\"The unique ID of the switch.\"\"\"\n        return self._client.get_unique_id(\"pause_charge\")\n\n    @property\n    def icon(self):\n        \"\"\"Icon of the switch.\"\"\"\n        return \"mdi:pause\"\n\n    @callback\n    def _handle_coordinator_update(self) -> None:\n        \"\"\"Determine if charge is paused.\n           We handle this differently to the sensors as the state of this switch\n           is evaluated only when new data is fetched to stop the switch flicking back then forth.\"\"\"\n        if self.coordinator.data is None:\n            self._attr_is_on = False\n        else:",
    "prefix": "import logging\nimport asyncio\nfrom homeassistant.core import callback, HomeAssistant\nfrom homeassistant.helpers.entity import generate_entity_id\nfrom homeassistant.helpers.update_coordinator import (\n    CoordinatorEntity\n)\nfrom homeassistant.components.switch import SwitchEntity\nfrom homeassistant.util.dt import (utcnow)\nfrom .const import DOMAIN, DATA_CLIENT, DATA_COORDINATORS, COORDINATOR_CHARGESESSIONS, COORDINATOR_ACCOUNTINFO\nfrom .coordinator import OhmeChargeSessionsCoordinator, OhmeAccountInfoCoordinator\nfrom __future__ import annotations\n\n\n\n\n_LOGGER = logging.getLogger(__name__)\n\n\nasync def async_setup_entry(\n    hass: HomeAssistant,\n    config_entry: config_entries.ConfigEntry,\n    async_add_entities\n):\n    \"\"\"Setup switches and configure coordinator.\"\"\"\n    coordinators = hass.data[DOMAIN][DATA_COORDINATORS]\n\n    coordinator = coordinators[COORDINATOR_CHARGESESSIONS]\n    accountinfo_coordinator = coordinators[COORDINATOR_ACCOUNTINFO]\n    client = hass.data[DOMAIN][DATA_CLIENT]\n\n    switches = [OhmePauseChargeSwitch(coordinator, hass, client),\n                OhmeMaxChargeSwitch(coordinator, hass, client)]\n\n    if client.is_capable(\"buttonsLockable\"):\n        switches.append(\n            OhmeConfigurationSwitch(\n                accountinfo_coordinator, hass, client, \"Lock Buttons\", \"lock\", \"buttonsLocked\")\n        )\n    if client.is_capable(\"pluginsRequireApprovalMode\"):\n        switches.append(\n            OhmeConfigurationSwitch(accountinfo_coordinator, hass, client,\n                                    \"Require Approval\", \"check-decagram\", \"pluginsRequireApproval\")\n        )\n    if client.is_capable(\"stealth\"):\n        switches.append(\n            OhmeConfigurationSwitch(accountinfo_coordinator, hass, client,\n                                    \"Sleep When Inactive\", \"power-sleep\", \"stealthEnabled\")\n        )\n\n    async_add_entities(switches, update_before_add=True)\n\n\nclass OhmePauseChargeSwitch(CoordinatorEntity[OhmeChargeSessionsCoordinator], SwitchEntity):\n    \"\"\"Switch for pausing a charge.\"\"\"\n    _attr_name = \"Pause Charge\"\n\n    def __init__(self, coordinator, hass: HomeAssistant, client):\n        super().__init__(coordinator=coordinator)\n\n        self._client = client\n\n        self._state = False\n        self._last_updated = None\n        self._attributes = {}\n\n        self.entity_id = generate_entity_id(\n            \"switch.{}\", \"ohme_pause_charge\", hass=hass)\n\n        self._attr_device_info = client.get_device_info()\n\n    @property\n    def unique_id(self):\n        \"\"\"The unique ID of the switch.\"\"\"\n        return self._client.get_unique_id(\"pause_charge\")\n\n    @property\n    def icon(self):\n        \"\"\"Icon of the switch.\"\"\"\n        return \"mdi:pause\"\n\n    @callback\n    def _handle_coordinator_update(self) -> None:\n        \"\"\"Determine if charge is paused.\n           We handle this differently to the sensors as the state of this switch\n           is evaluated only when new data is fetched to stop the switch flicking back then forth.\"\"\"\n        if self.coordinator.data is None:\n            self._attr_is_on = False\n        else:",
    "suffix": ""
  },
  {
    "name": "Almas-Ali/SpyIP:tests/test_backend.py@1427",
    "canonical_solution": "    def test_trace_ip_batch(self):",
    "prompt": "import unittest\nfrom spyip import trace_me, trace_dns, trace_ip, trace_ip_batch\n\n\n\nclass TestSpyIP(unittest.TestCase):\n    def test_trace_me(self):\n        self.assertEqual(trace_me().status, 'success')\n\n    def test_trace_dns(self):\n        self.assertNotEqual(trace_dns().ip, '')\n        self.assertNotEqual(trace_dns().geo, '')\n\n    def test_trace_ip(self):\n        self.assertEqual(trace_ip(query='31.13.64.35').status, 'success')\n",
    "prefix": "import unittest\nfrom spyip import trace_me, trace_dns, trace_ip, trace_ip_batch\n\n\n\nclass TestSpyIP(unittest.TestCase):\n    def test_trace_me(self):\n        self.assertEqual(trace_me().status, 'success')\n\n    def test_trace_dns(self):\n        self.assertNotEqual(trace_dns().ip, '')\n        self.assertNotEqual(trace_dns().geo, '')\n\n    def test_trace_ip(self):\n        self.assertEqual(trace_ip(query='31.13.64.35').status, 'success')\n",
    "suffix": ""
  },
  {
    "name": "leopedroso45/Stable-Diffusion-ImageGen:tests/test_process_task.py@1540",
    "canonical_solution": "        fake_executor = {\"num_of_exec\": 1, \"cfg_scale\": 7}",
    "prompt": "import unittest\nimport sys\nfrom unittest.mock import patch, MagicMock\nfrom sevsd.process_task import check_cuda_and_clear_cache, process_task, check_os_path\nsys.path.append('../')\n\nclass TestProcessTask(unittest.TestCase):\n\n    @patch('sevsd.process_task.generate_image')\n    def test_process_task(self, mock_generate_image):\n        mock_image = MagicMock()\n        mock_image.save = MagicMock()\n        mock_generate_image.return_value = [mock_image]\n\n        fake_job = {\"prompt\": \"prompt\", \"details\": (None, 50, 1, 7.5)}\n        fake_pipeline = MagicMock()\n        fake_executor = {\"num_of_exec\": 1, \"cfg_scale\": 7}\n        fake_path = \"test_path\"\n\n        process_task(fake_job, fake_pipeline, fake_executor, fake_path, parallel_exec=True)\n\n        mock_generate_image.assert_called_once_with(fake_job, fake_pipeline, fake_executor, True)\n        mock_image.save.assert_called()\n    \n    @patch('sevsd.process_task.generate_image')\n    @patch('sevsd.process_task.check_cuda_and_clear_cache')\n    @patch('sevsd.process_task.check_os_path')\n    @patch('sevsd.process_task.datetime')\n    @patch('os.path.exists')\n    @patch('os.makedirs')\n    def test_process_task_multiple_images(self, mock_makedirs, mock_exists, mock_datetime, \n                                          mock_check_os_path, mock_check_cuda_and_clear_cache, \n                                          mock_generate_image):\n        mock_generate_image.return_value = [MagicMock()]\n        mock_check_os_path.return_value = \"test_path\"\n        mock_exists.return_value = True\n        mock_datetime.now.return_value.strftime.return_value = \"20240101_123456789\"\n\n        fake_job = {\"prompt\": \"prompt\", \"negative_prompt\": None}\n        fake_pipeline = MagicMock()\n        fake_executor = {\"num_of_exec\": 3, \"cfg_scale\": 7.5}  # Teste com 3 execu\u00e7\u00f5es\n\n        process_task(fake_job, fake_pipeline, fake_executor, \"test_path\", parallel_exec=False)\n\n        self.assertEqual(mock_generate_image.call_count, 3)\n        self.assertEqual(mock_check_cuda_and_clear_cache.call_count, 4)  # 3 + 1 no finally\n\n    @patch('sevsd.process_task.generate_image')\n    def test_process_task_no_images(self, mock_generate_image):\n        mock_generate_image.return_value = None\n\n        fake_job = {\"prompt\": \"prompt\", \"details\": (None, 50, 1, 7.5)}\n        fake_pipeline = MagicMock()\n        fake_executor = {\"num_of_exec\": 1, \"cfg_scale\": 7}\n        fake_path = \"test_path\"\n\n        process_task(fake_job, fake_pipeline, fake_executor, fake_path, parallel_exec=True)\n\n        mock_generate_image.assert_called_once_with(fake_job, fake_pipeline, fake_executor, True)\n\n    @patch('sevsd.process_task.generate_image')\n    @patch('sevsd.process_task.check_cuda_and_clear_cache')\n    @patch('sevsd.process_task.print')\n    def test_process_task_exception(self, mock_print, mock_check_cuda_and_clear_cache, mock_generate_image):\n        mock_generate_image.side_effect = Exception(\"Test exception\")\n\n        fake_job = {\"prompt\": \"prompt\", \"details\": (None, 50, 1, 7.5)}\n        fake_pipeline = MagicMock()",
    "prefix": "import unittest\nimport sys\nfrom unittest.mock import patch, MagicMock\nfrom sevsd.process_task import check_cuda_and_clear_cache, process_task, check_os_path\nsys.path.append('../')\n\nclass TestProcessTask(unittest.TestCase):\n\n    @patch('sevsd.process_task.generate_image')\n    def test_process_task(self, mock_generate_image):\n        mock_image = MagicMock()\n        mock_image.save = MagicMock()\n        mock_generate_image.return_value = [mock_image]\n\n        fake_job = {\"prompt\": \"prompt\", \"details\": (None, 50, 1, 7.5)}\n        fake_pipeline = MagicMock()\n        fake_executor = {\"num_of_exec\": 1, \"cfg_scale\": 7}\n        fake_path = \"test_path\"\n\n        process_task(fake_job, fake_pipeline, fake_executor, fake_path, parallel_exec=True)\n\n        mock_generate_image.assert_called_once_with(fake_job, fake_pipeline, fake_executor, True)\n        mock_image.save.assert_called()\n    \n    @patch('sevsd.process_task.generate_image')\n    @patch('sevsd.process_task.check_cuda_and_clear_cache')\n    @patch('sevsd.process_task.check_os_path')\n    @patch('sevsd.process_task.datetime')\n    @patch('os.path.exists')\n    @patch('os.makedirs')\n    def test_process_task_multiple_images(self, mock_makedirs, mock_exists, mock_datetime, \n                                          mock_check_os_path, mock_check_cuda_and_clear_cache, \n                                          mock_generate_image):\n        mock_generate_image.return_value = [MagicMock()]\n        mock_check_os_path.return_value = \"test_path\"\n        mock_exists.return_value = True\n        mock_datetime.now.return_value.strftime.return_value = \"20240101_123456789\"\n\n        fake_job = {\"prompt\": \"prompt\", \"negative_prompt\": None}\n        fake_pipeline = MagicMock()\n        fake_executor = {\"num_of_exec\": 3, \"cfg_scale\": 7.5}  # Teste com 3 execu\u00e7\u00f5es\n\n        process_task(fake_job, fake_pipeline, fake_executor, \"test_path\", parallel_exec=False)\n\n        self.assertEqual(mock_generate_image.call_count, 3)\n        self.assertEqual(mock_check_cuda_and_clear_cache.call_count, 4)  # 3 + 1 no finally\n\n    @patch('sevsd.process_task.generate_image')\n    def test_process_task_no_images(self, mock_generate_image):\n        mock_generate_image.return_value = None\n\n        fake_job = {\"prompt\": \"prompt\", \"details\": (None, 50, 1, 7.5)}\n        fake_pipeline = MagicMock()\n        fake_executor = {\"num_of_exec\": 1, \"cfg_scale\": 7}\n        fake_path = \"test_path\"\n\n        process_task(fake_job, fake_pipeline, fake_executor, fake_path, parallel_exec=True)\n\n        mock_generate_image.assert_called_once_with(fake_job, fake_pipeline, fake_executor, True)\n\n    @patch('sevsd.process_task.generate_image')\n    @patch('sevsd.process_task.check_cuda_and_clear_cache')\n    @patch('sevsd.process_task.print')\n    def test_process_task_exception(self, mock_print, mock_check_cuda_and_clear_cache, mock_generate_image):\n        mock_generate_image.side_effect = Exception(\"Test exception\")\n\n        fake_job = {\"prompt\": \"prompt\", \"details\": (None, 50, 1, 7.5)}\n        fake_pipeline = MagicMock()",
    "suffix": ""
  },
  {
    "name": "Hassi34/iot-device-identification:src/stage_03_preprocess_data.py@1092",
    "canonical_solution": "        y,",
    "prompt": "import argparse\nimport joblib\nimport pandas as pd\nfrom src.utils.common import read_yaml\nfrom src.utils.sys_logging import get_logger\nfrom sklearn.preprocessing import LabelEncoder\nfrom src.utils.common import write_dict_to_yaml\nfrom src.utils.data_ops import gzip_np_arr\nfrom sklearn.model_selection import train_test_split\nfrom src.utils.data_ops import get_fitted_pipeline\nfrom pathlib import Path\n\nSTAGE = \"Preprocess Data\"\n\n\ndef preprocess_data():\n    complete_df = pd.read_parquet(RAW_DATA_FILE_PATH)\n    logger.info(\n        f'The raw data file has been loaded from \"{RAW_DATA_FILE_PATH}\" with the shape \"{complete_df.shape}\"'\n    )\n    duplicate_rows = complete_df.duplicated().sum()\n    if duplicate_rows > 0:\n        logger.warning(\n            f\"Found {duplicate_rows} duplicate rows, removing duplicate rows...\"\n        )\n        complete_df = complete_df.drop_duplicates(keep=\"first\")\n    X = complete_df.drop([TARGET_COLUMN_NAME], axis=1)\n    y = complete_df[TARGET_COLUMN_NAME]\n    feature_cols = params[\"input_features_schema\"]\n    feature_cols = list(feature_cols.keys())\n    logger.info(f\"Read {len(feature_cols)} feature columns from params\")\n    data_processing_pipeline = get_fitted_pipeline(\n    X, feature_cols, KNN_IMPUTER_NEIGHBORS=KNN_IMPUTER_NEIGHBORS\n    )\n    Path(DATA_PREPROCESSING_PIPELINE_FILE_PATH).parent.absolute().mkdir(parents=True, exist_ok=True)\n    joblib.dump(data_processing_pipeline, DATA_PREPROCESSING_PIPELINE_FILE_PATH, compress=1)\n    logger.info(f\"Saved the preprocessing pipeline to {DATA_PREPROCESSING_PIPELINE_FILE_PATH}\")\n    data_processing_pipeline = joblib.load(DATA_PREPROCESSING_PIPELINE_FILE_PATH)\n    data_processing_pipeline\n    data_processing_pipeline = joblib.load(DATA_PREPROCESSING_PIPELINE_FILE_PATH)\n    logger.info(\n        f'Loaded sklearn data preprocessing pipeline from \"{DATA_PREPROCESSING_PIPELINE_FILE_PATH}\"'\n    )\n    X_transformed = data_processing_pipeline.transform(X)\n    logger.info(f'Dataframe shape after transformation is \"{X_transformed.shape}\"')\n\n    le = LabelEncoder()\n    le.fit(y)\n    labels_mapping_dict = {\"labels_mapping\": \"\"}\n    le_dict = dict(zip(le.transform(le.classes_), le.classes_))\n    le_dict = {int(k): v for k, v in le_dict.items()}\n\n    labels_mapping_dict[\"labels_mapping\"] = le_dict\n    logger.info(f\"Label encoding map has the dictionary: {le_dict}\")\n    write_dict_to_yaml(labels_mapping_dict, parsed_args.params)\n    logger.info(f'Updated the label encoding map in the file at \"{parsed_args.params}\"')\n    labels_dict = read_yaml(parsed_args.params)[\"labels_mapping\"]\n    reverse_dict = {v: k for k, v in labels_dict.items()}\n    y = y.map(reverse_dict)\n    logger.info(\"Successfully mapped the target column\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_transformed,",
    "prefix": "import argparse\nimport joblib\nimport pandas as pd\nfrom src.utils.common import read_yaml\nfrom src.utils.sys_logging import get_logger\nfrom sklearn.preprocessing import LabelEncoder\nfrom src.utils.common import write_dict_to_yaml\nfrom src.utils.data_ops import gzip_np_arr\nfrom sklearn.model_selection import train_test_split\nfrom src.utils.data_ops import get_fitted_pipeline\nfrom pathlib import Path\n\nSTAGE = \"Preprocess Data\"\n\n\ndef preprocess_data():\n    complete_df = pd.read_parquet(RAW_DATA_FILE_PATH)\n    logger.info(\n        f'The raw data file has been loaded from \"{RAW_DATA_FILE_PATH}\" with the shape \"{complete_df.shape}\"'\n    )\n    duplicate_rows = complete_df.duplicated().sum()\n    if duplicate_rows > 0:\n        logger.warning(\n            f\"Found {duplicate_rows} duplicate rows, removing duplicate rows...\"\n        )\n        complete_df = complete_df.drop_duplicates(keep=\"first\")\n    X = complete_df.drop([TARGET_COLUMN_NAME], axis=1)\n    y = complete_df[TARGET_COLUMN_NAME]\n    feature_cols = params[\"input_features_schema\"]\n    feature_cols = list(feature_cols.keys())\n    logger.info(f\"Read {len(feature_cols)} feature columns from params\")\n    data_processing_pipeline = get_fitted_pipeline(\n    X, feature_cols, KNN_IMPUTER_NEIGHBORS=KNN_IMPUTER_NEIGHBORS\n    )\n    Path(DATA_PREPROCESSING_PIPELINE_FILE_PATH).parent.absolute().mkdir(parents=True, exist_ok=True)\n    joblib.dump(data_processing_pipeline, DATA_PREPROCESSING_PIPELINE_FILE_PATH, compress=1)\n    logger.info(f\"Saved the preprocessing pipeline to {DATA_PREPROCESSING_PIPELINE_FILE_PATH}\")\n    data_processing_pipeline = joblib.load(DATA_PREPROCESSING_PIPELINE_FILE_PATH)\n    data_processing_pipeline\n    data_processing_pipeline = joblib.load(DATA_PREPROCESSING_PIPELINE_FILE_PATH)\n    logger.info(\n        f'Loaded sklearn data preprocessing pipeline from \"{DATA_PREPROCESSING_PIPELINE_FILE_PATH}\"'\n    )\n    X_transformed = data_processing_pipeline.transform(X)\n    logger.info(f'Dataframe shape after transformation is \"{X_transformed.shape}\"')\n\n    le = LabelEncoder()\n    le.fit(y)\n    labels_mapping_dict = {\"labels_mapping\": \"\"}\n    le_dict = dict(zip(le.transform(le.classes_), le.classes_))\n    le_dict = {int(k): v for k, v in le_dict.items()}\n\n    labels_mapping_dict[\"labels_mapping\"] = le_dict\n    logger.info(f\"Label encoding map has the dictionary: {le_dict}\")\n    write_dict_to_yaml(labels_mapping_dict, parsed_args.params)\n    logger.info(f'Updated the label encoding map in the file at \"{parsed_args.params}\"')\n    labels_dict = read_yaml(parsed_args.params)[\"labels_mapping\"]\n    reverse_dict = {v: k for k, v in labels_dict.items()}\n    y = y.map(reverse_dict)\n    logger.info(\"Successfully mapped the target column\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_transformed,",
    "suffix": ""
  },
  {
    "name": "see2023/Bert-VITS2-ext:clap_gen.py@1201",
    "canonical_solution": "    with open(hps.data.validation_files, encoding=\"utf-8\") as f:",
    "prompt": "import argparse\nimport torch\nimport torch.multiprocessing as mp\nimport utils\nimport librosa\nimport os\nfrom multiprocessing import Pool, cpu_count\nfrom tqdm import tqdm\nfrom config import config\nfrom clap_wrapper import get_clap_audio_feature\n\n\n\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"\n\n\ndef process_line(line):\n    device = config.emo_gen_config.device\n    if config.emo_gen_config.use_multi_device:\n        rank = mp.current_process()._identity\n        rank = rank[0] if len(rank) > 0 else 0\n        if torch.cuda.is_available():\n            gpu_id = rank % torch.cuda.device_count()\n            device = torch.device(f\"cuda:{gpu_id}\")\n        else:\n            device = torch.device(\"cpu\")\n    wav_path, _, language_str, text, phones, tone, word2ph = line.strip().split(\"|\")\n\n    clap_path = wav_path.replace(\".WAV\", \".wav\").replace(\".wav\", \".emo.pt\")\n    if os.path.isfile(clap_path):\n        return\n\n    audio = librosa.load(wav_path, 48000)[0]\n    # audio = librosa.resample(audio, 44100, 48000)\n\n    clap = get_clap_audio_feature(audio, device)\n    torch.save(clap, clap_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-c\", \"--config\", type=str, default=config.emo_gen_config.config_path\n    )\n    parser.add_argument(\n        \"--num_processes\", type=int, default=config.emo_gen_config.num_processes\n    )\n    args, _ = parser.parse_known_args()\n    config_path = args.config\n    hps = utils.get_hparams_from_file(config_path)\n    lines = []\n    with open(hps.data.training_files, encoding=\"utf-8\") as f:\n        lines.extend(f.readlines())\n",
    "prefix": "import argparse\nimport torch\nimport torch.multiprocessing as mp\nimport utils\nimport librosa\nimport os\nfrom multiprocessing import Pool, cpu_count\nfrom tqdm import tqdm\nfrom config import config\nfrom clap_wrapper import get_clap_audio_feature\n\n\n\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"\n\n\ndef process_line(line):\n    device = config.emo_gen_config.device\n    if config.emo_gen_config.use_multi_device:\n        rank = mp.current_process()._identity\n        rank = rank[0] if len(rank) > 0 else 0\n        if torch.cuda.is_available():\n            gpu_id = rank % torch.cuda.device_count()\n            device = torch.device(f\"cuda:{gpu_id}\")\n        else:\n            device = torch.device(\"cpu\")\n    wav_path, _, language_str, text, phones, tone, word2ph = line.strip().split(\"|\")\n\n    clap_path = wav_path.replace(\".WAV\", \".wav\").replace(\".wav\", \".emo.pt\")\n    if os.path.isfile(clap_path):\n        return\n\n    audio = librosa.load(wav_path, 48000)[0]\n    # audio = librosa.resample(audio, 44100, 48000)\n\n    clap = get_clap_audio_feature(audio, device)\n    torch.save(clap, clap_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-c\", \"--config\", type=str, default=config.emo_gen_config.config_path\n    )\n    parser.add_argument(\n        \"--num_processes\", type=int, default=config.emo_gen_config.num_processes\n    )\n    args, _ = parser.parse_known_args()\n    config_path = args.config\n    hps = utils.get_hparams_from_file(config_path)\n    lines = []\n    with open(hps.data.training_files, encoding=\"utf-8\") as f:\n        lines.extend(f.readlines())\n",
    "suffix": ""
  },
  {
    "name": "chinhsuanwu/ifusion-threestudio:threestudio/models/background/neural_environment_map_background.py@1023",
    "canonical_solution": "            default_factory=lambda: {\"otype\": \"SphericalHarmonics\", \"degree\": 3}",
    "prompt": "import random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport threestudio\nfrom dataclasses import dataclass, field\nfrom threestudio.models.background.base import BaseBackground\nfrom threestudio.models.networks import get_encoding, get_mlp\nfrom threestudio.utils.ops import get_activation\nfrom threestudio.utils.typing import *\n\n\n\n\n@threestudio.register(\"neural-environment-map-background\")\nclass NeuralEnvironmentMapBackground(BaseBackground):\n    @dataclass\n    class Config(BaseBackground.Config):\n        n_output_dims: int = 3\n        color_activation: str = \"sigmoid\"\n        dir_encoding_config: dict = field(",
    "prefix": "import random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport threestudio\nfrom dataclasses import dataclass, field\nfrom threestudio.models.background.base import BaseBackground\nfrom threestudio.models.networks import get_encoding, get_mlp\nfrom threestudio.utils.ops import get_activation\nfrom threestudio.utils.typing import *\n\n\n\n\n@threestudio.register(\"neural-environment-map-background\")\nclass NeuralEnvironmentMapBackground(BaseBackground):\n    @dataclass\n    class Config(BaseBackground.Config):\n        n_output_dims: int = 3\n        color_activation: str = \"sigmoid\"\n        dir_encoding_config: dict = field(",
    "suffix": ""
  },
  {
    "name": "jasursadikov/mud:commands.py@675",
    "canonical_solution": "        self.current_color_index = 0",
    "prompt": "import utils\nimport asyncio\nimport subprocess\nfrom utils import TEXT, BACK, RESET, STYLES, END_STYLES, glyph\nfrom typing import List, Dict\nfrom collections import Counter\nfrom prettytable import PrettyTable, PLAIN_COLUMNS\n\n\n\nclass Commands:\n    def __init__(self, repos):\n        self.repos = repos\n        self.label_color_cache = {}",
    "prefix": "import utils\nimport asyncio\nimport subprocess\nfrom utils import TEXT, BACK, RESET, STYLES, END_STYLES, glyph\nfrom typing import List, Dict\nfrom collections import Counter\nfrom prettytable import PrettyTable, PLAIN_COLUMNS\n\n\n\nclass Commands:\n    def __init__(self, repos):\n        self.repos = repos\n        self.label_color_cache = {}",
    "suffix": ""
  },
  {
    "name": "Ananya2001-an/spotify-py-sdk:tests/endpoints/test_recommendations.py@1061",
    "canonical_solution": "def test_get_genres(api, get_genre_data):",
    "prompt": "import json\nimport pytest\nimport os\nfrom spotify_py_sdk import SpotifyApi\nfrom spotify_py_sdk.endpoints.recommendations import RecommendationsRequestRequiredArguments\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\n@pytest.fixture\ndef api():\n    return SpotifyApi(os.getenv(\"CLIENT_ID\"), os.getenv(\"CLIENT_SECRET\"))\n\n\n@pytest.fixture\ndef get_genre_data():\n    with open(\"tests/data/valid_genres.json\", \"r\") as f:\n        item = f.read()\n    return json.loads(item)\n\n",
    "prefix": "import json\nimport pytest\nimport os\nfrom spotify_py_sdk import SpotifyApi\nfrom spotify_py_sdk.endpoints.recommendations import RecommendationsRequestRequiredArguments\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\n@pytest.fixture\ndef api():\n    return SpotifyApi(os.getenv(\"CLIENT_ID\"), os.getenv(\"CLIENT_SECRET\"))\n\n\n@pytest.fixture\ndef get_genre_data():\n    with open(\"tests/data/valid_genres.json\", \"r\") as f:\n        item = f.read()\n    return json.loads(item)\n\n",
    "suffix": ""
  },
  {
    "name": "kyleliang919/Optimizer-Zoo:optimizer_zoo/Trainer/utils.py@1203",
    "canonical_solution": "    elif training_args.task == \"seq2seq\":",
    "prompt": "from transformers import Trainer, Seq2SeqTrainer\nfrom trl import SFTTrainer, DPOTrainer\nfrom .async_trainer import AsyncTrainer, AsyncSFTTrainer, AsyncDPOTrainer, AsyncSeq2SeqTrainer\ndef create_trainer(training_args):\n    if training_args.task == \"pretraining\":\n        return AsyncTrainer if training_args.async_grad else Trainer\n    elif training_args.task == \"sft\":\n        return AsyncSFTTrainer if training_args.async_grad else SFTTrainer\n    elif training_args.task == \"dpo\":\n        return AsyncDPOTrainer if training_args.async_grad else DPOTrainer",
    "prefix": "from transformers import Trainer, Seq2SeqTrainer\nfrom trl import SFTTrainer, DPOTrainer\nfrom .async_trainer import AsyncTrainer, AsyncSFTTrainer, AsyncDPOTrainer, AsyncSeq2SeqTrainer\ndef create_trainer(training_args):\n    if training_args.task == \"pretraining\":\n        return AsyncTrainer if training_args.async_grad else Trainer\n    elif training_args.task == \"sft\":\n        return AsyncSFTTrainer if training_args.async_grad else SFTTrainer\n    elif training_args.task == \"dpo\":\n        return AsyncDPOTrainer if training_args.async_grad else DPOTrainer",
    "suffix": ""
  },
  {
    "name": "giaminhgist/3D-DAM:lib/training/train.py@1384",
    "canonical_solution": "    metrics = OrderedDict([('loss', losses_m.avg), ('Acc', acc_m.avg)])",
    "prompt": "import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom collections import OrderedDict\nfrom lib.utils.utils import AverageMeter, accuracy\nfrom lib.utils.EarlyStopping import EarlyStopping\nfrom lib.training.train_helper import plot_result\nfrom sklearn.metrics import confusion_matrix\nfrom tqdm import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef train_one_epoch(\n        model,\n        loader,\n        optimizer,\n        epoch_idx: int,\n        lr_scheduler=None,\n):\n    losses_m = AverageMeter()\n    acc_m = AverageMeter()\n\n    model.train()\n    print('Start training epoch: ', epoch_idx)\n    for batch_idx, data in enumerate(tqdm(loader)):\n\n        images, target = data\n        images, target = images.to(device), target.to(device)\n        target = target.flatten()\n\n        output = model(images)\n\n        loss = nn.CrossEntropyLoss()(output, target)\n\n        losses_m.update(loss.item(), images.size(0))\n        acc1 = accuracy(output, target, topk=(1,))\n        acc_m.update(acc1[0].item(), output.size(0))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        torch.cuda.synchronize()\n\n    print(optimizer.param_groups[0]['lr'])\n\n    if hasattr(optimizer, 'sync_lookahead'):\n        optimizer.sync_lookahead()\n\n    metrics = OrderedDict([('loss', losses_m.avg), ('Acc', acc_m.avg)])\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n\n    return metrics\n\n\ndef validate(model, loader):\n    losses_m = AverageMeter()\n    acc_m = AverageMeter()\n\n    model.eval()\n\n    with torch.no_grad():\n        for batch_idx, data in enumerate(loader):\n            images, target = data\n            images, target = images.to(device), target.to(device)\n            target = target.flatten()\n\n            output = model(images)\n\n            loss = nn.CrossEntropyLoss()(output, target)\n            acc1 = accuracy(output, target, topk=(1,))\n            # reduced_loss = loss.data\n\n            torch.cuda.synchronize()\n\n            losses_m.update(loss.item(), images.size(0))\n            acc_m.update(acc1[0].item(), output.size(0))\n",
    "prefix": "import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom collections import OrderedDict\nfrom lib.utils.utils import AverageMeter, accuracy\nfrom lib.utils.EarlyStopping import EarlyStopping\nfrom lib.training.train_helper import plot_result\nfrom sklearn.metrics import confusion_matrix\nfrom tqdm import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef train_one_epoch(\n        model,\n        loader,\n        optimizer,\n        epoch_idx: int,\n        lr_scheduler=None,\n):\n    losses_m = AverageMeter()\n    acc_m = AverageMeter()\n\n    model.train()\n    print('Start training epoch: ', epoch_idx)\n    for batch_idx, data in enumerate(tqdm(loader)):\n\n        images, target = data\n        images, target = images.to(device), target.to(device)\n        target = target.flatten()\n\n        output = model(images)\n\n        loss = nn.CrossEntropyLoss()(output, target)\n\n        losses_m.update(loss.item(), images.size(0))\n        acc1 = accuracy(output, target, topk=(1,))\n        acc_m.update(acc1[0].item(), output.size(0))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        torch.cuda.synchronize()\n\n    print(optimizer.param_groups[0]['lr'])\n\n    if hasattr(optimizer, 'sync_lookahead'):\n        optimizer.sync_lookahead()\n\n    metrics = OrderedDict([('loss', losses_m.avg), ('Acc', acc_m.avg)])\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n\n    return metrics\n\n\ndef validate(model, loader):\n    losses_m = AverageMeter()\n    acc_m = AverageMeter()\n\n    model.eval()\n\n    with torch.no_grad():\n        for batch_idx, data in enumerate(loader):\n            images, target = data\n            images, target = images.to(device), target.to(device)\n            target = target.flatten()\n\n            output = model(images)\n\n            loss = nn.CrossEntropyLoss()(output, target)\n            acc1 = accuracy(output, target, topk=(1,))\n            # reduced_loss = loss.data\n\n            torch.cuda.synchronize()\n\n            losses_m.update(loss.item(), images.size(0))\n            acc_m.update(acc1[0].item(), output.size(0))\n",
    "suffix": ""
  },
  {
    "name": "xiaoye0x0/pfgo_tg_bot:utils/task/set_args.py@680",
    "canonical_solution": "def create_folder_if_not_exists(folder_path):",
    "prompt": "import os\nimport argparse\nfrom .model import Task\nfrom ..log import Logmanager\n\n\n\ndef is_file_exists(file_path) -> bool:\n    r = os.path.exists(file_path)\n    if not r:\n        LOGGER.error(f\"\u6587\u4ef6{file_path}\u4e0d\u5b58\u5728\")\n    return r\n\n",
    "prefix": "import os\nimport argparse\nfrom .model import Task\nfrom ..log import Logmanager\n\n\n\ndef is_file_exists(file_path) -> bool:\n    r = os.path.exists(file_path)\n    if not r:\n        LOGGER.error(f\"\u6587\u4ef6{file_path}\u4e0d\u5b58\u5728\")\n    return r\n\n",
    "suffix": ""
  },
  {
    "name": "shibing624/chatgpt-webui:src/shared.py@670",
    "canonical_solution": "            self.api_key_queue.put(api_key)",
    "prompt": "import os\nimport queue\nfrom src.presets import OPENAI_API_BASE, CHAT_COMPLETION_URL, BALANCE_API_URL, USAGE_API_URL, API_HOST, IMAGES_COMPLETION_URL\n\n\n\nclass State:\n    interrupted = False\n    multi_api_key = False\n    chat_completion_url = CHAT_COMPLETION_URL\n    balance_api_url = BALANCE_API_URL\n    usage_api_url = USAGE_API_URL\n    openai_api_base = OPENAI_API_BASE\n    images_completion_url = IMAGES_COMPLETION_URL\n\n    def interrupt(self):\n        self.interrupted = True\n\n    def recover(self):\n        self.interrupted = False\n\n    def set_api_host(self, api_host: str):\n        api_host = api_host.rstrip(\"/\")\n        if not api_host.startswith(\"http\"):\n            api_host = f\"https://{api_host}\"\n        if api_host.endswith(\"/v1\"):\n            api_host = api_host[:-3]\n        self.chat_completion_url = f\"{api_host}/v1/chat/completions\"\n        self.images_completion_url = f\"{api_host}/v1/images/generations\"\n        self.openai_api_base = f\"{api_host}/v1\"\n        self.balance_api_url = f\"{api_host}/dashboard/billing/credit_grants\"\n        self.usage_api_url = f\"{api_host}/dashboard/billing/usage\"\n        os.environ[\"OPENAI_API_BASE\"] = api_host\n\n    def reset_api_host(self):\n        self.chat_completion_url = CHAT_COMPLETION_URL\n        self.images_completion_url = IMAGES_COMPLETION_URL\n        self.balance_api_url = BALANCE_API_URL\n        self.usage_api_url = USAGE_API_URL\n        os.environ[\"OPENAI_API_BASE\"] = f\"https://{API_HOST}\"\n        return API_HOST\n\n    def reset_all(self):\n        self.interrupted = False\n        self.chat_completion_url = CHAT_COMPLETION_URL\n\n    def set_api_key_queue(self, api_key_list):\n        self.multi_api_key = True\n        self.api_key_queue = queue.Queue()\n        for api_key in api_key_list:\n            self.api_key_queue.put(api_key)\n\n    def switching_api_key(self, func):\n        if not hasattr(self, \"api_key_queue\"):\n            return func\n\n        def wrapped(*args, **kwargs):\n            api_key = self.api_key_queue.get()\n            args[0].api_key = api_key\n            ret = func(*args, **kwargs)",
    "prefix": "import os\nimport queue\nfrom src.presets import OPENAI_API_BASE, CHAT_COMPLETION_URL, BALANCE_API_URL, USAGE_API_URL, API_HOST, IMAGES_COMPLETION_URL\n\n\n\nclass State:\n    interrupted = False\n    multi_api_key = False\n    chat_completion_url = CHAT_COMPLETION_URL\n    balance_api_url = BALANCE_API_URL\n    usage_api_url = USAGE_API_URL\n    openai_api_base = OPENAI_API_BASE\n    images_completion_url = IMAGES_COMPLETION_URL\n\n    def interrupt(self):\n        self.interrupted = True\n\n    def recover(self):\n        self.interrupted = False\n\n    def set_api_host(self, api_host: str):\n        api_host = api_host.rstrip(\"/\")\n        if not api_host.startswith(\"http\"):\n            api_host = f\"https://{api_host}\"\n        if api_host.endswith(\"/v1\"):\n            api_host = api_host[:-3]\n        self.chat_completion_url = f\"{api_host}/v1/chat/completions\"\n        self.images_completion_url = f\"{api_host}/v1/images/generations\"\n        self.openai_api_base = f\"{api_host}/v1\"\n        self.balance_api_url = f\"{api_host}/dashboard/billing/credit_grants\"\n        self.usage_api_url = f\"{api_host}/dashboard/billing/usage\"\n        os.environ[\"OPENAI_API_BASE\"] = api_host\n\n    def reset_api_host(self):\n        self.chat_completion_url = CHAT_COMPLETION_URL\n        self.images_completion_url = IMAGES_COMPLETION_URL\n        self.balance_api_url = BALANCE_API_URL\n        self.usage_api_url = USAGE_API_URL\n        os.environ[\"OPENAI_API_BASE\"] = f\"https://{API_HOST}\"\n        return API_HOST\n\n    def reset_all(self):\n        self.interrupted = False\n        self.chat_completion_url = CHAT_COMPLETION_URL\n\n    def set_api_key_queue(self, api_key_list):\n        self.multi_api_key = True\n        self.api_key_queue = queue.Queue()\n        for api_key in api_key_list:\n            self.api_key_queue.put(api_key)\n\n    def switching_api_key(self, func):\n        if not hasattr(self, \"api_key_queue\"):\n            return func\n\n        def wrapped(*args, **kwargs):\n            api_key = self.api_key_queue.get()\n            args[0].api_key = api_key\n            ret = func(*args, **kwargs)",
    "suffix": ""
  },
  {
    "name": "camenduru/AnyDoor-online-hf:dinov2/dinov2/data/augmentations.py@1057",
    "canonical_solution": "        output[\"local_crops\"] = local_crops",
    "prompt": "import logging\nfrom torchvision import transforms\nfrom .transforms import (\n    GaussianBlur,\n    make_normalize_transform,\n)\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n\n\n\nlogger = logging.getLogger(\"dinov2\")\n\n\nclass DataAugmentationDINO(object):\n    def __init__(\n        self,\n        global_crops_scale,\n        local_crops_scale,\n        local_crops_number,\n        global_crops_size=224,\n        local_crops_size=96,\n    ):\n        self.global_crops_scale = global_crops_scale\n        self.local_crops_scale = local_crops_scale\n        self.local_crops_number = local_crops_number\n        self.global_crops_size = global_crops_size\n        self.local_crops_size = local_crops_size\n\n        logger.info(\"###################################\")\n        logger.info(\"Using data augmentation parameters:\")\n        logger.info(f\"global_crops_scale: {global_crops_scale}\")\n        logger.info(f\"local_crops_scale: {local_crops_scale}\")\n        logger.info(f\"local_crops_number: {local_crops_number}\")\n        logger.info(f\"global_crops_size: {global_crops_size}\")\n        logger.info(f\"local_crops_size: {local_crops_size}\")\n        logger.info(\"###################################\")\n\n        # random resized crop and flip\n        self.geometric_augmentation_global = transforms.Compose(\n            [\n                transforms.RandomResizedCrop(\n                    global_crops_size, scale=global_crops_scale, interpolation=transforms.InterpolationMode.BICUBIC\n                ),\n                transforms.RandomHorizontalFlip(p=0.5),\n            ]\n        )\n\n        self.geometric_augmentation_local = transforms.Compose(\n            [\n                transforms.RandomResizedCrop(\n                    local_crops_size, scale=local_crops_scale, interpolation=transforms.InterpolationMode.BICUBIC\n                ),\n                transforms.RandomHorizontalFlip(p=0.5),\n            ]\n        )\n\n        # color distorsions / blurring\n        color_jittering = transforms.Compose(\n            [\n                transforms.RandomApply(\n                    [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n                    p=0.8,\n                ),\n                transforms.RandomGrayscale(p=0.2),\n            ]\n        )\n\n        global_transfo1_extra = GaussianBlur(p=1.0)\n\n        global_transfo2_extra = transforms.Compose(\n            [\n                GaussianBlur(p=0.1),\n                transforms.RandomSolarize(threshold=128, p=0.2),\n            ]\n        )\n\n        local_transfo_extra = GaussianBlur(p=0.5)\n\n        # normalization\n        self.normalize = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                make_normalize_transform(),\n            ]\n        )\n\n        self.global_transfo1 = transforms.Compose([color_jittering, global_transfo1_extra, self.normalize])\n        self.global_transfo2 = transforms.Compose([color_jittering, global_transfo2_extra, self.normalize])\n        self.local_transfo = transforms.Compose([color_jittering, local_transfo_extra, self.normalize])\n\n    def __call__(self, image):\n        output = {}\n\n        # global crops:\n        im1_base = self.geometric_augmentation_global(image)\n        global_crop_1 = self.global_transfo1(im1_base)\n\n        im2_base = self.geometric_augmentation_global(image)\n        global_crop_2 = self.global_transfo2(im2_base)\n\n        output[\"global_crops\"] = [global_crop_1, global_crop_2]\n\n        # global crops for teacher:\n        output[\"global_crops_teacher\"] = [global_crop_1, global_crop_2]\n\n        # local crops:\n        local_crops = [\n            self.local_transfo(self.geometric_augmentation_local(image)) for _ in range(self.local_crops_number)\n        ]",
    "prefix": "import logging\nfrom torchvision import transforms\nfrom .transforms import (\n    GaussianBlur,\n    make_normalize_transform,\n)\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n\n\n\nlogger = logging.getLogger(\"dinov2\")\n\n\nclass DataAugmentationDINO(object):\n    def __init__(\n        self,\n        global_crops_scale,\n        local_crops_scale,\n        local_crops_number,\n        global_crops_size=224,\n        local_crops_size=96,\n    ):\n        self.global_crops_scale = global_crops_scale\n        self.local_crops_scale = local_crops_scale\n        self.local_crops_number = local_crops_number\n        self.global_crops_size = global_crops_size\n        self.local_crops_size = local_crops_size\n\n        logger.info(\"###################################\")\n        logger.info(\"Using data augmentation parameters:\")\n        logger.info(f\"global_crops_scale: {global_crops_scale}\")\n        logger.info(f\"local_crops_scale: {local_crops_scale}\")\n        logger.info(f\"local_crops_number: {local_crops_number}\")\n        logger.info(f\"global_crops_size: {global_crops_size}\")\n        logger.info(f\"local_crops_size: {local_crops_size}\")\n        logger.info(\"###################################\")\n\n        # random resized crop and flip\n        self.geometric_augmentation_global = transforms.Compose(\n            [\n                transforms.RandomResizedCrop(\n                    global_crops_size, scale=global_crops_scale, interpolation=transforms.InterpolationMode.BICUBIC\n                ),\n                transforms.RandomHorizontalFlip(p=0.5),\n            ]\n        )\n\n        self.geometric_augmentation_local = transforms.Compose(\n            [\n                transforms.RandomResizedCrop(\n                    local_crops_size, scale=local_crops_scale, interpolation=transforms.InterpolationMode.BICUBIC\n                ),\n                transforms.RandomHorizontalFlip(p=0.5),\n            ]\n        )\n\n        # color distorsions / blurring\n        color_jittering = transforms.Compose(\n            [\n                transforms.RandomApply(\n                    [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n                    p=0.8,\n                ),\n                transforms.RandomGrayscale(p=0.2),\n            ]\n        )\n\n        global_transfo1_extra = GaussianBlur(p=1.0)\n\n        global_transfo2_extra = transforms.Compose(\n            [\n                GaussianBlur(p=0.1),\n                transforms.RandomSolarize(threshold=128, p=0.2),\n            ]\n        )\n\n        local_transfo_extra = GaussianBlur(p=0.5)\n\n        # normalization\n        self.normalize = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                make_normalize_transform(),\n            ]\n        )\n\n        self.global_transfo1 = transforms.Compose([color_jittering, global_transfo1_extra, self.normalize])\n        self.global_transfo2 = transforms.Compose([color_jittering, global_transfo2_extra, self.normalize])\n        self.local_transfo = transforms.Compose([color_jittering, local_transfo_extra, self.normalize])\n\n    def __call__(self, image):\n        output = {}\n\n        # global crops:\n        im1_base = self.geometric_augmentation_global(image)\n        global_crop_1 = self.global_transfo1(im1_base)\n\n        im2_base = self.geometric_augmentation_global(image)\n        global_crop_2 = self.global_transfo2(im2_base)\n\n        output[\"global_crops\"] = [global_crop_1, global_crop_2]\n\n        # global crops for teacher:\n        output[\"global_crops_teacher\"] = [global_crop_1, global_crop_2]\n\n        # local crops:\n        local_crops = [\n            self.local_transfo(self.geometric_augmentation_local(image)) for _ in range(self.local_crops_number)\n        ]",
    "suffix": ""
  },
  {
    "name": "omkarcloud/google-scraper:src/google_scraper.py@837",
    "canonical_solution": "    return success, credits_exhausted, not_subscribed, unknown_error, no_key",
    "prompt": "from typing import List,Optional, Union, Dict\nfrom botasaurus import bt\nfrom .write_output import write_output\nfrom .search import FAILED_DUE_TO_CREDITS_EXHAUSTED, FAILED_DUE_TO_NO_KEY,FAILED_DUE_TO_NOT_SUBSCRIBED, FAILED_DUE_TO_UNKNOWN_ERROR, search\n\n\n\ndef clean_data(social_details):\n    success, credits_exhausted, not_subscribed, unknown_error, no_key = [], [], [], [], []\n\n    for detail in social_details:\n        if detail.get(\"error\") is None:\n            success.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_CREDITS_EXHAUSTED:\n            credits_exhausted.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_NOT_SUBSCRIBED:\n            not_subscribed.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_UNKNOWN_ERROR:\n            unknown_error.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_NO_KEY:\n            no_key.append(detail)\n",
    "prefix": "from typing import List,Optional, Union, Dict\nfrom botasaurus import bt\nfrom .write_output import write_output\nfrom .search import FAILED_DUE_TO_CREDITS_EXHAUSTED, FAILED_DUE_TO_NO_KEY,FAILED_DUE_TO_NOT_SUBSCRIBED, FAILED_DUE_TO_UNKNOWN_ERROR, search\n\n\n\ndef clean_data(social_details):\n    success, credits_exhausted, not_subscribed, unknown_error, no_key = [], [], [], [], []\n\n    for detail in social_details:\n        if detail.get(\"error\") is None:\n            success.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_CREDITS_EXHAUSTED:\n            credits_exhausted.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_NOT_SUBSCRIBED:\n            not_subscribed.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_UNKNOWN_ERROR:\n            unknown_error.append(detail)\n        elif detail[\"error\"] == FAILED_DUE_TO_NO_KEY:\n            no_key.append(detail)\n",
    "suffix": ""
  },
  {
    "name": "PyPSA/nowcast:concatenate_weeks.py@919",
    "canonical_solution": "        config.update(yaml.safe_load(file))",
    "prompt": "import pypsa, yaml, pandas as pd, os, datetime, sys\nfrom concatenate_networks import concatenate, safe_pypsa_import\n## Copyright 2023-4 Tom Brown\n\n## This program is free software; you can redistribute it and/or\n## modify it under the terms of the GNU Affero General Public License as\n## published by the Free Software Foundation; either version 3 of the\n## License, or (at your option) any later version.\n\n## This program is distributed in the hope that it will be useful,\n## but WITHOUT ANY WARRANTY; without even the implied warranty of\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n## GNU Affero General Public License for more details.\n\n## License and more information at:\n## https://github.com/PyPSA/nowcast\n\n\n\n\n\ndef concatenate_week(date_strings, week_fn, config):\n\n    print(f\"concatenating {date_strings} to {week_fn}\")\n\n    ct = config[\"countries\"][0]\n\n    extended_hours = config[\"extended_hours\"]\n\n    results_dir = f\"{config['results_dir']}/{config['scenario']}\"\n\n    for i,date_string in enumerate(date_strings):\n\n        fn = f\"{results_dir}/{ct}-day-{date_string}.nc\"\n\n        ni = safe_pypsa_import(fn)\n\n        #truncate overlap\n        ni.set_snapshots(ni.snapshots[:-extended_hours])\n\n        if i == 0:\n            n = ni\n        else:\n            n = concatenate(n,ni)\n    n.export_to_netcdf(week_fn)\n\ndef concatenate_weeks(config):\n\n    print(\"concatenating all day networks to week networks\")\n\n    ct = config[\"countries\"][0]\n\n    results_dir = f\"{config['results_dir']}/{config['scenario']}\"\n\n    end_date = config[\"end_date\"]\n\n    if end_date == \"today\":\n        end_date = datetime.date.today()\n    elif end_date == \"yesterday\":\n        end_date = datetime.date.today() - datetime.timedelta(days=1)\n\n    date_index = pd.date_range(start=config[\"start_date\"],\n                               end=end_date)\n\n    isocalendar = date_index.isocalendar()\n\n    for year in isocalendar.year.unique():\n\n        for week in isocalendar[isocalendar.year == year].week.unique():\n            dates = date_index[(isocalendar.week == week) & (isocalendar.year == year)]\n            date_strings = [str(date.date()) for date in dates]\n\n            week_fn = f\"{results_dir}/{ct}-week-{year}-{week}.nc\"\n            date_fns = [f\"{results_dir}/{ct}-day-{ds}.nc\" for ds in date_strings]\n\n            if os.path.isfile(week_fn):\n                if pd.Index([os.path.getmtime(week_fn) > os.path.getmtime(date_fn) for date_fn in date_fns]).all():\n                    continue\n            concatenate_week(date_strings,week_fn,config)\n\n\nif __name__ == \"__main__\":\n\n    with open('config.yaml', 'r') as file:\n        config = yaml.safe_load(file)\n\n    scenario_fn = sys.argv[1]\n\n    with open(scenario_fn, 'r') as file:",
    "prefix": "import pypsa, yaml, pandas as pd, os, datetime, sys\nfrom concatenate_networks import concatenate, safe_pypsa_import\n## Copyright 2023-4 Tom Brown\n\n## This program is free software; you can redistribute it and/or\n## modify it under the terms of the GNU Affero General Public License as\n## published by the Free Software Foundation; either version 3 of the\n## License, or (at your option) any later version.\n\n## This program is distributed in the hope that it will be useful,\n## but WITHOUT ANY WARRANTY; without even the implied warranty of\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n## GNU Affero General Public License for more details.\n\n## License and more information at:\n## https://github.com/PyPSA/nowcast\n\n\n\n\n\ndef concatenate_week(date_strings, week_fn, config):\n\n    print(f\"concatenating {date_strings} to {week_fn}\")\n\n    ct = config[\"countries\"][0]\n\n    extended_hours = config[\"extended_hours\"]\n\n    results_dir = f\"{config['results_dir']}/{config['scenario']}\"\n\n    for i,date_string in enumerate(date_strings):\n\n        fn = f\"{results_dir}/{ct}-day-{date_string}.nc\"\n\n        ni = safe_pypsa_import(fn)\n\n        #truncate overlap\n        ni.set_snapshots(ni.snapshots[:-extended_hours])\n\n        if i == 0:\n            n = ni\n        else:\n            n = concatenate(n,ni)\n    n.export_to_netcdf(week_fn)\n\ndef concatenate_weeks(config):\n\n    print(\"concatenating all day networks to week networks\")\n\n    ct = config[\"countries\"][0]\n\n    results_dir = f\"{config['results_dir']}/{config['scenario']}\"\n\n    end_date = config[\"end_date\"]\n\n    if end_date == \"today\":\n        end_date = datetime.date.today()\n    elif end_date == \"yesterday\":\n        end_date = datetime.date.today() - datetime.timedelta(days=1)\n\n    date_index = pd.date_range(start=config[\"start_date\"],\n                               end=end_date)\n\n    isocalendar = date_index.isocalendar()\n\n    for year in isocalendar.year.unique():\n\n        for week in isocalendar[isocalendar.year == year].week.unique():\n            dates = date_index[(isocalendar.week == week) & (isocalendar.year == year)]\n            date_strings = [str(date.date()) for date in dates]\n\n            week_fn = f\"{results_dir}/{ct}-week-{year}-{week}.nc\"\n            date_fns = [f\"{results_dir}/{ct}-day-{ds}.nc\" for ds in date_strings]\n\n            if os.path.isfile(week_fn):\n                if pd.Index([os.path.getmtime(week_fn) > os.path.getmtime(date_fn) for date_fn in date_fns]).all():\n                    continue\n            concatenate_week(date_strings,week_fn,config)\n\n\nif __name__ == \"__main__\":\n\n    with open('config.yaml', 'r') as file:\n        config = yaml.safe_load(file)\n\n    scenario_fn = sys.argv[1]\n\n    with open(scenario_fn, 'r') as file:",
    "suffix": ""
  },
  {
    "name": "AI2lab/comfyUI-tool-2lab:nodes/image/image_process.py@1212",
    "canonical_solution": "        return {",
    "prompt": "from ..common.sizes import get_image_size\nfrom ..constants import get_project_name, get_project_category\nfrom ..common import fields\nimport math\nimport comfy.utils   # comfyUI\u5185\u90e8\u5305\n\n\nNODE_CATEGORY = get_project_category(\"image\")\n\n# fork from https://github.com/Derfuu/Derfuu_ComfyUI_ModdedNodes/, thx!\nclass ImageScale_Side:\n    NAME = get_project_name('ImageScale_Side')\n    CATEGORY = NODE_CATEGORY\n    RETURN_TYPES = (\"IMAGE\", )\n    RETURN_NAMES = (\"image\", )\n    OUTPUT_NODE = True\n    FUNCTION = \"doWork\"\n\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\"]\n    crop_methods = [\"disabled\", \"center\"]\n\n    def __init__(self) -> None:\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": fields.IMAGE,\n                \"side_length\": fields.INT,\n                \"side\": ([\"Longest\", \"Width\", \"Height\"],),\n                \"upscale_method\": (cls.upscale_methods,),\n                \"crop\": (cls.crop_methods,)}}\n\n    def doWork(self, image, upscale_method, side_length: int, side: str, crop):\n        samples = image.movedim(-1, 1)\n\n        size = get_image_size(image)\n\n        width_B = int(size[0])\n        height_B = int(size[1])\n\n        width = width_B\n        height = height_B\n\n        def determineSide(_side: str) -> tuple[int, int]:\n            width, height = 0, 0\n            if _side == \"Width\":\n                heigh_ratio = height_B / width_B\n                width = side_length\n                height = heigh_ratio * width\n            elif _side == \"Height\":\n                width_ratio = width_B / height_B\n                height = side_length\n                width = width_ratio * height\n            return width, height\n\n        if side == \"Longest\":\n            if width > height:\n                width, height = determineSide(\"Width\")\n            else:\n                width, height = determineSide(\"Height\")\n        else:\n            width, height = determineSide(side)\n\n        width = math.ceil(width)\n        height = math.ceil(height)\n\n        cls = comfy.utils.common_upscale(samples, width, height, upscale_method, crop)\n        cls = cls.movedim(1, -1)\n        return (cls,)\n\nclass CropImage:\n    NAME = get_project_name('CropImage')\n    CATEGORY = NODE_CATEGORY\n    RETURN_TYPES = (\"IMAGE\", )\n    RETURN_NAMES = (\"image\", )\n    OUTPUT_NODE = True\n    FUNCTION = \"doWork\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": fields.IMAGE_FORCEINPUT,\n            }\n        }\n\n    def doWork(self, image):\n        samples = image.movedim(-1, 1)\n        # \u83b7\u53d6\u539f\u59cb\u5c3a\u5bf8\n        old_width = samples.shape[3]\n        old_height = samples.shape[2]\n\n        # \u8ba1\u7b97\u526a\u88c1\u5c3a\u5bf8\n        min_size = min(old_width, old_height)\n        print(min_size)\n\n        # \u8ba1\u7b97\u526a\u88c1\u8d77\u70b9\n        x_start = (old_width - min_size) // 2\n        y_start = (old_height - min_size) // 2\n\n        # \u6267\u884c\u4e2d\u5fc3\u526a\u88c1\n        cropped_image = samples[:, :, y_start:y_start + min_size, x_start:x_start + min_size]\n        result_image = cropped_image.movedim(1, -1)\n\n        return {\"result\": (result_image,)}\n\nclass WatermarkOffset:\n    NAME = get_project_name('WatermarkOffset')\n    CATEGORY = NODE_CATEGORY\n    RETURN_TYPES = (\"INT\", \"INT\",)\n    RETURN_NAMES = (\"xOffset\", \"yOffset\",)\n    OUTPUT_NODE = True\n    FUNCTION = \"doWork\"\n\n    @classmethod\n    def INPUT_TYPES(cls):",
    "prefix": "from ..common.sizes import get_image_size\nfrom ..constants import get_project_name, get_project_category\nfrom ..common import fields\nimport math\nimport comfy.utils   # comfyUI\u5185\u90e8\u5305\n\n\nNODE_CATEGORY = get_project_category(\"image\")\n\n# fork from https://github.com/Derfuu/Derfuu_ComfyUI_ModdedNodes/, thx!\nclass ImageScale_Side:\n    NAME = get_project_name('ImageScale_Side')\n    CATEGORY = NODE_CATEGORY\n    RETURN_TYPES = (\"IMAGE\", )\n    RETURN_NAMES = (\"image\", )\n    OUTPUT_NODE = True\n    FUNCTION = \"doWork\"\n\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\"]\n    crop_methods = [\"disabled\", \"center\"]\n\n    def __init__(self) -> None:\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": fields.IMAGE,\n                \"side_length\": fields.INT,\n                \"side\": ([\"Longest\", \"Width\", \"Height\"],),\n                \"upscale_method\": (cls.upscale_methods,),\n                \"crop\": (cls.crop_methods,)}}\n\n    def doWork(self, image, upscale_method, side_length: int, side: str, crop):\n        samples = image.movedim(-1, 1)\n\n        size = get_image_size(image)\n\n        width_B = int(size[0])\n        height_B = int(size[1])\n\n        width = width_B\n        height = height_B\n\n        def determineSide(_side: str) -> tuple[int, int]:\n            width, height = 0, 0\n            if _side == \"Width\":\n                heigh_ratio = height_B / width_B\n                width = side_length\n                height = heigh_ratio * width\n            elif _side == \"Height\":\n                width_ratio = width_B / height_B\n                height = side_length\n                width = width_ratio * height\n            return width, height\n\n        if side == \"Longest\":\n            if width > height:\n                width, height = determineSide(\"Width\")\n            else:\n                width, height = determineSide(\"Height\")\n        else:\n            width, height = determineSide(side)\n\n        width = math.ceil(width)\n        height = math.ceil(height)\n\n        cls = comfy.utils.common_upscale(samples, width, height, upscale_method, crop)\n        cls = cls.movedim(1, -1)\n        return (cls,)\n\nclass CropImage:\n    NAME = get_project_name('CropImage')\n    CATEGORY = NODE_CATEGORY\n    RETURN_TYPES = (\"IMAGE\", )\n    RETURN_NAMES = (\"image\", )\n    OUTPUT_NODE = True\n    FUNCTION = \"doWork\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": fields.IMAGE_FORCEINPUT,\n            }\n        }\n\n    def doWork(self, image):\n        samples = image.movedim(-1, 1)\n        # \u83b7\u53d6\u539f\u59cb\u5c3a\u5bf8\n        old_width = samples.shape[3]\n        old_height = samples.shape[2]\n\n        # \u8ba1\u7b97\u526a\u88c1\u5c3a\u5bf8\n        min_size = min(old_width, old_height)\n        print(min_size)\n\n        # \u8ba1\u7b97\u526a\u88c1\u8d77\u70b9\n        x_start = (old_width - min_size) // 2\n        y_start = (old_height - min_size) // 2\n\n        # \u6267\u884c\u4e2d\u5fc3\u526a\u88c1\n        cropped_image = samples[:, :, y_start:y_start + min_size, x_start:x_start + min_size]\n        result_image = cropped_image.movedim(1, -1)\n\n        return {\"result\": (result_image,)}\n\nclass WatermarkOffset:\n    NAME = get_project_name('WatermarkOffset')\n    CATEGORY = NODE_CATEGORY\n    RETURN_TYPES = (\"INT\", \"INT\",)\n    RETURN_NAMES = (\"xOffset\", \"yOffset\",)\n    OUTPUT_NODE = True\n    FUNCTION = \"doWork\"\n\n    @classmethod\n    def INPUT_TYPES(cls):",
    "suffix": ""
  },
  {
    "name": "Amirtheahmed/ddd-cqrs-fastapi:src/contexts/photostore/photo/application/createone/CreatePhotoCommandHandler.py@772",
    "canonical_solution": "        return self.__subscription",
    "prompt": "from typing import NoReturn\nfrom src.contexts.backoffice.users.domain.entities.UserId import UserId\nfrom src.contexts.photostore.photo.application.createone.CreatePhotoCommand import CreatePhotoCommand\nfrom src.contexts.photostore.photo.application.createone.PhotoCreator import PhotoCreator\nfrom src.contexts.photostore.photo.domain.entities.PhotoFile import PhotoFile\nfrom src.contexts.photostore.photo.domain.entities.PhotoId import PhotoId\nfrom src.contexts.photostore.photo.domain.entities.PhotoName import PhotoName\nfrom src.contexts.shared.domain.BaseObject import BaseObject\nfrom src.contexts.shared.domain.CommandHandler import CommandHandler\n\n\n\nclass CreatePhotoCommandHandler(BaseObject, CommandHandler):\n\n    __subscription: str = CreatePhotoCommand.COMMAND_TYPE\n\n    def __init__(self, creator: PhotoCreator):\n        self.__creator = creator\n\n    def subscribed_to(self) -> str:",
    "prefix": "from typing import NoReturn\nfrom src.contexts.backoffice.users.domain.entities.UserId import UserId\nfrom src.contexts.photostore.photo.application.createone.CreatePhotoCommand import CreatePhotoCommand\nfrom src.contexts.photostore.photo.application.createone.PhotoCreator import PhotoCreator\nfrom src.contexts.photostore.photo.domain.entities.PhotoFile import PhotoFile\nfrom src.contexts.photostore.photo.domain.entities.PhotoId import PhotoId\nfrom src.contexts.photostore.photo.domain.entities.PhotoName import PhotoName\nfrom src.contexts.shared.domain.BaseObject import BaseObject\nfrom src.contexts.shared.domain.CommandHandler import CommandHandler\n\n\n\nclass CreatePhotoCommandHandler(BaseObject, CommandHandler):\n\n    __subscription: str = CreatePhotoCommand.COMMAND_TYPE\n\n    def __init__(self, creator: PhotoCreator):\n        self.__creator = creator\n\n    def subscribed_to(self) -> str:",
    "suffix": ""
  },
  {
    "name": "serpatsama/Pythox:pythox/scanner.py@1591",
    "canonical_solution": "            print(f\"Unterminated comment on line: {self.line}\",",
    "prompt": "import sys\nfrom .token import Token, TokenType\n\n\nclass Scanner():\n    def __init__(self, source: str):\n        self.tokens: list[Token] = []\n        self.start: int   = 0\n        self.current: int = 0\n        self.line: int    = 1\n        self.column: int  = 1\n        self.source: str  = source.strip()\n        self._keywords: dict[str, TokenType] = {\n        'and'    : TokenType.AND,\n        'class'  : TokenType.CLASS,\n        'else'   : TokenType.ELSE,\n        'false'  : TokenType.FALSE,\n        'fun'    : TokenType.FUN,\n        'for'    : TokenType.FOR,\n        'if'     : TokenType.IF,\n        'nil'    : TokenType.NIL,\n        'or'     : TokenType.OR,\n        'print'  : TokenType.PRINT,\n        'return' : TokenType.RETURN,\n        'super'  : TokenType.SUPER,\n        'this'   : TokenType.THIS,\n        'true'   : TokenType.TRUE,\n        'var'    : TokenType.VAR,\n        'while'  : TokenType.WHILE}\n    \n    def scanTokens(self) -> list:\n        while(not self.isAtEnd()):\n            self.start = self.current\n            self.scanToken()\n        \n        self.tokens.append(Token(TokenType.EOF,\n                                 \"\",\n                                 None,\n                                 self.line))\n        return self.tokens\n\n    def scanToken(self) -> None:\n        c: str = self.advance()\n        match c:\n            case '(': self.addToken(TokenType.LEFT_PAREN)\n            case ')': self.addToken(TokenType.RIGHT_PAREN)\n            case '{': self.addToken(TokenType.LEFT_BRACE)\n            case '}': self.addToken(TokenType.RIGHT_BRACE)\n            case ',': self.addToken(TokenType.COMMA)\n            case '.': self.addToken(TokenType.DOT)\n            case '-': self.addToken(TokenType.MINUS)\n            case '+': self.addToken(TokenType.PLUS)\n            case ';': self.addToken(TokenType.SEMICOLON)\n            case '*': self.addToken(TokenType.STAR)\n            case '!': self.addToken(TokenType.BANG_EQUAL\n                                    if self.match('=')\n                                    else TokenType.BANG)\n            case '=': self.addToken(TokenType.EQUAL_EQUAL\n                                    if self.match('=')\n                                    else TokenType.EQUAL)\n            case '<': self.addToken(TokenType.LESS_EQUAL\n                                    if self.match('=')\n                                    else TokenType.LESS)\n            case '>': self.addToken(TokenType.GREATER_EQUAL\n                                    if self.match('=')\n                                    else TokenType.GREATER)\n            case '?': self.addToken(TokenType.QUESTION)\n            case ':': self.addToken(TokenType.COLON)\n            case '/': \n                if self.match('/'):\n                    while self.peek() != '\\n' and not self.isAtEnd():\n                        self.advance()\n                elif self.match('*'): # FIXME: Convoluted garbage..refactor\n                    self.blockComment()\n                else:\n                    self.addToken(TokenType.SLASH)\n            case ' ' | '\\r' | '\\t': pass\n            case '\\n': \n                self.line += 1\n                self.column = 1\n            case '\"' : self.string()\n            case  _  : \n                if self.isDigit(c):\n                    self.number()\n                elif self.isAlpha(c):\n                    self.identifier()\n                else:\n                    print(f\"Unexpected token {c} on line: {self.line}\",\n                          file=sys.stderr)\n                    \n                    print(self.printScanError(),\n                          file=sys.stderr) # FIXME: Set hadError\n                \n    def string(self) -> None:\n        while self.peek() != '\"' and not self.isAtEnd():\n            if self.peek() == '\\n':\n                self.line += 1\n            self.advance()\n        \n        if self.isAtEnd():\n            print(\"Unterminated String\", file=sys.stderr)\n            return\n        \n        # Consume closing \"\n        self.advance()\n\n        value: str = self.source[self.start + 1 : self.current - 1]\n        self.addToken(TokenType.STRING, value)\n    \n    def number(self) -> None:\n        while self.isDigit(self.peek()):\n            self.advance()\n        \n        if self.peek() == '.' and self.isDigit(self.peekNext()):\n            self.advance()\n            while self.isDigit(self.peek()):\n                self.advance()\n        \n        self.addToken(TokenType.NUMBER,\n                      float(self.source[self.start : self.current]))\n\n    def identifier(self):\n        while self.isAlphaNumeric(self.peek()):\n            self.advance()\n        \n        text: str = self.source[self.start : self.current]\n        try:\n            type: TokenType = self._keywords[text]\n            self.addToken(TokenType(type))\n        except KeyError:\n            self.addToken(TokenType.IDENTIFIER)\n        \n    def blockComment(self) -> None:\n        while ((self.isAtEnd() == False) and (self.peek() != '*')):\n            self.advance()\n            if self.peek() == '\\n':\n                self.line += 1\n        \n        if self.isAtEnd():\n            print(f\"Unterminated comment on line: {self.line}:{self.column}\",\n                  self.printScanError(), sep='\\n' ,file=sys.stderr)\n\n            return None\n        \n        if not self.isAtEnd() and self.peek() == '*':\n            self.advance()\n            if not self.isAtEnd() and self.peek() == '/':\n                self.advance()\n            else:\n                print(f\"Unterminated comment on line: {self.line}\",\n                      self.printScanError(), sep='\\n', file=sys.stderr)\n        else:",
    "prefix": "import sys\nfrom .token import Token, TokenType\n\n\nclass Scanner():\n    def __init__(self, source: str):\n        self.tokens: list[Token] = []\n        self.start: int   = 0\n        self.current: int = 0\n        self.line: int    = 1\n        self.column: int  = 1\n        self.source: str  = source.strip()\n        self._keywords: dict[str, TokenType] = {\n        'and'    : TokenType.AND,\n        'class'  : TokenType.CLASS,\n        'else'   : TokenType.ELSE,\n        'false'  : TokenType.FALSE,\n        'fun'    : TokenType.FUN,\n        'for'    : TokenType.FOR,\n        'if'     : TokenType.IF,\n        'nil'    : TokenType.NIL,\n        'or'     : TokenType.OR,\n        'print'  : TokenType.PRINT,\n        'return' : TokenType.RETURN,\n        'super'  : TokenType.SUPER,\n        'this'   : TokenType.THIS,\n        'true'   : TokenType.TRUE,\n        'var'    : TokenType.VAR,\n        'while'  : TokenType.WHILE}\n    \n    def scanTokens(self) -> list:\n        while(not self.isAtEnd()):\n            self.start = self.current\n            self.scanToken()\n        \n        self.tokens.append(Token(TokenType.EOF,\n                                 \"\",\n                                 None,\n                                 self.line))\n        return self.tokens\n\n    def scanToken(self) -> None:\n        c: str = self.advance()\n        match c:\n            case '(': self.addToken(TokenType.LEFT_PAREN)\n            case ')': self.addToken(TokenType.RIGHT_PAREN)\n            case '{': self.addToken(TokenType.LEFT_BRACE)\n            case '}': self.addToken(TokenType.RIGHT_BRACE)\n            case ',': self.addToken(TokenType.COMMA)\n            case '.': self.addToken(TokenType.DOT)\n            case '-': self.addToken(TokenType.MINUS)\n            case '+': self.addToken(TokenType.PLUS)\n            case ';': self.addToken(TokenType.SEMICOLON)\n            case '*': self.addToken(TokenType.STAR)\n            case '!': self.addToken(TokenType.BANG_EQUAL\n                                    if self.match('=')\n                                    else TokenType.BANG)\n            case '=': self.addToken(TokenType.EQUAL_EQUAL\n                                    if self.match('=')\n                                    else TokenType.EQUAL)\n            case '<': self.addToken(TokenType.LESS_EQUAL\n                                    if self.match('=')\n                                    else TokenType.LESS)\n            case '>': self.addToken(TokenType.GREATER_EQUAL\n                                    if self.match('=')\n                                    else TokenType.GREATER)\n            case '?': self.addToken(TokenType.QUESTION)\n            case ':': self.addToken(TokenType.COLON)\n            case '/': \n                if self.match('/'):\n                    while self.peek() != '\\n' and not self.isAtEnd():\n                        self.advance()\n                elif self.match('*'): # FIXME: Convoluted garbage..refactor\n                    self.blockComment()\n                else:\n                    self.addToken(TokenType.SLASH)\n            case ' ' | '\\r' | '\\t': pass\n            case '\\n': \n                self.line += 1\n                self.column = 1\n            case '\"' : self.string()\n            case  _  : \n                if self.isDigit(c):\n                    self.number()\n                elif self.isAlpha(c):\n                    self.identifier()\n                else:\n                    print(f\"Unexpected token {c} on line: {self.line}\",\n                          file=sys.stderr)\n                    \n                    print(self.printScanError(),\n                          file=sys.stderr) # FIXME: Set hadError\n                \n    def string(self) -> None:\n        while self.peek() != '\"' and not self.isAtEnd():\n            if self.peek() == '\\n':\n                self.line += 1\n            self.advance()\n        \n        if self.isAtEnd():\n            print(\"Unterminated String\", file=sys.stderr)\n            return\n        \n        # Consume closing \"\n        self.advance()\n\n        value: str = self.source[self.start + 1 : self.current - 1]\n        self.addToken(TokenType.STRING, value)\n    \n    def number(self) -> None:\n        while self.isDigit(self.peek()):\n            self.advance()\n        \n        if self.peek() == '.' and self.isDigit(self.peekNext()):\n            self.advance()\n            while self.isDigit(self.peek()):\n                self.advance()\n        \n        self.addToken(TokenType.NUMBER,\n                      float(self.source[self.start : self.current]))\n\n    def identifier(self):\n        while self.isAlphaNumeric(self.peek()):\n            self.advance()\n        \n        text: str = self.source[self.start : self.current]\n        try:\n            type: TokenType = self._keywords[text]\n            self.addToken(TokenType(type))\n        except KeyError:\n            self.addToken(TokenType.IDENTIFIER)\n        \n    def blockComment(self) -> None:\n        while ((self.isAtEnd() == False) and (self.peek() != '*')):\n            self.advance()\n            if self.peek() == '\\n':\n                self.line += 1\n        \n        if self.isAtEnd():\n            print(f\"Unterminated comment on line: {self.line}:{self.column}\",\n                  self.printScanError(), sep='\\n' ,file=sys.stderr)\n\n            return None\n        \n        if not self.isAtEnd() and self.peek() == '*':\n            self.advance()\n            if not self.isAtEnd() and self.peek() == '/':\n                self.advance()\n            else:\n                print(f\"Unterminated comment on line: {self.line}\",\n                      self.printScanError(), sep='\\n', file=sys.stderr)\n        else:",
    "suffix": ""
  },
  {
    "name": "kvojps/open-source-project-observatory:backend/api/services/dtos/repository.py@686",
    "canonical_solution": "            .get(\"totalCommits\", None)",
    "prompt": "from typing import List, Optional\nfrom pydantic import BaseModel\nfrom .repository_graphics import (\n    IssueGraphicResponse,\n    LabelResponse,\n    PullRequestGraphicResponse,\n)\n\n\nclass RepositoryResponse(BaseModel):\n    owner: Optional[str]\n    name: Optional[str]\n    description: Optional[str]\n    last_activity: Optional[str]\n    license_url: Optional[str]\n    website_url: Optional[str]\n    topics: Optional[List[str]]\n    stars_amount: Optional[int]\n    readme_url: Optional[str]\n    contribution_url: Optional[str]\n    contributors_amount: Optional[int]\n    forks_amount: Optional[int]\n    branches_amount: Optional[int]\n    issues_amount: Optional[int]\n    prs_amount: Optional[int]\n    commits_amount: Optional[int]\n    issues_graphic: Optional[IssueGraphicResponse]\n    prs_graphic: Optional[PullRequestGraphicResponse]\n    top_ten_labels_graphic: Optional[List[LabelResponse]]\n\n    @classmethod\n    def from_repository_response(\n        cls, owner: str, name: str, repository_data\n    ) -> \"RepositoryResponse\":\n        labels_data = repository_data.get(\"labels\", None).get(\"nodes\", None)\n        sorted_labels_data = sorted(\n            labels_data, key=lambda x: x[\"issues\"][\"totalCount\"], reverse=True\n        )\n\n        return cls(\n            owner=owner,\n            name=name,\n            description=repository_data.get(\"description\", None),\n            last_activity=repository_data.get(\"defaultBranchRef\", None)\n            .get(\"target\", None)\n            .get(\"history\", None)\n            .get(\"edges\", None)[0]\n            .get(\"node\", None)\n            .get(\"committedDate\", None),\n            license_url=None,\n            website_url=repository_data.get(\"homepageUrl\", None),\n            topics=[\n                topic[\"node\"][\"topic\"][\"name\"]\n                for topic in repository_data.get(\"repositoryTopics\", None).get(\n                    \"edges\", []\n                )\n            ],\n            stars_amount=repository_data.get(\"stargazers\", None).get(\n                \"totalCount\", None\n            ),\n            readme_url=None,\n            contribution_url=None,\n            contributors_amount=repository_data.get(\"mentionableUsers\", None).get(\n                \"totalCount\", None\n            ),\n            forks_amount=repository_data.get(\"forks\", None).get(\"totalCount\", None),\n            branches_amount=repository_data.get(\"refs\", None).get(\"totalCount\", None),\n            issues_amount=repository_data.get(\"issues\", None).get(\"totalCount\", None),\n            prs_amount=repository_data.get(\"pullRequests\", None).get(\n                \"totalCount\", None\n            ),\n            commits_amount=repository_data.get(\"defaultBranchRef\", None)\n            .get(\"target\", None)",
    "prefix": "from typing import List, Optional\nfrom pydantic import BaseModel\nfrom .repository_graphics import (\n    IssueGraphicResponse,\n    LabelResponse,\n    PullRequestGraphicResponse,\n)\n\n\nclass RepositoryResponse(BaseModel):\n    owner: Optional[str]\n    name: Optional[str]\n    description: Optional[str]\n    last_activity: Optional[str]\n    license_url: Optional[str]\n    website_url: Optional[str]\n    topics: Optional[List[str]]\n    stars_amount: Optional[int]\n    readme_url: Optional[str]\n    contribution_url: Optional[str]\n    contributors_amount: Optional[int]\n    forks_amount: Optional[int]\n    branches_amount: Optional[int]\n    issues_amount: Optional[int]\n    prs_amount: Optional[int]\n    commits_amount: Optional[int]\n    issues_graphic: Optional[IssueGraphicResponse]\n    prs_graphic: Optional[PullRequestGraphicResponse]\n    top_ten_labels_graphic: Optional[List[LabelResponse]]\n\n    @classmethod\n    def from_repository_response(\n        cls, owner: str, name: str, repository_data\n    ) -> \"RepositoryResponse\":\n        labels_data = repository_data.get(\"labels\", None).get(\"nodes\", None)\n        sorted_labels_data = sorted(\n            labels_data, key=lambda x: x[\"issues\"][\"totalCount\"], reverse=True\n        )\n\n        return cls(\n            owner=owner,\n            name=name,\n            description=repository_data.get(\"description\", None),\n            last_activity=repository_data.get(\"defaultBranchRef\", None)\n            .get(\"target\", None)\n            .get(\"history\", None)\n            .get(\"edges\", None)[0]\n            .get(\"node\", None)\n            .get(\"committedDate\", None),\n            license_url=None,\n            website_url=repository_data.get(\"homepageUrl\", None),\n            topics=[\n                topic[\"node\"][\"topic\"][\"name\"]\n                for topic in repository_data.get(\"repositoryTopics\", None).get(\n                    \"edges\", []\n                )\n            ],\n            stars_amount=repository_data.get(\"stargazers\", None).get(\n                \"totalCount\", None\n            ),\n            readme_url=None,\n            contribution_url=None,\n            contributors_amount=repository_data.get(\"mentionableUsers\", None).get(\n                \"totalCount\", None\n            ),\n            forks_amount=repository_data.get(\"forks\", None).get(\"totalCount\", None),\n            branches_amount=repository_data.get(\"refs\", None).get(\"totalCount\", None),\n            issues_amount=repository_data.get(\"issues\", None).get(\"totalCount\", None),\n            prs_amount=repository_data.get(\"pullRequests\", None).get(\n                \"totalCount\", None\n            ),\n            commits_amount=repository_data.get(\"defaultBranchRef\", None)\n            .get(\"target\", None)",
    "suffix": ""
  },
  {
    "name": "Qazalbash/jaxtro:jaxtro/main.py@994",
    "canonical_solution": "    pg.generate()",
    "prompt": "from .utils import PopulationGenerator, parser\n# Copyright 2023 The Jaxtro Authors\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\ndef main():\n    args = parser.cmd_parser.parse_args()\n    configuration_dict = parser.parse_config(args.my_config)\n\n    general = configuration_dict['general']\n    models = [configuration_dict.get('mass_model', None), configuration_dict.get('spin_model', None)]\n\n    pg = PopulationGenerator(general=general, models=models)",
    "prefix": "from .utils import PopulationGenerator, parser\n# Copyright 2023 The Jaxtro Authors\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\ndef main():\n    args = parser.cmd_parser.parse_args()\n    configuration_dict = parser.parse_config(args.my_config)\n\n    general = configuration_dict['general']\n    models = [configuration_dict.get('mass_model', None), configuration_dict.get('spin_model', None)]\n\n    pg = PopulationGenerator(general=general, models=models)",
    "suffix": ""
  },
  {
    "name": "willfinnigan/RetroBioCat_2:rbc2/expansion/expanders/retrobiocat_reaction_retrieval/yaml_rxn_class.py@1600",
    "canonical_solution": "        reactions.append(rxn)",
    "prompt": "from dataclasses import dataclass, field\nfrom typing import List\nfrom rbc2.configs.data_path import path_to_data_folder\nfrom rbc2.expansion.expanders.retrobiocat_reaction_retrieval.rxn_class_interface import RetroBioCatReactions, \\\n    rxns_dict, multi_rxns_dict\nfrom rbc2.template_application.apply_template.rdchiral.initialization import rdchiralReaction\nfrom rbc2.utils.add_logger import add_logger\nimport yaml\n\n\n\ndata_folder = f'{path_to_data_folder}/retrobiocat'\n\n\n@dataclass\nclass RetroBioCatReaction:\n    name: str = 'no_name'\n    smarts: List[str] = field(default_factory=list)\n    enzymes: List[str] = field(default_factory=list)\n    cofactors: dict = field(default_factory=dict)\n    positive_tests: List[str] = field(default_factory=list)\n    negative_tests: List[str] = field(default_factory=list)\n    example_rxn_string: str = ''\n    type: str = ''\n    experimental: bool = False\n    two_step: bool = False\n    requires_absence_of_water: bool = False\n\n    product_seeds: List[str] = field(default_factory=list)\n    substrate_1_seeds: List[str] = field(default_factory=list)\n    substrate_2_seeds: List[str] = field(default_factory=list)\n\n    # reaction can have no smarts, but reference multiple other reactions which can be run in sequence\n    steps: List[List[str]] = field(default_factory=list)\n\n\ndef load_reactions_from_yaml() -> List[RetroBioCatReaction]:\n    with open(f\"{data_folder}/rxns_yaml.yaml\") as stream:\n        data_loaded = yaml.safe_load(stream)\n\n    reactions = []\n    for reaction_name, values in data_loaded.items():\n        values['name'] = reaction_name\n        rxn = RetroBioCatReaction(**values)",
    "prefix": "from dataclasses import dataclass, field\nfrom typing import List\nfrom rbc2.configs.data_path import path_to_data_folder\nfrom rbc2.expansion.expanders.retrobiocat_reaction_retrieval.rxn_class_interface import RetroBioCatReactions, \\\n    rxns_dict, multi_rxns_dict\nfrom rbc2.template_application.apply_template.rdchiral.initialization import rdchiralReaction\nfrom rbc2.utils.add_logger import add_logger\nimport yaml\n\n\n\ndata_folder = f'{path_to_data_folder}/retrobiocat'\n\n\n@dataclass\nclass RetroBioCatReaction:\n    name: str = 'no_name'\n    smarts: List[str] = field(default_factory=list)\n    enzymes: List[str] = field(default_factory=list)\n    cofactors: dict = field(default_factory=dict)\n    positive_tests: List[str] = field(default_factory=list)\n    negative_tests: List[str] = field(default_factory=list)\n    example_rxn_string: str = ''\n    type: str = ''\n    experimental: bool = False\n    two_step: bool = False\n    requires_absence_of_water: bool = False\n\n    product_seeds: List[str] = field(default_factory=list)\n    substrate_1_seeds: List[str] = field(default_factory=list)\n    substrate_2_seeds: List[str] = field(default_factory=list)\n\n    # reaction can have no smarts, but reference multiple other reactions which can be run in sequence\n    steps: List[List[str]] = field(default_factory=list)\n\n\ndef load_reactions_from_yaml() -> List[RetroBioCatReaction]:\n    with open(f\"{data_folder}/rxns_yaml.yaml\") as stream:\n        data_loaded = yaml.safe_load(stream)\n\n    reactions = []\n    for reaction_name, values in data_loaded.items():\n        values['name'] = reaction_name\n        rxn = RetroBioCatReaction(**values)",
    "suffix": ""
  },
  {
    "name": "DomingoJoseCab/AutoTube:utils/information/extractor.py@820",
    "canonical_solution": "    return (product,full_description)\r",
    "prompt": "import json\r\nfrom playwright.sync_api import sync_playwright\r\nfrom selectolax.parser import HTMLParser\r\nfrom utils.information.name_extractor import get_brand, get_model\r\nfrom utils.information.description_extractor import get_description, get_video_url\r\nfrom utils.information.video_extractor import download_video\r\n# ==============================================================================\r\n# AutoTube Script\r\n# Creado por: Domingo Caballero\r\n# Canal de YouTube: https://www.youtube.com/@emprendedomingo?=sub_confirmation=1\r\n# Lista de Correo: https://emprendecondomingo.substack.com/\r\n# ==============================================================================\r\n\r\n\r\n\r\ndef get_html(asin):\r\n    pw = sync_playwright().start()\r\n    browser = pw.chromium.launch()\r\n    page = browser.new_page()\r\n    \r\n    url = f\"https://www.amazon.es/dp/{asin}\"\r\n    \r\n    page.goto(url)\r\n    html = HTMLParser(page.content())\r\n\r\n    browser.close()\r\n    pw.stop()\r\n\r\n    return html\r\n\r\ndef extractor(asin, video_path):\r\n    with open('../AutoTube/argss.json', 'r', encoding='utf-8') as archivo:\r\n        datos = json.load(archivo)\r\n\r\n    print(\"Getting html...\")\r\n    html = get_html(asin)\r\n    print(\"Getting brand and model...\")\r\n    product = get_brand(html) + get_model(html)\r\n    print(\"Getting description...\")\r\n    data, description = get_description(html, asin)\r\n\r\n    full_description = product + ', ' + description\r\n\r\n    print(\"Downloading video...\")\r\n    try:\r\n        download_video(data['video_url'], video_path)\r\n    except:\r\n        print(\"Retrying to download video...\")\r\n        url = get_video_url(html, asin)\r\n        download_video(url, video_path)\r\n\r",
    "prefix": "import json\r\nfrom playwright.sync_api import sync_playwright\r\nfrom selectolax.parser import HTMLParser\r\nfrom utils.information.name_extractor import get_brand, get_model\r\nfrom utils.information.description_extractor import get_description, get_video_url\r\nfrom utils.information.video_extractor import download_video\r\n# ==============================================================================\r\n# AutoTube Script\r\n# Creado por: Domingo Caballero\r\n# Canal de YouTube: https://www.youtube.com/@emprendedomingo?=sub_confirmation=1\r\n# Lista de Correo: https://emprendecondomingo.substack.com/\r\n# ==============================================================================\r\n\r\n\r\n\r\ndef get_html(asin):\r\n    pw = sync_playwright().start()\r\n    browser = pw.chromium.launch()\r\n    page = browser.new_page()\r\n    \r\n    url = f\"https://www.amazon.es/dp/{asin}\"\r\n    \r\n    page.goto(url)\r\n    html = HTMLParser(page.content())\r\n\r\n    browser.close()\r\n    pw.stop()\r\n\r\n    return html\r\n\r\ndef extractor(asin, video_path):\r\n    with open('../AutoTube/argss.json', 'r', encoding='utf-8') as archivo:\r\n        datos = json.load(archivo)\r\n\r\n    print(\"Getting html...\")\r\n    html = get_html(asin)\r\n    print(\"Getting brand and model...\")\r\n    product = get_brand(html) + get_model(html)\r\n    print(\"Getting description...\")\r\n    data, description = get_description(html, asin)\r\n\r\n    full_description = product + ', ' + description\r\n\r\n    print(\"Downloading video...\")\r\n    try:\r\n        download_video(data['video_url'], video_path)\r\n    except:\r\n        print(\"Retrying to download video...\")\r\n        url = get_video_url(html, asin)\r\n        download_video(url, video_path)\r\n\r",
    "suffix": ""
  },
  {
    "name": "gregorybchris/typogenetics:tests/test_search.py@1094",
    "canonical_solution": "        rng = np.random.default_rng(42)",
    "prompt": "import numpy as np\nfrom typogenetics.search import Editor, EditType\nfrom typogenetics.typogenetics import Strand\n\n\n\nclass TestSearch:\n    def test_select_edit_type(self) -> None:\n        rng = np.random.default_rng(42)\n        assert Editor.select_edit_type(rng) == EditType.INSERT\n\n    def test_mutate(self) -> None:\n        rng = np.random.default_rng(42)\n        strand = Strand.from_str(\"ACGT\")\n        new_strand = Editor.mutate(strand, rng)\n        assert new_strand == Strand.from_str(\"TCGT\")\n\n    def test_insert(self) -> None:\n        rng = np.random.default_rng(42)\n        strand = Strand.from_str(\"ACGT\")\n        new_strand = Editor.insert(strand, rng)\n        assert new_strand == Strand.from_str(\"TACGT\")\n\n    def test_delete(self) -> None:",
    "prefix": "import numpy as np\nfrom typogenetics.search import Editor, EditType\nfrom typogenetics.typogenetics import Strand\n\n\n\nclass TestSearch:\n    def test_select_edit_type(self) -> None:\n        rng = np.random.default_rng(42)\n        assert Editor.select_edit_type(rng) == EditType.INSERT\n\n    def test_mutate(self) -> None:\n        rng = np.random.default_rng(42)\n        strand = Strand.from_str(\"ACGT\")\n        new_strand = Editor.mutate(strand, rng)\n        assert new_strand == Strand.from_str(\"TCGT\")\n\n    def test_insert(self) -> None:\n        rng = np.random.default_rng(42)\n        strand = Strand.from_str(\"ACGT\")\n        new_strand = Editor.insert(strand, rng)\n        assert new_strand == Strand.from_str(\"TACGT\")\n\n    def test_delete(self) -> None:",
    "suffix": ""
  },
  {
    "name": "chaoren2357/gsplatstudio:gsplatstudio/utils/camera_utils.py@1493",
    "canonical_solution": "                if not WARNED:",
    "prompt": "import numpy as np\nimport torch\nimport numpy as np\nfrom gsplatstudio.utils.general_utils import PILtoTorch\nfrom gsplatstudio.utils.graphics_utils import fov2focal\nfrom torch import nn\nfrom gsplatstudio.utils.graphics_utils import getWorld2View2, getProjectionMatrix\n#\n# Copyright (C) 2023, Inria\n# GRAPHDECO research group, https://team.inria.fr/graphdeco\n# All rights reserved.\n#\n# This software is free for non-commercial, research and evaluation use \n# under the terms of the LICENSE.md file.\n#\n# For inquiries contact  george.drettakis@inria.fr\n#\n\n\nWARNED = False\n\n\n\nclass Camera(nn.Module):\n    def __init__(self, colmap_id, R, T, FoVx, FoVy, image, gt_alpha_mask,\n                 image_name, uid,\n                 trans=np.array([0.0, 0.0, 0.0]), scale=1.0, data_device = \"cuda\"\n                 ):\n        super(Camera, self).__init__()\n\n        self.uid = uid\n        self.colmap_id = colmap_id\n        self.R = R\n        self.T = T\n        self.FoVx = FoVx\n        self.FoVy = FoVy\n        self.image_name = image_name\n\n        try:\n            self.data_device = torch.device(data_device)\n        except Exception as e:\n            print(e)\n            print(f\"[Warning] Custom device {data_device} failed, fallback to default cuda device\" )\n            self.data_device = torch.device(\"cuda\")\n\n        self.original_image = image.clamp(0.0, 1.0).to(self.data_device)\n        self.image_width = self.original_image.shape[2]\n        self.image_height = self.original_image.shape[1]\n\n        if gt_alpha_mask is not None:\n            self.original_image *= gt_alpha_mask.to(self.data_device)\n        else:\n            self.original_image *= torch.ones((1, self.image_height, self.image_width), device=self.data_device)\n\n        self.zfar = 100.0\n        self.znear = 0.01\n\n        self.trans = trans\n        self.scale = scale\n\n        self.world_view_transform = torch.tensor(getWorld2View2(R, T, trans, scale)).transpose(0, 1).cuda()\n        self.projection_matrix = getProjectionMatrix(znear=self.znear, zfar=self.zfar, fovX=self.FoVx, fovY=self.FoVy).transpose(0,1).cuda()\n        self.full_proj_transform = (self.world_view_transform.unsqueeze(0).bmm(self.projection_matrix.unsqueeze(0))).squeeze(0)\n        self.camera_center = self.world_view_transform.inverse()[3, :3]\n    def __repr__(self):\n        return f\"Camera(uid={self.uid}, colmap_id={self.colmap_id}, R={self.R}, T={self.T}, FoVx={self.FoVx}, FoVy={self.FoVy}, image_name='{self.image_name}')\"\n\n\nclass MiniCam:\n    def __init__(self, width, height, fovy, fovx, znear, zfar, world_view_transform, full_proj_transform):\n        self.image_width = width\n        self.image_height = height    \n        self.FoVy = fovy\n        self.FoVx = fovx\n        self.znear = znear\n        self.zfar = zfar\n        self.world_view_transform = world_view_transform\n        self.full_proj_transform = full_proj_transform\n        view_inv = torch.inverse(self.world_view_transform)\n        self.camera_center = view_inv[3][:3]\n\n\n\ndef loadCam(args, id, cam_info, resolution_scale):\n    orig_w, orig_h = cam_info.image.size\n\n    if args.resolution in [1, 2, 4, 8]:\n        resolution = round(orig_w/(resolution_scale * args.resolution)), round(orig_h/(resolution_scale * args.resolution))\n    else:  # should be a type that converts to float\n        if args.resolution == -1:\n            if orig_w > 1600:\n                global WARNED",
    "prefix": "import numpy as np\nimport torch\nimport numpy as np\nfrom gsplatstudio.utils.general_utils import PILtoTorch\nfrom gsplatstudio.utils.graphics_utils import fov2focal\nfrom torch import nn\nfrom gsplatstudio.utils.graphics_utils import getWorld2View2, getProjectionMatrix\n#\n# Copyright (C) 2023, Inria\n# GRAPHDECO research group, https://team.inria.fr/graphdeco\n# All rights reserved.\n#\n# This software is free for non-commercial, research and evaluation use \n# under the terms of the LICENSE.md file.\n#\n# For inquiries contact  george.drettakis@inria.fr\n#\n\n\nWARNED = False\n\n\n\nclass Camera(nn.Module):\n    def __init__(self, colmap_id, R, T, FoVx, FoVy, image, gt_alpha_mask,\n                 image_name, uid,\n                 trans=np.array([0.0, 0.0, 0.0]), scale=1.0, data_device = \"cuda\"\n                 ):\n        super(Camera, self).__init__()\n\n        self.uid = uid\n        self.colmap_id = colmap_id\n        self.R = R\n        self.T = T\n        self.FoVx = FoVx\n        self.FoVy = FoVy\n        self.image_name = image_name\n\n        try:\n            self.data_device = torch.device(data_device)\n        except Exception as e:\n            print(e)\n            print(f\"[Warning] Custom device {data_device} failed, fallback to default cuda device\" )\n            self.data_device = torch.device(\"cuda\")\n\n        self.original_image = image.clamp(0.0, 1.0).to(self.data_device)\n        self.image_width = self.original_image.shape[2]\n        self.image_height = self.original_image.shape[1]\n\n        if gt_alpha_mask is not None:\n            self.original_image *= gt_alpha_mask.to(self.data_device)\n        else:\n            self.original_image *= torch.ones((1, self.image_height, self.image_width), device=self.data_device)\n\n        self.zfar = 100.0\n        self.znear = 0.01\n\n        self.trans = trans\n        self.scale = scale\n\n        self.world_view_transform = torch.tensor(getWorld2View2(R, T, trans, scale)).transpose(0, 1).cuda()\n        self.projection_matrix = getProjectionMatrix(znear=self.znear, zfar=self.zfar, fovX=self.FoVx, fovY=self.FoVy).transpose(0,1).cuda()\n        self.full_proj_transform = (self.world_view_transform.unsqueeze(0).bmm(self.projection_matrix.unsqueeze(0))).squeeze(0)\n        self.camera_center = self.world_view_transform.inverse()[3, :3]\n    def __repr__(self):\n        return f\"Camera(uid={self.uid}, colmap_id={self.colmap_id}, R={self.R}, T={self.T}, FoVx={self.FoVx}, FoVy={self.FoVy}, image_name='{self.image_name}')\"\n\n\nclass MiniCam:\n    def __init__(self, width, height, fovy, fovx, znear, zfar, world_view_transform, full_proj_transform):\n        self.image_width = width\n        self.image_height = height    \n        self.FoVy = fovy\n        self.FoVx = fovx\n        self.znear = znear\n        self.zfar = zfar\n        self.world_view_transform = world_view_transform\n        self.full_proj_transform = full_proj_transform\n        view_inv = torch.inverse(self.world_view_transform)\n        self.camera_center = view_inv[3][:3]\n\n\n\ndef loadCam(args, id, cam_info, resolution_scale):\n    orig_w, orig_h = cam_info.image.size\n\n    if args.resolution in [1, 2, 4, 8]:\n        resolution = round(orig_w/(resolution_scale * args.resolution)), round(orig_h/(resolution_scale * args.resolution))\n    else:  # should be a type that converts to float\n        if args.resolution == -1:\n            if orig_w > 1600:\n                global WARNED",
    "suffix": ""
  },
  {
    "name": "ddjerqq/beam:src/video_stream.py@780",
    "canonical_solution": "    def is_ok(self):",
    "prompt": "import httpx\nfrom typing import AsyncIterator\nfrom dataclasses import dataclass\nfrom time import time\nfrom src.types.error import Error\nfrom src.types.user_video_list_post_response_data import UserVideoListPostResponseData\nfrom src.types.video import Video\n\n\n\nAPI_URL = f\"https://open.tiktokapis.com/v2/video/list/\"\n\n\n@dataclass\nclass Response:\n    data: UserVideoListPostResponseData\n    error: Error\n",
    "prefix": "import httpx\nfrom typing import AsyncIterator\nfrom dataclasses import dataclass\nfrom time import time\nfrom src.types.error import Error\nfrom src.types.user_video_list_post_response_data import UserVideoListPostResponseData\nfrom src.types.video import Video\n\n\n\nAPI_URL = f\"https://open.tiktokapis.com/v2/video/list/\"\n\n\n@dataclass\nclass Response:\n    data: UserVideoListPostResponseData\n    error: Error\n",
    "suffix": ""
  },
  {
    "name": "onestepai/api_rag:src/api_rag/gpt_api.py@1228",
    "canonical_solution": "            self.final_response_prompt = \"\u8bf7\u5e2e\u6211\u76f4\u63a5\u56de\u590d\u4e0b\u9762\u7684\u63d0\u95ee\uff1a{}\uff0c\u4f60\u9700\u8981\u4ece\u4ee5\u4e0b\u6211\u4eec\u5185\u90e8\u4fe1\u606f\u89e3\u6790\uff0c\" \\",
    "prompt": "import json\nimport openai\nimport logging\nfrom urllib import response\nfrom src.config.ServiceApiConfig import ServiceApiConfig\nfrom src.utils.apis import apis_info\n\n\n\nclass GPTChatBot():\n    def __init__(self):\n        if 'zh_cn' == ServiceApiConfig.prompt_language:",
    "prefix": "import json\nimport openai\nimport logging\nfrom urllib import response\nfrom src.config.ServiceApiConfig import ServiceApiConfig\nfrom src.utils.apis import apis_info\n\n\n\nclass GPTChatBot():\n    def __init__(self):\n        if 'zh_cn' == ServiceApiConfig.prompt_language:",
    "suffix": ""
  },
  {
    "name": "T0kyoB0y/PotatoWidgets:PotatoWidgets/Widget/_Common/_BasicProps.py@1365",
    "canonical_solution": "        elif isinstance(param, (list)):",
    "prompt": "from ...__Import import *\nfrom ...Variable import Listener, Poll, Variable\n\n\nclass BasicProps(Gtk.Widget):\n    def __init__(\n        self,\n        halign,\n        valign,\n        hexpand,\n        vexpand,\n        active,\n        visible,\n        classname,\n        # tooltip,\n        css,\n        size=[10, 10],\n    ):\n        Gtk.Widget.__init__(self)\n        self.set_hexpand(True if hexpand else False)\n        self.set_vexpand(True if vexpand else False)\n        self.set_halign(halign)\n        self.set_valign(valign)\n        self.set_visible(visible)\n        self.set_sensitive(active) if active is not None else None\n        self.set_classname(classname)\n        self.__clasif_size(size)\n        self.apply_css(css) if css else None\n\n        for key, value in locals().items():\n            callback = {\n                \"halign\": self.set_halign,\n                \"valign\": self.set_valign,\n                \"hexpand\": self.set_hexpand,\n                \"vexpand\": self.set_vexpand,\n                \"active\": self.set_sensitive,\n                \"visible\": self.set_visible,\n                \"size\": self.set_size,\n                \"classname\": self.set_classname,\n            }.get(key)\n\n            self.bind(value, callback) if callback else None\n\n    def set_size(self, size):\n        self.__clasif_size(size)\n\n    def set_halign(self, param):\n        super().set_halign(self.__clasif_align(str(param)))\n\n    def set_valign(self, param):\n        super().set_valign(self.__clasif_align(str(param)))\n\n    def __clasif_size(self, size):\n        if isinstance(size, int):\n            self.set_size_request(size, size)\n        elif isinstance(size, list):\n            if len(size) == 2:\n                self.set_size_request(size[0], size[1])\n            elif len(size) == 1:\n                self.set_size_request(size[0], size[0])\n\n    def __clasif_align(self, param):\n        dict = {\n            \"fill\": Gtk.Align.FILL,\n            \"start\": Gtk.Align.START,\n            \"end\": Gtk.Align.END,\n            \"center\": Gtk.Align.CENTER,\n            \"baseline\": Gtk.Align.BASELINE,\n        }\n        return dict.get(param.lower(), Gtk.Align.FILL)\n\n    def set_classname(self, param):\n        if isinstance(param, (str)):\n            context = self.get_style_context()\n            [context.add_class(i) for i in param.split(\" \") if i != \" \"]",
    "prefix": "from ...__Import import *\nfrom ...Variable import Listener, Poll, Variable\n\n\nclass BasicProps(Gtk.Widget):\n    def __init__(\n        self,\n        halign,\n        valign,\n        hexpand,\n        vexpand,\n        active,\n        visible,\n        classname,\n        # tooltip,\n        css,\n        size=[10, 10],\n    ):\n        Gtk.Widget.__init__(self)\n        self.set_hexpand(True if hexpand else False)\n        self.set_vexpand(True if vexpand else False)\n        self.set_halign(halign)\n        self.set_valign(valign)\n        self.set_visible(visible)\n        self.set_sensitive(active) if active is not None else None\n        self.set_classname(classname)\n        self.__clasif_size(size)\n        self.apply_css(css) if css else None\n\n        for key, value in locals().items():\n            callback = {\n                \"halign\": self.set_halign,\n                \"valign\": self.set_valign,\n                \"hexpand\": self.set_hexpand,\n                \"vexpand\": self.set_vexpand,\n                \"active\": self.set_sensitive,\n                \"visible\": self.set_visible,\n                \"size\": self.set_size,\n                \"classname\": self.set_classname,\n            }.get(key)\n\n            self.bind(value, callback) if callback else None\n\n    def set_size(self, size):\n        self.__clasif_size(size)\n\n    def set_halign(self, param):\n        super().set_halign(self.__clasif_align(str(param)))\n\n    def set_valign(self, param):\n        super().set_valign(self.__clasif_align(str(param)))\n\n    def __clasif_size(self, size):\n        if isinstance(size, int):\n            self.set_size_request(size, size)\n        elif isinstance(size, list):\n            if len(size) == 2:\n                self.set_size_request(size[0], size[1])\n            elif len(size) == 1:\n                self.set_size_request(size[0], size[0])\n\n    def __clasif_align(self, param):\n        dict = {\n            \"fill\": Gtk.Align.FILL,\n            \"start\": Gtk.Align.START,\n            \"end\": Gtk.Align.END,\n            \"center\": Gtk.Align.CENTER,\n            \"baseline\": Gtk.Align.BASELINE,\n        }\n        return dict.get(param.lower(), Gtk.Align.FILL)\n\n    def set_classname(self, param):\n        if isinstance(param, (str)):\n            context = self.get_style_context()\n            [context.add_class(i) for i in param.split(\" \") if i != \" \"]",
    "suffix": ""
  },
  {
    "name": "0xn0ne/sensitive-helper:sensitive-helper.py@1105",
    "canonical_solution": "    flags_int = 0",
    "prompt": "import base64\nimport binascii\nimport csv\nimport json\nimport pathlib\nimport re\nimport time\nimport pandas\nimport tqdm\n    import argparse\nfrom typing import Any, AnyStr, Dict, List, Union\nfrom utils import compress, configurator, office, process\n#!/bin/python3\n# _*_ coding:utf-8 _*_\n#\n# sensitive-helper.py\n# \u672c\u5730\u6587\u4ef6\u654f\u611f\u4fe1\u606f\u641c\u7d22\u5de5\u5177\n\n\n\n\n\ndef log_run_times(func):\n    def wrapper(*args, **kwargs):\n        s_time = time.time()\n        ret = func(*args, **kwargs)\n        total_time = time.time() - s_time\n        if total_time <= 1:\n            return ret\n        with open('run_times.log', 'a') as _f:\n            _f.write('total time(s): {}, args: {}\\n'.format(time.time() - s_time, args[0][:127]))\n        return ret\n\n    return wrapper\n\n\ndef string_to_reg_flags(flags: str):",
    "prefix": "import base64\nimport binascii\nimport csv\nimport json\nimport pathlib\nimport re\nimport time\nimport pandas\nimport tqdm\n    import argparse\nfrom typing import Any, AnyStr, Dict, List, Union\nfrom utils import compress, configurator, office, process\n#!/bin/python3\n# _*_ coding:utf-8 _*_\n#\n# sensitive-helper.py\n# \u672c\u5730\u6587\u4ef6\u654f\u611f\u4fe1\u606f\u641c\u7d22\u5de5\u5177\n\n\n\n\n\ndef log_run_times(func):\n    def wrapper(*args, **kwargs):\n        s_time = time.time()\n        ret = func(*args, **kwargs)\n        total_time = time.time() - s_time\n        if total_time <= 1:\n            return ret\n        with open('run_times.log', 'a') as _f:\n            _f.write('total time(s): {}, args: {}\\n'.format(time.time() - s_time, args[0][:127]))\n        return ret\n\n    return wrapper\n\n\ndef string_to_reg_flags(flags: str):",
    "suffix": ""
  },
  {
    "name": "Zerohertz/Streamlit-Quant:lib/visual.py@731",
    "canonical_solution": "    for idx, window in enumerate([5, 20, 60, 120]):",
    "prompt": "import plotly.graph_objs as go\nimport streamlit as st\nimport zerohertzLib as zz\nfrom plotly.subplots import make_subplots\nfrom lib.layout import _main, _transaction\nfrom lib.util import _color\n\n\n\ndef candle():\n    data, xdata = st.session_state[\"cache\"][\"data\"], st.session_state[\"cache\"][\"xdata\"]\n    st.session_state[\"cache\"][\"candle\"] = go.Candlestick(\n        x=xdata,\n        open=data.Open,\n        high=data.High,\n        low=data.Low,\n        close=data.Close,\n        increasing={\"line\": {\"color\": \"red\"}},\n        decreasing={\"line\": {\"color\": \"blue\"}},\n        name=st.session_state[\"cache\"][\"name\"],\n    )\n    st.session_state[\"logger\"].info(\n        f\"\"\"[Plot] Candle Chart: {st.session_state[\"cache\"][\"name\"]} ({st.session_state[\"cache\"][\"symbol\"]})\"\"\"\n    )\n\n\ndef moving_average():\n    xdata = st.session_state[\"cache\"][\"xdata\"]\n    st.session_state[\"cache\"][\"ma\"] = []\n    colors = _color(4, 0.5, \"Set1\")",
    "prefix": "import plotly.graph_objs as go\nimport streamlit as st\nimport zerohertzLib as zz\nfrom plotly.subplots import make_subplots\nfrom lib.layout import _main, _transaction\nfrom lib.util import _color\n\n\n\ndef candle():\n    data, xdata = st.session_state[\"cache\"][\"data\"], st.session_state[\"cache\"][\"xdata\"]\n    st.session_state[\"cache\"][\"candle\"] = go.Candlestick(\n        x=xdata,\n        open=data.Open,\n        high=data.High,\n        low=data.Low,\n        close=data.Close,\n        increasing={\"line\": {\"color\": \"red\"}},\n        decreasing={\"line\": {\"color\": \"blue\"}},\n        name=st.session_state[\"cache\"][\"name\"],\n    )\n    st.session_state[\"logger\"].info(\n        f\"\"\"[Plot] Candle Chart: {st.session_state[\"cache\"][\"name\"]} ({st.session_state[\"cache\"][\"symbol\"]})\"\"\"\n    )\n\n\ndef moving_average():\n    xdata = st.session_state[\"cache\"][\"xdata\"]\n    st.session_state[\"cache\"][\"ma\"] = []\n    colors = _color(4, 0.5, \"Set1\")",
    "suffix": ""
  },
  {
    "name": "acman/py_june:categories/tests.py@739",
    "canonical_solution": "        self.assertEqual(response.status_code, 200)",
    "prompt": "from django.test import TestCase\nfrom django.urls import reverse\nfrom categories.models import Category, MainCategory\nfrom core.tests import TestDataMixin\n\n\n\nclass CategoryModelTest(TestCase):\n    def setUp(self):\n        self.main_category = MainCategory.objects.create(title=\"MainCategory\")\n        self.category = Category(\n            title=\"\u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0456\u044f\",\n            description=\"Test description\",\n            main_category=self.main_category,\n        )\n        # It is required to create slug\n        self.category.save()\n\n    def test_category_model(self):\n        self.assertEqual(self.category.title, \"\u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0456\u044f\")\n        self.assertEqual(self.category.slug, \"ukrayinska-kategoriia\")\n        self.assertEqual(self.category.description, \"Test description\")\n        self.assertEqual(self.category.main_category.title, \"MainCategory\")\n\n    def test_category_model_str(self):\n        self.assertEqual(str(self.category), \"\u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0456\u044f\")\n\n\nclass CategoryListViewTest(TestCase):\n    def setUp(self):\n        # Create some MainCategory instances for testing\n        self.category1 = MainCategory.objects.create(title=\"Category 1\")\n        self.category2 = MainCategory.objects.create(title=\"Category 2\")\n\n    def test_category_list(self):\n        response = self.client.get(reverse(\"categories:list\"))\n\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"Category 1\")\n\n\nclass CategoryDetailViewTest(TestDataMixin, TestCase):\n    def test_category_detail(self):\n        response = self.client.get(\n            reverse(\"categories:detail\", kwargs={\"category_slug\": self.category.slug})\n        )\n",
    "prefix": "from django.test import TestCase\nfrom django.urls import reverse\nfrom categories.models import Category, MainCategory\nfrom core.tests import TestDataMixin\n\n\n\nclass CategoryModelTest(TestCase):\n    def setUp(self):\n        self.main_category = MainCategory.objects.create(title=\"MainCategory\")\n        self.category = Category(\n            title=\"\u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0456\u044f\",\n            description=\"Test description\",\n            main_category=self.main_category,\n        )\n        # It is required to create slug\n        self.category.save()\n\n    def test_category_model(self):\n        self.assertEqual(self.category.title, \"\u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0456\u044f\")\n        self.assertEqual(self.category.slug, \"ukrayinska-kategoriia\")\n        self.assertEqual(self.category.description, \"Test description\")\n        self.assertEqual(self.category.main_category.title, \"MainCategory\")\n\n    def test_category_model_str(self):\n        self.assertEqual(str(self.category), \"\u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0456\u044f\")\n\n\nclass CategoryListViewTest(TestCase):\n    def setUp(self):\n        # Create some MainCategory instances for testing\n        self.category1 = MainCategory.objects.create(title=\"Category 1\")\n        self.category2 = MainCategory.objects.create(title=\"Category 2\")\n\n    def test_category_list(self):\n        response = self.client.get(reverse(\"categories:list\"))\n\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"Category 1\")\n\n\nclass CategoryDetailViewTest(TestDataMixin, TestCase):\n    def test_category_detail(self):\n        response = self.client.get(\n            reverse(\"categories:detail\", kwargs={\"category_slug\": self.category.slug})\n        )\n",
    "suffix": ""
  },
  {
    "name": "pkariz/grin-explorer:backend/api/signals/receivers.py@1482",
    "canonical_solution": "    dispatch_uid=\"on_block_create\",",
    "prompt": "from django.db.models.signals import post_save\nfrom django.dispatch import receiver\nfrom backend.api.models import Block, Reorg\nfrom backend.api.helpers import fix_outputs_and_inputs_from_reorg\nimport logging\n\n\n\nlogger = logging.getLogger(__name__)\n\n\n@receiver(\n    post_save,\n    sender=Block,",
    "prefix": "from django.db.models.signals import post_save\nfrom django.dispatch import receiver\nfrom backend.api.models import Block, Reorg\nfrom backend.api.helpers import fix_outputs_and_inputs_from_reorg\nimport logging\n\n\n\nlogger = logging.getLogger(__name__)\n\n\n@receiver(\n    post_save,\n    sender=Block,",
    "suffix": ""
  },
  {
    "name": "CodeWithEmad/num2fa:num2fa/cli.py@650",
    "canonical_solution": "@click.option(\"--ordinal\", \"-o\", is_flag=True, help=\"Convert to ordinal from\")",
    "prompt": "import click\nfrom num2fa.__about__ import __version__\nfrom num2fa.converters.number_converter import numbers\nfrom num2fa.converters.word_converter import ordinal_words, words\n\n\n\nclass NumberType(click.ParamType):\n    name = \"number\"\n\n    def convert(self, value, param, ctx):\n        try:\n            if value.startswith(\"-\"):\n                return -self.parse_number(value[1:])\n            return self.parse_number(value)\n        except ValueError:\n            self.fail(f\"{value} is not a valid number\", param, ctx)\n\n    def parse_number(self, value):\n        try:\n            return int(value)\n        except ValueError:\n            return float(value)\n\n\n@click.command(\n    help=\"This command takes a NUMBER as an argument and converts it to Farsi numbers, words or ordinal form depending on the options used:\"\n)\n@click.version_option(version=__version__)\n@click.argument(\"number\", type=NumberType())\n@click.option(\"--word\", \"-w\", is_flag=True, help=\"Convert to Farsi words\")",
    "prefix": "import click\nfrom num2fa.__about__ import __version__\nfrom num2fa.converters.number_converter import numbers\nfrom num2fa.converters.word_converter import ordinal_words, words\n\n\n\nclass NumberType(click.ParamType):\n    name = \"number\"\n\n    def convert(self, value, param, ctx):\n        try:\n            if value.startswith(\"-\"):\n                return -self.parse_number(value[1:])\n            return self.parse_number(value)\n        except ValueError:\n            self.fail(f\"{value} is not a valid number\", param, ctx)\n\n    def parse_number(self, value):\n        try:\n            return int(value)\n        except ValueError:\n            return float(value)\n\n\n@click.command(\n    help=\"This command takes a NUMBER as an argument and converts it to Farsi numbers, words or ordinal form depending on the options used:\"\n)\n@click.version_option(version=__version__)\n@click.argument(\"number\", type=NumberType())\n@click.option(\"--word\", \"-w\", is_flag=True, help=\"Convert to Farsi words\")",
    "suffix": ""
  },
  {
    "name": "the-seeds/cardinal:src/cardinal/core/vectorstore/milvus.py@1583",
    "canonical_solution": "            self.store = Collection(name=self.name)",
    "prompt": "import base64\nimport os\nimport pickle\nfrom collections import defaultdict\nfrom typing import Any, List, Optional, Tuple, TypeVar\nfrom pydantic import BaseModel\nfrom typing_extensions import Self\nfrom ..schema import Condition, Operator, VectorStore\nfrom ..utils.import_utils import is_pymilvus_availble\n    from pymilvus import Collection, CollectionSchema, DataType, FieldSchema, connections, utility\n    from pymilvus.orm.types import infer_dtype_bydata\n\n\n\n\nif is_pymilvus_availble():\n\n\nK = List[float]\nV = TypeVar(\"V\", bound=BaseModel)\n\n\nclass MilvusCondition(Condition):\n    def __init__(self, key: str, value: Any, op: Operator) -> None:\n        self._key = key\n        _ops = [\"==\", \"!=\", \">\", \">=\", \"<\", \"<=\", \"in\", \"not in\", \"&&\", \"||\"]\n        if op < len(_ops):\n            self._op = _ops[op]\n        else:\n            raise NotImplementedError\n\n        if isinstance(value, list) and op in [Operator.In, Operator.Notin]:\n            self._value = value\n        elif isinstance(value, str) and op in [Operator.And, Operator.Or]:\n            self._value = value\n        elif isinstance(value, str):\n            self._value = '\"{}\"'.format(value)\n        elif isinstance(value, (int, float)):\n            self._value = value\n        else:\n            raise ValueError(\"Unsupported operation {} for value {}\".format(self._op, value))\n\n    def to_filter(self) -> str:\n        return \" \".join((self._key, self._op, self._value))\n\n\nclass Milvus(VectorStore[V]):\n    def __init__(self, name: str) -> None:\n        self.name = name\n        self.store: Optional[Collection] = None\n        self._fields: List[str] = []\n        self._alias = \"default\"\n        self._batch_size = 1000\n        self._primary_field = \"pk\"\n        self._embedding_field = \"embedding\"\n        self._data_field = \"data\"\n        self._index_params = {\"metric_type\": \"L2\", \"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 1024}}\n        self._search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n\n    def _check_connection(self) -> None:\n        if not connections.has_connection(self._alias):\n            connections.connect(\n                alias=self._alias, uri=os.environ.get(\"MILVUS_URI\"), token=os.environ.get(\"MILVUS_TOKEN\")\n            )\n\n    def _create_collection(self, embedding: K, example: V) -> None:\n        fields = [\n            FieldSchema(name=self._primary_field, dtype=DataType.INT64, is_primary=True, auto_id=True),\n            FieldSchema(name=self._embedding_field, dtype=DataType.FLOAT_VECTOR, dim=len(embedding)),\n            FieldSchema(name=self._data_field, dtype=DataType.VARCHAR, max_length=2048),\n        ]\n\n        for key, value in example.model_dump().items():\n            dtype = infer_dtype_bydata(value)\n            if dtype == DataType.UNKNOWN or dtype == DataType.NONE:\n                raise ValueError(\"Unrecognized datatype for {}\".format(key))\n            elif dtype == DataType.VARCHAR:\n                fields.append(FieldSchema(name=key, dtype=DataType.VARCHAR, max_length=512))\n            else:\n                fields.append(FieldSchema(name=key, dtype=dtype))\n\n        self.store = Collection(name=self.name, schema=CollectionSchema(fields=fields))\n\n    def _create_index(self) -> None:\n        if len(self.store.indexes) == 0:\n            self.store.create_index(field_name=self._embedding_field, index_params=self._index_params)\n\n    def _extract_fields(self) -> None:\n        if len(self._fields) == 0:\n            for field in self.store.schema.fields:\n                if isinstance(field, FieldSchema) and field.name != self._primary_field:\n                    self._fields.append(field.name)\n\n    def _init(self, embedding: Optional[K] = None, example: Optional[V] = None) -> None:\n        # build connection\n        self._check_connection()\n\n        # check existing store\n        if utility.has_collection(self.name):",
    "prefix": "import base64\nimport os\nimport pickle\nfrom collections import defaultdict\nfrom typing import Any, List, Optional, Tuple, TypeVar\nfrom pydantic import BaseModel\nfrom typing_extensions import Self\nfrom ..schema import Condition, Operator, VectorStore\nfrom ..utils.import_utils import is_pymilvus_availble\n    from pymilvus import Collection, CollectionSchema, DataType, FieldSchema, connections, utility\n    from pymilvus.orm.types import infer_dtype_bydata\n\n\n\n\nif is_pymilvus_availble():\n\n\nK = List[float]\nV = TypeVar(\"V\", bound=BaseModel)\n\n\nclass MilvusCondition(Condition):\n    def __init__(self, key: str, value: Any, op: Operator) -> None:\n        self._key = key\n        _ops = [\"==\", \"!=\", \">\", \">=\", \"<\", \"<=\", \"in\", \"not in\", \"&&\", \"||\"]\n        if op < len(_ops):\n            self._op = _ops[op]\n        else:\n            raise NotImplementedError\n\n        if isinstance(value, list) and op in [Operator.In, Operator.Notin]:\n            self._value = value\n        elif isinstance(value, str) and op in [Operator.And, Operator.Or]:\n            self._value = value\n        elif isinstance(value, str):\n            self._value = '\"{}\"'.format(value)\n        elif isinstance(value, (int, float)):\n            self._value = value\n        else:\n            raise ValueError(\"Unsupported operation {} for value {}\".format(self._op, value))\n\n    def to_filter(self) -> str:\n        return \" \".join((self._key, self._op, self._value))\n\n\nclass Milvus(VectorStore[V]):\n    def __init__(self, name: str) -> None:\n        self.name = name\n        self.store: Optional[Collection] = None\n        self._fields: List[str] = []\n        self._alias = \"default\"\n        self._batch_size = 1000\n        self._primary_field = \"pk\"\n        self._embedding_field = \"embedding\"\n        self._data_field = \"data\"\n        self._index_params = {\"metric_type\": \"L2\", \"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 1024}}\n        self._search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n\n    def _check_connection(self) -> None:\n        if not connections.has_connection(self._alias):\n            connections.connect(\n                alias=self._alias, uri=os.environ.get(\"MILVUS_URI\"), token=os.environ.get(\"MILVUS_TOKEN\")\n            )\n\n    def _create_collection(self, embedding: K, example: V) -> None:\n        fields = [\n            FieldSchema(name=self._primary_field, dtype=DataType.INT64, is_primary=True, auto_id=True),\n            FieldSchema(name=self._embedding_field, dtype=DataType.FLOAT_VECTOR, dim=len(embedding)),\n            FieldSchema(name=self._data_field, dtype=DataType.VARCHAR, max_length=2048),\n        ]\n\n        for key, value in example.model_dump().items():\n            dtype = infer_dtype_bydata(value)\n            if dtype == DataType.UNKNOWN or dtype == DataType.NONE:\n                raise ValueError(\"Unrecognized datatype for {}\".format(key))\n            elif dtype == DataType.VARCHAR:\n                fields.append(FieldSchema(name=key, dtype=DataType.VARCHAR, max_length=512))\n            else:\n                fields.append(FieldSchema(name=key, dtype=dtype))\n\n        self.store = Collection(name=self.name, schema=CollectionSchema(fields=fields))\n\n    def _create_index(self) -> None:\n        if len(self.store.indexes) == 0:\n            self.store.create_index(field_name=self._embedding_field, index_params=self._index_params)\n\n    def _extract_fields(self) -> None:\n        if len(self._fields) == 0:\n            for field in self.store.schema.fields:\n                if isinstance(field, FieldSchema) and field.name != self._primary_field:\n                    self._fields.append(field.name)\n\n    def _init(self, embedding: Optional[K] = None, example: Optional[V] = None) -> None:\n        # build connection\n        self._check_connection()\n\n        # check existing store\n        if utility.has_collection(self.name):",
    "suffix": ""
  },
  {
    "name": "datrocity/pond:tests/test_conventions.py@739",
    "canonical_solution": "def test_data_location():",
    "prompt": "from pond.conventions import (\n    METADATA_DIRNAME,\n    MANIFEST_FILENAME,\n    version_data_location,\n    version_manifest_location,\n    version_uri,\n    urijoinpath,\n)\nfrom pond.version_name import SimpleVersionName\n\n\ndef test_urijoinpath():\n    joined = urijoinpath('a', 'b/', 'c/')\n    expected = 'a/b/c'\n    assert joined == expected\n\n",
    "prefix": "from pond.conventions import (\n    METADATA_DIRNAME,\n    MANIFEST_FILENAME,\n    version_data_location,\n    version_manifest_location,\n    version_uri,\n    urijoinpath,\n)\nfrom pond.version_name import SimpleVersionName\n\n\ndef test_urijoinpath():\n    joined = urijoinpath('a', 'b/', 'c/')\n    expected = 'a/b/c'\n    assert joined == expected\n\n",
    "suffix": ""
  },
  {
    "name": "Zitronenjoghurt/Colonaut:src/save_state/global_state.py@1444",
    "canonical_solution": "            validator.validate_int(retrieved_data['run_count'], \"run_count\", 0)",
    "prompt": "import src.utils.validator as validator\nfrom src.constants.config import Config\nfrom src.save_state.save_state import SaveState\nfrom src.utils.file_operations import construct_path\n\nCONFIG = Config.get_instance()\n\nclass GlobalState(SaveState):\n    _instance = None\n    SAVE_FILE_PATH = CONFIG.GLOBAL_STATE_FILE_PATH + \"global_state.\" + CONFIG.SAVE_FILE_MODE\n    DEFAULT_SAVE_FILE_PATH = construct_path(\"src/data/default_global_state.json\")\n\n    def __init__(\n            self, \n            run_count: int, \n            finished_intro: bool,\n            finished_tutorial: bool\n        ) -> None:\n        if self._instance is not None:\n            raise RuntimeError(\"Tried to initialize two instances of GlobalState.\")\n        self.run_count = run_count\n        self.finished_intro = finished_intro\n        self.finished_tutorial = finished_tutorial\n        super().__init__()\n\n    @staticmethod\n    def get_instance() -> 'GlobalState':\n        if GlobalState._instance is None:\n            GlobalState._instance = GlobalState.load()\n        return GlobalState._instance\n    \n    @staticmethod\n    def reset_instance() -> None:\n        GlobalState._instance = None\n\n    @staticmethod\n    def load() -> 'GlobalState':\n        data = GlobalState.load_data()\n\n        retrieved_data = {\n            \"run_count\": data.get(\"run_count\", None),\n            \"finished_intro\": data.get(\"finished_intro\", None),\n            \"finished_tutorial\": data.get(\"finished_tutorial\", None),\n        }\n\n        for key, value in retrieved_data.items():\n            if value is None:\n                raise RuntimeError(f\"An error occured while loading the global state save file: missing {key} data.\")\n        try:",
    "prefix": "import src.utils.validator as validator\nfrom src.constants.config import Config\nfrom src.save_state.save_state import SaveState\nfrom src.utils.file_operations import construct_path\n\nCONFIG = Config.get_instance()\n\nclass GlobalState(SaveState):\n    _instance = None\n    SAVE_FILE_PATH = CONFIG.GLOBAL_STATE_FILE_PATH + \"global_state.\" + CONFIG.SAVE_FILE_MODE\n    DEFAULT_SAVE_FILE_PATH = construct_path(\"src/data/default_global_state.json\")\n\n    def __init__(\n            self, \n            run_count: int, \n            finished_intro: bool,\n            finished_tutorial: bool\n        ) -> None:\n        if self._instance is not None:\n            raise RuntimeError(\"Tried to initialize two instances of GlobalState.\")\n        self.run_count = run_count\n        self.finished_intro = finished_intro\n        self.finished_tutorial = finished_tutorial\n        super().__init__()\n\n    @staticmethod\n    def get_instance() -> 'GlobalState':\n        if GlobalState._instance is None:\n            GlobalState._instance = GlobalState.load()\n        return GlobalState._instance\n    \n    @staticmethod\n    def reset_instance() -> None:\n        GlobalState._instance = None\n\n    @staticmethod\n    def load() -> 'GlobalState':\n        data = GlobalState.load_data()\n\n        retrieved_data = {\n            \"run_count\": data.get(\"run_count\", None),\n            \"finished_intro\": data.get(\"finished_intro\", None),\n            \"finished_tutorial\": data.get(\"finished_tutorial\", None),\n        }\n\n        for key, value in retrieved_data.items():\n            if value is None:\n                raise RuntimeError(f\"An error occured while loading the global state save file: missing {key} data.\")\n        try:",
    "suffix": ""
  },
  {
    "name": "xIMRANx/secret_postcard:app/handlers/user/file.py@1466",
    "canonical_solution": "        description=caption,",
    "prompt": "from aiogram import Router, Bot, F\nfrom aiogram.types import Message\nfrom app.db.functions import User\nfrom app.db.functions import Card\nfrom app.keyboards.inline import get_approve_keyboard\nfrom app.config import Config\n\n\nrouter = Router()\n\n\n@router.message(F.content_type.in_({\"photo\", \"video\", \"animation\"}))\nasync def get_postcard(message: Message, bot: Bot, config: Config):\n    if await Card.check_exists(message.from_user.id):\n        await message.answer(\"\u0412\u044b \u0443\u0436\u0435 \u043e\u0442\u043f\u0440\u0430\u0432\u0438\u043b\u0438 \u0441\u0432\u043e\u044e \u043e\u0442\u043a\u0440\u044b\u0442\u043a\u0443!\")\n        return\n\n    postcard_type = message.content_type\n    if message.photo is not None:\n        file_id = message.photo[-1].file_id\n    elif message.video is not None:\n        file_id = message.video.file_id\n    elif message.animation is not None:\n        file_id = message.animation.file_id\n    else:\n        file_id = None\n\n    user_id = message.from_user.id\n    chat_id = config.settings.chat_id\n    if not await User.is_registered(user_id):\n        await message.answer(\n            \"<b>\u0423\u0443\u043f\u0441, \u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u0430 \u043e\u0448\u0438\u0431\u043a\u0430</b>\"\n            \"\\n\\n\"\n            \"\u041f\u043e\u0445\u043e\u0436\u0435, \u0447\u0442\u043e \u0432\u044b \u043d\u0435 \u0437\u0430\u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u0432 \u0441\u0438\u0441\u0442\u0435\u043c\u0435.\"\n            \"\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0439\u0442\u0435 \u043f\u0435\u0440\u0435\u0437\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u0431\u043e\u0442\u0430 \u043a\u043e\u043c\u0430\u043d\u0434\u043e\u0439 /start\"\n        )\n\n    caption = message.caption.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n    text = (f'<b>\u041d\u043e\u0432\u0430\u044f \u043e\u0442\u043a\u0440\u044b\u0442\u043a\u0430</b> \u043e\u0442 <a href=\"tg://user?id={user_id}\">{message.from_user.full_name}</a>\\n\\n'\n            f'\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435: {str(caption)}')\n\n    match postcard_type:\n        case \"photo\":\n            await bot.send_photo(\n                chat_id,\n                file_id,\n                caption=text,\n                parse_mode=\"HTML\",\n                reply_markup=get_approve_keyboard(user_id),\n            )\n        case \"video\":\n            await bot.send_video(\n                chat_id,\n                file_id,\n                caption=text,\n                parse_mode=\"HTML\",\n                reply_markup=get_approve_keyboard(user_id),\n            )\n        case \"animation\":\n            await bot.send_animation(\n                chat_id,\n                file_id,\n                caption=text,\n                parse_mode=\"HTML\",\n                reply_markup=get_approve_keyboard(user_id),\n            )\n\n    await Card.create_card(\n        file_id=file_id,",
    "prefix": "from aiogram import Router, Bot, F\nfrom aiogram.types import Message\nfrom app.db.functions import User\nfrom app.db.functions import Card\nfrom app.keyboards.inline import get_approve_keyboard\nfrom app.config import Config\n\n\nrouter = Router()\n\n\n@router.message(F.content_type.in_({\"photo\", \"video\", \"animation\"}))\nasync def get_postcard(message: Message, bot: Bot, config: Config):\n    if await Card.check_exists(message.from_user.id):\n        await message.answer(\"\u0412\u044b \u0443\u0436\u0435 \u043e\u0442\u043f\u0440\u0430\u0432\u0438\u043b\u0438 \u0441\u0432\u043e\u044e \u043e\u0442\u043a\u0440\u044b\u0442\u043a\u0443!\")\n        return\n\n    postcard_type = message.content_type\n    if message.photo is not None:\n        file_id = message.photo[-1].file_id\n    elif message.video is not None:\n        file_id = message.video.file_id\n    elif message.animation is not None:\n        file_id = message.animation.file_id\n    else:\n        file_id = None\n\n    user_id = message.from_user.id\n    chat_id = config.settings.chat_id\n    if not await User.is_registered(user_id):\n        await message.answer(\n            \"<b>\u0423\u0443\u043f\u0441, \u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u0430 \u043e\u0448\u0438\u0431\u043a\u0430</b>\"\n            \"\\n\\n\"\n            \"\u041f\u043e\u0445\u043e\u0436\u0435, \u0447\u0442\u043e \u0432\u044b \u043d\u0435 \u0437\u0430\u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u0432 \u0441\u0438\u0441\u0442\u0435\u043c\u0435.\"\n            \"\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0439\u0442\u0435 \u043f\u0435\u0440\u0435\u0437\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u0431\u043e\u0442\u0430 \u043a\u043e\u043c\u0430\u043d\u0434\u043e\u0439 /start\"\n        )\n\n    caption = message.caption.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n    text = (f'<b>\u041d\u043e\u0432\u0430\u044f \u043e\u0442\u043a\u0440\u044b\u0442\u043a\u0430</b> \u043e\u0442 <a href=\"tg://user?id={user_id}\">{message.from_user.full_name}</a>\\n\\n'\n            f'\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435: {str(caption)}')\n\n    match postcard_type:\n        case \"photo\":\n            await bot.send_photo(\n                chat_id,\n                file_id,\n                caption=text,\n                parse_mode=\"HTML\",\n                reply_markup=get_approve_keyboard(user_id),\n            )\n        case \"video\":\n            await bot.send_video(\n                chat_id,\n                file_id,\n                caption=text,\n                parse_mode=\"HTML\",\n                reply_markup=get_approve_keyboard(user_id),\n            )\n        case \"animation\":\n            await bot.send_animation(\n                chat_id,\n                file_id,\n                caption=text,\n                parse_mode=\"HTML\",\n                reply_markup=get_approve_keyboard(user_id),\n            )\n\n    await Card.create_card(\n        file_id=file_id,",
    "suffix": ""
  },
  {
    "name": "akkoaya/ArticleSpider:ArticleSpider/spiders/cnblog.py@1021",
    "canonical_solution": "    def parse(self, response):",
    "prompt": "import scrapy\nimport datetime\nimport re\nfrom scrapy.http import Request\nfrom urllib import parse\nfrom ..items import CnblogItem\nfrom ..utils.common import get_md5\nfrom scrapy.loader import ItemLoader\nfrom scrapy_redis.spiders import RedisSpider\n\n\nclass CnblogSpider(scrapy.Spider):\n    name = \"cnblog\"\n    allowed_domains = [\"www.cnblogs.com\"]\n    start_urls = [\"https://www.cnblogs.com/sitehome/p/1\"]\n    # redis_key = 'cnblog:start_urls'\n\n    next_url = \"https://www.cnblogs.com/sitehome/p/{0}\"\n    # headers = {\n    #     \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n    # }\n",
    "prefix": "import scrapy\nimport datetime\nimport re\nfrom scrapy.http import Request\nfrom urllib import parse\nfrom ..items import CnblogItem\nfrom ..utils.common import get_md5\nfrom scrapy.loader import ItemLoader\nfrom scrapy_redis.spiders import RedisSpider\n\n\nclass CnblogSpider(scrapy.Spider):\n    name = \"cnblog\"\n    allowed_domains = [\"www.cnblogs.com\"]\n    start_urls = [\"https://www.cnblogs.com/sitehome/p/1\"]\n    # redis_key = 'cnblog:start_urls'\n\n    next_url = \"https://www.cnblogs.com/sitehome/p/{0}\"\n    # headers = {\n    #     \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n    # }\n",
    "suffix": ""
  },
  {
    "name": "Asa-Nisi-Masa/christmas-tree:christmas_tree/rpi/server.py@894",
    "canonical_solution": "    if request.method == \"POST\":",
    "prompt": "import board\nimport neopixel\nfrom flask import Flask, jsonify, render_template, request\nfrom christmas_tree.common import effect_registry\nfrom christmas_tree.common.settings import PATH_SAVE, TOTAL_LEDS\nfrom christmas_tree.common.utils import load_coordinates\nfrom christmas_tree.rpi.light_show import LightShow\n\n\npixels = neopixel.NeoPixel(board.D21, TOTAL_LEDS, auto_write=False, pixel_order=neopixel.RGB, brightness=0.5)\n\npixels.fill((0, 0, 0))\npixels.show()\n\ncoords = load_coordinates(PATH_SAVE)\n\nlight_show = LightShow(pixels, coords)\n\napp = Flask(__name__)\n\n\n@app.route(\"/\")\ndef home():\n    return render_template(\"index.html\", effects=effect_registry.get_display_names())\n\n\n@app.route(\"/ping\")\ndef ping():\n    return \"pong\"\n\n\n@app.route(\"/process\", methods=[\"POST\"])\ndef process_data():\n    selected_effects = request.json.get(\"selected_effects\", [])\n    light_show.show_effects(selected_effects)\n\n    return jsonify({\"message\": \"Received!\"})\n\n\n@app.route(\"/fire/<int:led_id>\", methods=[\"POST\", \"DELETE\"])\ndef control(led_id):",
    "prefix": "import board\nimport neopixel\nfrom flask import Flask, jsonify, render_template, request\nfrom christmas_tree.common import effect_registry\nfrom christmas_tree.common.settings import PATH_SAVE, TOTAL_LEDS\nfrom christmas_tree.common.utils import load_coordinates\nfrom christmas_tree.rpi.light_show import LightShow\n\n\npixels = neopixel.NeoPixel(board.D21, TOTAL_LEDS, auto_write=False, pixel_order=neopixel.RGB, brightness=0.5)\n\npixels.fill((0, 0, 0))\npixels.show()\n\ncoords = load_coordinates(PATH_SAVE)\n\nlight_show = LightShow(pixels, coords)\n\napp = Flask(__name__)\n\n\n@app.route(\"/\")\ndef home():\n    return render_template(\"index.html\", effects=effect_registry.get_display_names())\n\n\n@app.route(\"/ping\")\ndef ping():\n    return \"pong\"\n\n\n@app.route(\"/process\", methods=[\"POST\"])\ndef process_data():\n    selected_effects = request.json.get(\"selected_effects\", [])\n    light_show.show_effects(selected_effects)\n\n    return jsonify({\"message\": \"Received!\"})\n\n\n@app.route(\"/fire/<int:led_id>\", methods=[\"POST\", \"DELETE\"])\ndef control(led_id):",
    "suffix": ""
  },
  {
    "name": "lchen1019/Image_Cropper:ISAT/widgets/polygon.py@1206",
    "canonical_solution": "        self.setPath(self.nohover)",
    "prompt": "from PyQt5 import QtCore, QtWidgets, QtGui\nfrom ISAT.annotation import Object\nfrom ISAT.configs import STATUSMode, CLICKMode, DRAWMode, CONTOURMode\nimport typing\n# -*- coding: utf-8 -*-\n# @Author  : LG\n\n\n\nclass PromptPoint(QtWidgets.QGraphicsPathItem):\n    def __init__(self, pos, type=0):\n        super(PromptPoint, self).__init__()\n        self.color = QtGui.QColor('#0000FF') if type==0 else QtGui.QColor('#00FF00')\n        self.color.setAlpha(255)\n        self.painterpath = QtGui.QPainterPath()\n        self.painterpath.addEllipse(\n            QtCore.QRectF(-1, -1, 2, 2))\n        self.setPath(self.painterpath)\n        self.setBrush(self.color)\n        self.setPen(QtGui.QPen(self.color, 3))\n        self.setZValue(1e5)\n\n        self.setPos(pos)\n\n\nclass Vertex(QtWidgets.QGraphicsPathItem):\n    def __init__(self, polygon, color, nohover_size=2):\n        super(Vertex, self).__init__()\n        self.polygon = polygon\n        self.color = color\n        self.color.setAlpha(255)\n        self.nohover_size = nohover_size\n        self.hover_size = self.nohover_size + 2\n        self.line_width = 0\n\n        self.nohover = QtGui.QPainterPath()\n        self.nohover.addEllipse(QtCore.QRectF(-self.nohover_size//2, -self.nohover_size//2, self.nohover_size, self.nohover_size))\n        self.hover = QtGui.QPainterPath()\n        self.hover.addRect(QtCore.QRectF(-self.nohover_size//2, -self.nohover_size//2, self.nohover_size, self.nohover_size))\n\n        self.setPath(self.nohover)\n        self.setBrush(self.color)\n        self.setPen(QtGui.QPen(self.color, self.line_width))\n        self.setFlag(QtWidgets.QGraphicsItem.GraphicsItemFlag.ItemIsSelectable, True)\n        self.setFlag(QtWidgets.QGraphicsItem.GraphicsItemFlag.ItemIsMovable, True)\n        self.setFlag(QtWidgets.QGraphicsItem.GraphicsItemFlag.ItemSendsGeometryChanges, True)\n        self.setAcceptHoverEvents(True)\n        self.setZValue(1e5)\n\n    def setColor(self, color):\n        self.color = QtGui.QColor(color)\n        self.color.setAlpha(255)\n        self.setPen(QtGui.QPen(self.color, self.line_width))\n        self.setBrush(self.color)\n\n    def itemChange(self, change: 'QtWidgets.QGraphicsItem.GraphicsItemChange', value: typing.Any):\n        if change == QtWidgets.QGraphicsItem.GraphicsItemChange.ItemSelectedHasChanged:\n            self.scene().mainwindow.actionDelete.setEnabled(self.isSelected())\n            if self.isSelected():\n                selected_color = QtGui.QColor('#00A0FF')\n                self.setBrush(selected_color)\n            else:\n                self.setBrush(self.color)\n\n        if change == QtWidgets.QGraphicsItem.GraphicsItemChange.ItemPositionChange and self.isEnabled():\n            # \u9650\u5236\u9876\u70b9\u79fb\u52a8\u5230\u56fe\u5916\n            if value.x() < 0:\n                value.setX(0)\n            if value.x() > self.scene().width()-1:\n                value.setX(self.scene().width()-1)\n            if value.y() < 0:\n                value.setY(0)\n            if value.y() > self.scene().height()-1:\n                value.setY(self.scene().height()-1)\n            index = self.polygon.vertexs.index(self)\n            self.polygon.movePoint(index, value)\n\n        return super(Vertex, self).itemChange(change, value)\n    \n    def hoverEnterEvent(self, event: 'QGraphicsSceneHoverEvent'):\n        if self.scene().mode == STATUSMode.CREATE: # CREATE\n            self.setCursor(QtGui.QCursor(QtCore.Qt.CursorShape.CrossCursor))\n        else: # EDIT, VIEW\n            self.setCursor(QtGui.QCursor(QtCore.Qt.CursorShape.OpenHandCursor))\n            if not self.isSelected():\n                self.setBrush(QtGui.QColor(255, 255, 255, 255))\n            self.setPath(self.hover)\n        super(Vertex, self).hoverEnterEvent(event)\n\n    def hoverLeaveEvent(self, event: 'QGraphicsSceneHoverEvent'):\n        if not self.isSelected():\n            self.setBrush(self.color)",
    "prefix": "from PyQt5 import QtCore, QtWidgets, QtGui\nfrom ISAT.annotation import Object\nfrom ISAT.configs import STATUSMode, CLICKMode, DRAWMode, CONTOURMode\nimport typing\n# -*- coding: utf-8 -*-\n# @Author  : LG\n\n\n\nclass PromptPoint(QtWidgets.QGraphicsPathItem):\n    def __init__(self, pos, type=0):\n        super(PromptPoint, self).__init__()\n        self.color = QtGui.QColor('#0000FF') if type==0 else QtGui.QColor('#00FF00')\n        self.color.setAlpha(255)\n        self.painterpath = QtGui.QPainterPath()\n        self.painterpath.addEllipse(\n            QtCore.QRectF(-1, -1, 2, 2))\n        self.setPath(self.painterpath)\n        self.setBrush(self.color)\n        self.setPen(QtGui.QPen(self.color, 3))\n        self.setZValue(1e5)\n\n        self.setPos(pos)\n\n\nclass Vertex(QtWidgets.QGraphicsPathItem):\n    def __init__(self, polygon, color, nohover_size=2):\n        super(Vertex, self).__init__()\n        self.polygon = polygon\n        self.color = color\n        self.color.setAlpha(255)\n        self.nohover_size = nohover_size\n        self.hover_size = self.nohover_size + 2\n        self.line_width = 0\n\n        self.nohover = QtGui.QPainterPath()\n        self.nohover.addEllipse(QtCore.QRectF(-self.nohover_size//2, -self.nohover_size//2, self.nohover_size, self.nohover_size))\n        self.hover = QtGui.QPainterPath()\n        self.hover.addRect(QtCore.QRectF(-self.nohover_size//2, -self.nohover_size//2, self.nohover_size, self.nohover_size))\n\n        self.setPath(self.nohover)\n        self.setBrush(self.color)\n        self.setPen(QtGui.QPen(self.color, self.line_width))\n        self.setFlag(QtWidgets.QGraphicsItem.GraphicsItemFlag.ItemIsSelectable, True)\n        self.setFlag(QtWidgets.QGraphicsItem.GraphicsItemFlag.ItemIsMovable, True)\n        self.setFlag(QtWidgets.QGraphicsItem.GraphicsItemFlag.ItemSendsGeometryChanges, True)\n        self.setAcceptHoverEvents(True)\n        self.setZValue(1e5)\n\n    def setColor(self, color):\n        self.color = QtGui.QColor(color)\n        self.color.setAlpha(255)\n        self.setPen(QtGui.QPen(self.color, self.line_width))\n        self.setBrush(self.color)\n\n    def itemChange(self, change: 'QtWidgets.QGraphicsItem.GraphicsItemChange', value: typing.Any):\n        if change == QtWidgets.QGraphicsItem.GraphicsItemChange.ItemSelectedHasChanged:\n            self.scene().mainwindow.actionDelete.setEnabled(self.isSelected())\n            if self.isSelected():\n                selected_color = QtGui.QColor('#00A0FF')\n                self.setBrush(selected_color)\n            else:\n                self.setBrush(self.color)\n\n        if change == QtWidgets.QGraphicsItem.GraphicsItemChange.ItemPositionChange and self.isEnabled():\n            # \u9650\u5236\u9876\u70b9\u79fb\u52a8\u5230\u56fe\u5916\n            if value.x() < 0:\n                value.setX(0)\n            if value.x() > self.scene().width()-1:\n                value.setX(self.scene().width()-1)\n            if value.y() < 0:\n                value.setY(0)\n            if value.y() > self.scene().height()-1:\n                value.setY(self.scene().height()-1)\n            index = self.polygon.vertexs.index(self)\n            self.polygon.movePoint(index, value)\n\n        return super(Vertex, self).itemChange(change, value)\n    \n    def hoverEnterEvent(self, event: 'QGraphicsSceneHoverEvent'):\n        if self.scene().mode == STATUSMode.CREATE: # CREATE\n            self.setCursor(QtGui.QCursor(QtCore.Qt.CursorShape.CrossCursor))\n        else: # EDIT, VIEW\n            self.setCursor(QtGui.QCursor(QtCore.Qt.CursorShape.OpenHandCursor))\n            if not self.isSelected():\n                self.setBrush(QtGui.QColor(255, 255, 255, 255))\n            self.setPath(self.hover)\n        super(Vertex, self).hoverEnterEvent(event)\n\n    def hoverLeaveEvent(self, event: 'QGraphicsSceneHoverEvent'):\n        if not self.isSelected():\n            self.setBrush(self.color)",
    "suffix": ""
  },
  {
    "name": "Hatins/DEOE:validation.py@1572",
    "canonical_solution": "            file_path = os.path.join(config.img_save_path, filename)",
    "prompt": "import os\nimport shutil\nimport torch\nimport hydra\nimport pytorch_lightning as pl\nfrom pathlib import Path\nfrom torch.backends import cuda, cudnn\nfrom callbacks.custom import get_ckpt_callback, get_viz_callback\nfrom pytorch_lightning.loggers import WandbLogger\nfrom omegaconf import DictConfig, OmegaConf\nfrom pytorch_lightning.loggers import CSVLogger\nfrom pytorch_lightning.callbacks import ModelSummary\nfrom config.modifier import dynamically_modify_train_config\nfrom modules.utils.fetch import fetch_data_module, fetch_model_module\n\nos.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"\nos.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\nos.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\ncuda.matmul.allow_tf32 = True\ncudnn.allow_tf32 = True\ntorch.multiprocessing.set_sharing_strategy('file_system')\n\n\n\n@hydra.main(config_path='config', config_name='val', version_base='1.2')\ndef main(config: DictConfig):\n    dynamically_modify_train_config(config)\n    # Just to check whether config can be resolved\n    OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n\n    print('------ Configuration ------')\n    print(OmegaConf.to_yaml(config))\n    print('---------------------------')\n\n    # ---------------------\n    # GPU options\n    # ---------------------\n    gpus = config.hardware.gpus\n    assert isinstance(gpus, int), 'no more than 1 GPU supported'\n    gpus = [gpus]\n\n    # ---------------------\n    # Data\n    # ---------------------\n    data_module = fetch_data_module(config=config)\n\n    # ---------------------\n    # Logging and Checkpoints\n    logger = WandbLogger(project=config.wandb.project_name,name='testing', group=config.wandb.group_name)\n    ckpt_path = Path(config.checkpoint)\n\n    # ---------------------\n    # Model\n    # ---------------------\n\n    module = fetch_model_module(config=config)\n    module = module.load_from_checkpoint(str(ckpt_path), **{'full_config': config})\n\n    # ---------------------\n    # Callbacks and Misc\n    # ---------------------\n    callbacks = list()\n    viz_callback = get_viz_callback(config=config)\n    callbacks.append(viz_callback)\n    callbacks.append(ModelSummary(max_depth=2))\n\n    if os.path.exists(config.img_save_path) and os.path.isdir(config.img_save_path):\n        for filename in os.listdir(config.img_save_path):",
    "prefix": "import os\nimport shutil\nimport torch\nimport hydra\nimport pytorch_lightning as pl\nfrom pathlib import Path\nfrom torch.backends import cuda, cudnn\nfrom callbacks.custom import get_ckpt_callback, get_viz_callback\nfrom pytorch_lightning.loggers import WandbLogger\nfrom omegaconf import DictConfig, OmegaConf\nfrom pytorch_lightning.loggers import CSVLogger\nfrom pytorch_lightning.callbacks import ModelSummary\nfrom config.modifier import dynamically_modify_train_config\nfrom modules.utils.fetch import fetch_data_module, fetch_model_module\n\nos.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"\nos.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\nos.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\ncuda.matmul.allow_tf32 = True\ncudnn.allow_tf32 = True\ntorch.multiprocessing.set_sharing_strategy('file_system')\n\n\n\n@hydra.main(config_path='config', config_name='val', version_base='1.2')\ndef main(config: DictConfig):\n    dynamically_modify_train_config(config)\n    # Just to check whether config can be resolved\n    OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n\n    print('------ Configuration ------')\n    print(OmegaConf.to_yaml(config))\n    print('---------------------------')\n\n    # ---------------------\n    # GPU options\n    # ---------------------\n    gpus = config.hardware.gpus\n    assert isinstance(gpus, int), 'no more than 1 GPU supported'\n    gpus = [gpus]\n\n    # ---------------------\n    # Data\n    # ---------------------\n    data_module = fetch_data_module(config=config)\n\n    # ---------------------\n    # Logging and Checkpoints\n    logger = WandbLogger(project=config.wandb.project_name,name='testing', group=config.wandb.group_name)\n    ckpt_path = Path(config.checkpoint)\n\n    # ---------------------\n    # Model\n    # ---------------------\n\n    module = fetch_model_module(config=config)\n    module = module.load_from_checkpoint(str(ckpt_path), **{'full_config': config})\n\n    # ---------------------\n    # Callbacks and Misc\n    # ---------------------\n    callbacks = list()\n    viz_callback = get_viz_callback(config=config)\n    callbacks.append(viz_callback)\n    callbacks.append(ModelSummary(max_depth=2))\n\n    if os.path.exists(config.img_save_path) and os.path.isdir(config.img_save_path):\n        for filename in os.listdir(config.img_save_path):",
    "suffix": ""
  },
  {
    "name": "yeyingdege/ctr-din-pytorch:din/model.py@1231",
    "canonical_solution": "        print(f\"DinAttentionLayer trainable parameters: {att_params}\")",
    "prompt": "import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom .embedding import EmbeddingLayer\nfrom .fc import FCLayer\nfrom .attention import DinAttentionLayer\n\n\n\nclass DeepInterestNetwork(nn.Module):\n    def __init__(self, n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_DIM=[162,200,80,2]):\n        super(DeepInterestNetwork, self).__init__()\n        self.embedding_dim = EMBEDDING_DIM\n        self.hid_dim = HIDDEN_DIM\n\n        # embeddings\n        self.uid_embeddings = EmbeddingLayer(n_uid, self.embedding_dim)\n        self.mid_embeddings = EmbeddingLayer(n_mid, self.embedding_dim)\n        self.cat_embeddings = EmbeddingLayer(n_cat, self.embedding_dim)\n\n        self.attn = DinAttentionLayer(embedding_dim=self.embedding_dim*2)\n        mlp_input_dim = self.embedding_dim * 9\n        self.mlp = nn.Sequential(\n            FCLayer(mlp_input_dim, hidden_size=self.hid_dim[1], bias=True, batch_norm=True, activation='dice'),\n            FCLayer(self.hid_dim[1], hidden_size=self.hid_dim[2], bias=True, activation='dice'),\n            FCLayer(self.hid_dim[2], hidden_size=self.hid_dim[3], bias=False, activation='none')\n        )\n        uid_params = sum(p.numel() for p in self.uid_embeddings.parameters() if p.requires_grad)\n        print(f\"uid_embeddings trainable parameters: {uid_params}\")\n        mid_params = sum(p.numel() for p in self.mid_embeddings.parameters() if p.requires_grad)\n        print(f\"mid_embeddings trainable parameters: {mid_params}\")\n        cat_params = sum(p.numel() for p in self.cat_embeddings.parameters() if p.requires_grad)\n        print(f\"cat_embeddings trainable parameters: {cat_params}\")\n        att_params = sum(p.numel() for p in self.attn.parameters() if p.requires_grad)",
    "prefix": "import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom .embedding import EmbeddingLayer\nfrom .fc import FCLayer\nfrom .attention import DinAttentionLayer\n\n\n\nclass DeepInterestNetwork(nn.Module):\n    def __init__(self, n_uid, n_mid, n_cat, EMBEDDING_DIM, HIDDEN_DIM=[162,200,80,2]):\n        super(DeepInterestNetwork, self).__init__()\n        self.embedding_dim = EMBEDDING_DIM\n        self.hid_dim = HIDDEN_DIM\n\n        # embeddings\n        self.uid_embeddings = EmbeddingLayer(n_uid, self.embedding_dim)\n        self.mid_embeddings = EmbeddingLayer(n_mid, self.embedding_dim)\n        self.cat_embeddings = EmbeddingLayer(n_cat, self.embedding_dim)\n\n        self.attn = DinAttentionLayer(embedding_dim=self.embedding_dim*2)\n        mlp_input_dim = self.embedding_dim * 9\n        self.mlp = nn.Sequential(\n            FCLayer(mlp_input_dim, hidden_size=self.hid_dim[1], bias=True, batch_norm=True, activation='dice'),\n            FCLayer(self.hid_dim[1], hidden_size=self.hid_dim[2], bias=True, activation='dice'),\n            FCLayer(self.hid_dim[2], hidden_size=self.hid_dim[3], bias=False, activation='none')\n        )\n        uid_params = sum(p.numel() for p in self.uid_embeddings.parameters() if p.requires_grad)\n        print(f\"uid_embeddings trainable parameters: {uid_params}\")\n        mid_params = sum(p.numel() for p in self.mid_embeddings.parameters() if p.requires_grad)\n        print(f\"mid_embeddings trainable parameters: {mid_params}\")\n        cat_params = sum(p.numel() for p in self.cat_embeddings.parameters() if p.requires_grad)\n        print(f\"cat_embeddings trainable parameters: {cat_params}\")\n        att_params = sum(p.numel() for p in self.attn.parameters() if p.requires_grad)",
    "suffix": ""
  },
  {
    "name": "iamlooper/VIC-TG-Bot:app/core/client/filters.py@841",
    "canonical_solution": "    start_str = message.text.split(maxsplit=1)[0]",
    "prompt": "from pyrogram import filters as _filters\nfrom pyrogram.types import Message\nfrom app import Config\nfrom app.core.client.conversation import Conversation\n\n\n# Overall BOT filters\n\nconvo_filter = _filters.create(\n    lambda _, __, message: (message.chat.id in Conversation.CONVO_DICT.keys())\n    and (not message.reactions)\n)\n\n\ndef cmd_check(message: Message, trigger: str) -> bool:",
    "prefix": "from pyrogram import filters as _filters\nfrom pyrogram.types import Message\nfrom app import Config\nfrom app.core.client.conversation import Conversation\n\n\n# Overall BOT filters\n\nconvo_filter = _filters.create(\n    lambda _, __, message: (message.chat.id in Conversation.CONVO_DICT.keys())\n    and (not message.reactions)\n)\n\n\ndef cmd_check(message: Message, trigger: str) -> bool:",
    "suffix": ""
  },
  {
    "name": "Enthusiasm23/primkit:src/primkit/tests/PytestTrigger.py@805",
    "canonical_solution": "    unittest.main()",
    "prompt": "import unittest\nimport logging\nfrom ..utils import get_chrome_driver\nfrom ..config import TEST_URL, DEFAULT_HEADLESS\n\nlogger = logging.getLogger(__name__)\n\n\nclass TestSeleniumEnvironment(unittest.TestCase):\n    def setUp(self):\n        try:\n            # Attempt to obtain Chrome driver\n            self.driver = get_chrome_driver(headless=DEFAULT_HEADLESS)\n        except Exception as e:\n            self.driver = None\n            logger.error(f\"Selenium environment setup failed: {e}\")\n\n    def tearDown(self):\n        # Cleanup code executed at the end of testing\n        if self.driver:\n            self.driver.quit()\n\n    def test_env_setup(self):\n        # Directly check if the self.driver has been successfully created to determine if the Selenium environment has been successfully set up\n        if self.driver is None:\n            raise Exception(\"Selenium environment setup failed.\")\n\n    def test_page_load(self, test_url=TEST_URL):\n        if self.driver is None:\n            raise Exception(\"Selenium environment setup failed.\")\n        # Perform actual testing, such as accessing web pages\n        self.driver.get(test_url)\n        if \"Example\" not in self.driver.title:\n            raise Exception(\"Website title does not contain 'Example'.\")\n\n\nif __name__ == '__main__':",
    "prefix": "import unittest\nimport logging\nfrom ..utils import get_chrome_driver\nfrom ..config import TEST_URL, DEFAULT_HEADLESS\n\nlogger = logging.getLogger(__name__)\n\n\nclass TestSeleniumEnvironment(unittest.TestCase):\n    def setUp(self):\n        try:\n            # Attempt to obtain Chrome driver\n            self.driver = get_chrome_driver(headless=DEFAULT_HEADLESS)\n        except Exception as e:\n            self.driver = None\n            logger.error(f\"Selenium environment setup failed: {e}\")\n\n    def tearDown(self):\n        # Cleanup code executed at the end of testing\n        if self.driver:\n            self.driver.quit()\n\n    def test_env_setup(self):\n        # Directly check if the self.driver has been successfully created to determine if the Selenium environment has been successfully set up\n        if self.driver is None:\n            raise Exception(\"Selenium environment setup failed.\")\n\n    def test_page_load(self, test_url=TEST_URL):\n        if self.driver is None:\n            raise Exception(\"Selenium environment setup failed.\")\n        # Perform actual testing, such as accessing web pages\n        self.driver.get(test_url)\n        if \"Example\" not in self.driver.title:\n            raise Exception(\"Website title does not contain 'Example'.\")\n\n\nif __name__ == '__main__':",
    "suffix": ""
  },
  {
    "name": "karthicksivakumarp/gui_read_csv:main.py@724",
    "canonical_solution": "root.title('Csv DataAnalyzer')  # Set the title of the Tkinter window\r",
    "prompt": "from read_from_csv import read_csv_file\r\nfrom data_analysis import analyze_data\r\nfrom report_generation import generate_report\r\nfrom tkinter import Tk\r\nfrom user_interface import gui\r\n# Import necessary modules\r\n\r\n# Initialize CSV reader instance\r\nread_csv = read_csv_file.read_csv_data()\r\n\r\n# Obtain the function/method for reading multiple CSV files\r\n# Note: \"read_mult_csv_file\" is a function or method defined in the \"read_csv_file\" module\r\nmain_read_csv = read_csv.read_mult_csv_file\r\n\r\n# Initialize data analyzer instance\r\nanalyze_data = analyze_data.analyze_csv_data()\r\n\r\n# Initialize report generator instance\r\nreport_gen = generate_report.generate_report()\r\n\r\n# Create the main Tkinter window\r\nroot = Tk()\r",
    "prefix": "from read_from_csv import read_csv_file\r\nfrom data_analysis import analyze_data\r\nfrom report_generation import generate_report\r\nfrom tkinter import Tk\r\nfrom user_interface import gui\r\n# Import necessary modules\r\n\r\n# Initialize CSV reader instance\r\nread_csv = read_csv_file.read_csv_data()\r\n\r\n# Obtain the function/method for reading multiple CSV files\r\n# Note: \"read_mult_csv_file\" is a function or method defined in the \"read_csv_file\" module\r\nmain_read_csv = read_csv.read_mult_csv_file\r\n\r\n# Initialize data analyzer instance\r\nanalyze_data = analyze_data.analyze_csv_data()\r\n\r\n# Initialize report generator instance\r\nreport_gen = generate_report.generate_report()\r\n\r\n# Create the main Tkinter window\r\nroot = Tk()\r",
    "suffix": ""
  },
  {
    "name": "davidsvy/fractal_video:src/synthetic/args.py@1096",
    "canonical_solution": "        return datetime.timedelta(seconds=int(sec))",
    "prompt": "import argparse\nimport datetime\nfrom functools import partial\nfrom .fractal.variations import var_idxs as var_available\nfrom ..utils.data import find_files\nfrom ..utils.other import size_to_str, time_to_secs\n\n\n\ndef positive(arg_name, dtype):\n    def require_positive(value):\n        number = dtype(value)\n        if number <= 0:\n            raise ValueError(f'Number {value} for arg \"{arg_name}\" must be positive.')\n        \n        return number\n\n    return require_positive\n\npos_int = partial(positive, dtype=int)\npos_float = partial(positive, dtype=float)\n\n\ndef print_args(args):\n    time_str = datetime.timedelta(seconds=int(args.time))\n    print(f'Generating dataset with args:')\n    print('#' * 50)\n    blacklist = ['time', 'paths_param']\n    print(f'\\t--time: {time_str}')\n    \n    if hasattr(args, 'paths_param'):\n        print(f'\\t--n_classes: {len(args.paths_param)}')\n    \n    for attr, value in vars(args).items():\n        if not attr in blacklist:\n            print(f'\\t--{attr}: {value}')\n            \n\ndef print_status(args, n_created, time_run, counter_size):\n    def secs_to_str(sec):",
    "prefix": "import argparse\nimport datetime\nfrom functools import partial\nfrom .fractal.variations import var_idxs as var_available\nfrom ..utils.data import find_files\nfrom ..utils.other import size_to_str, time_to_secs\n\n\n\ndef positive(arg_name, dtype):\n    def require_positive(value):\n        number = dtype(value)\n        if number <= 0:\n            raise ValueError(f'Number {value} for arg \"{arg_name}\" must be positive.')\n        \n        return number\n\n    return require_positive\n\npos_int = partial(positive, dtype=int)\npos_float = partial(positive, dtype=float)\n\n\ndef print_args(args):\n    time_str = datetime.timedelta(seconds=int(args.time))\n    print(f'Generating dataset with args:')\n    print('#' * 50)\n    blacklist = ['time', 'paths_param']\n    print(f'\\t--time: {time_str}')\n    \n    if hasattr(args, 'paths_param'):\n        print(f'\\t--n_classes: {len(args.paths_param)}')\n    \n    for attr, value in vars(args).items():\n        if not attr in blacklist:\n            print(f'\\t--{attr}: {value}')\n            \n\ndef print_status(args, n_created, time_run, counter_size):\n    def secs_to_str(sec):",
    "suffix": ""
  },
  {
    "name": "ryosoraa/APIRetrys:APIRetrys/ApiRetrys.py@862",
    "canonical_solution": "            sleep(retry_interval)",
    "prompt": "import requests\nfrom requests import Response\nfrom typing import Union\nfrom time import sleep\nfrom .MaxRetryExceptions import MaxRetryExceptions\nfrom .Logs import logger\n\nclass ApiRetry:\n    def __init__(self, show_logs: bool = False) -> None:\n        self.show_logs = show_logs\n\n    def get(\n            self, \n            url: str, \n            max_retries: int = 5, \n            retry_interval: Union[int, float] = 0.2,\n            headers = None, \n            params  = None,\n            data    = None,\n            cookies = None,\n            files   = None,\n            auth    = None,\n            timeout = None,\n            proxies = None,\n            hooks   = None,\n            stream  = None,\n            verify  = None,\n            cert    = None,\n            json    = None,\n            ) -> Response:\n        \n        for retry in range(max_retries):\n            try:\n                response = requests.get(\n                        url     = url,\n                        params  = params,\n                        data    = data,\n                        headers = headers,\n                        cookies = cookies,\n                        files   = files,\n                        auth    = auth,\n                        timeout = timeout,\n                        proxies = proxies,\n                        hooks   = hooks,\n                        stream  = stream,\n                        verify  = verify,\n                        cert    = cert,\n                        json    = json,\n                )\n\n                if self.show_logs and response.status_code == 200:\n                    logger.info(f\"method: GET\")\n                    logger.info(f\"status code: {response.status_code}\")\n                    logger.info(f\"retry to: {retry}\")\n\n                elif self.show_logs and response.status_code != 200:\n                    logger.warning(f\"method: GET\")\n                    logger.warning(f\"status code: {response.status_code}\")\n                    logger.warning(f\"retry to: {retry}\")\n\n\n                return response\n            except Exception as err:\n                logger.error(f'message: {err}')\n                logger.warning(f'try the request again')\n                \n            sleep(retry_interval)\n            retry_interval+= 0.2\n        \n        raise MaxRetryExceptions(message=f\"Failed to retrieve data after {max_retries} retries. The server may be unreachable or experiencing issues\")\n        \n\n    def post(\n            self, \n            url: str, \n            max_retries: int = 5, \n            retry_interval: Union[int, float] = 0.2,\n            headers = None, \n            data    = None,\n            json    = None,\n            params  = None,\n            cookies = None,\n            files   = None,\n            auth    = None,\n            timeout = None,\n            proxies = None,\n            hooks   = None,\n            stream  = None,\n            verify  = None,\n            cert    = None,\n            ) -> Response:\n        \n        for retry in range(max_retries):\n            try:\n                response = requests.post(\n                    url     = url,\n                    data    = data,\n                    json    = json,\n                    params  = params,\n                    headers = headers,\n                    cookies = cookies,\n                    files   = files,\n                    auth    = auth,\n                    timeout = timeout,\n                    proxies = proxies,\n                    hooks   = hooks,\n                    stream  = stream,\n                    verify  = verify,\n                    cert    = cert,\n                )\n\n                if self.show_logs and response.status_code == 200:\n                    logger.info(f\"method: POST\")\n                    logger.info(f\"status code: {response.status_code}\")\n                    logger.info(f\"retry to: {retry}\")\n\n                elif self.show_logs and response.status_code != 200:\n                    logger.warning(f\"method: POST\")\n                    logger.warning(f\"status code: {response.status_code}\")\n                    logger.warning(f\"retry to: {retry}\")\n\n\n                return response\n            except Exception as err:\n                logger.error(f'message: {err}')\n                logger.warning(f'try the request again')\n                ",
    "prefix": "import requests\nfrom requests import Response\nfrom typing import Union\nfrom time import sleep\nfrom .MaxRetryExceptions import MaxRetryExceptions\nfrom .Logs import logger\n\nclass ApiRetry:\n    def __init__(self, show_logs: bool = False) -> None:\n        self.show_logs = show_logs\n\n    def get(\n            self, \n            url: str, \n            max_retries: int = 5, \n            retry_interval: Union[int, float] = 0.2,\n            headers = None, \n            params  = None,\n            data    = None,\n            cookies = None,\n            files   = None,\n            auth    = None,\n            timeout = None,\n            proxies = None,\n            hooks   = None,\n            stream  = None,\n            verify  = None,\n            cert    = None,\n            json    = None,\n            ) -> Response:\n        \n        for retry in range(max_retries):\n            try:\n                response = requests.get(\n                        url     = url,\n                        params  = params,\n                        data    = data,\n                        headers = headers,\n                        cookies = cookies,\n                        files   = files,\n                        auth    = auth,\n                        timeout = timeout,\n                        proxies = proxies,\n                        hooks   = hooks,\n                        stream  = stream,\n                        verify  = verify,\n                        cert    = cert,\n                        json    = json,\n                )\n\n                if self.show_logs and response.status_code == 200:\n                    logger.info(f\"method: GET\")\n                    logger.info(f\"status code: {response.status_code}\")\n                    logger.info(f\"retry to: {retry}\")\n\n                elif self.show_logs and response.status_code != 200:\n                    logger.warning(f\"method: GET\")\n                    logger.warning(f\"status code: {response.status_code}\")\n                    logger.warning(f\"retry to: {retry}\")\n\n\n                return response\n            except Exception as err:\n                logger.error(f'message: {err}')\n                logger.warning(f'try the request again')\n                \n            sleep(retry_interval)\n            retry_interval+= 0.2\n        \n        raise MaxRetryExceptions(message=f\"Failed to retrieve data after {max_retries} retries. The server may be unreachable or experiencing issues\")\n        \n\n    def post(\n            self, \n            url: str, \n            max_retries: int = 5, \n            retry_interval: Union[int, float] = 0.2,\n            headers = None, \n            data    = None,\n            json    = None,\n            params  = None,\n            cookies = None,\n            files   = None,\n            auth    = None,\n            timeout = None,\n            proxies = None,\n            hooks   = None,\n            stream  = None,\n            verify  = None,\n            cert    = None,\n            ) -> Response:\n        \n        for retry in range(max_retries):\n            try:\n                response = requests.post(\n                    url     = url,\n                    data    = data,\n                    json    = json,\n                    params  = params,\n                    headers = headers,\n                    cookies = cookies,\n                    files   = files,\n                    auth    = auth,\n                    timeout = timeout,\n                    proxies = proxies,\n                    hooks   = hooks,\n                    stream  = stream,\n                    verify  = verify,\n                    cert    = cert,\n                )\n\n                if self.show_logs and response.status_code == 200:\n                    logger.info(f\"method: POST\")\n                    logger.info(f\"status code: {response.status_code}\")\n                    logger.info(f\"retry to: {retry}\")\n\n                elif self.show_logs and response.status_code != 200:\n                    logger.warning(f\"method: POST\")\n                    logger.warning(f\"status code: {response.status_code}\")\n                    logger.warning(f\"retry to: {retry}\")\n\n\n                return response\n            except Exception as err:\n                logger.error(f'message: {err}')\n                logger.warning(f'try the request again')\n                ",
    "suffix": ""
  },
  {
    "name": "OpenBrickProtocolFoundation/client:main.py@961",
    "canonical_solution": "                    x, y = position.x, position.y",
    "prompt": "import pygame\nfrom tetrion import Event\nfrom tetrion import EventType\nfrom tetrion import Key\nfrom tetrion import Tetrion\n\n\n\ndef main() -> None:\n    frame = 0\n\n    with Tetrion() as tetrion:\n        pygame.init()\n\n        RECT_SIZE = 30\n        size = (RECT_SIZE * tetrion.width, (RECT_SIZE + 2) * tetrion.height)\n        screen = pygame.display.set_mode(size)\n\n        COLORS = [(0, 0, 0),\n                  (0, 240, 240),\n                  (0, 0, 240),\n                  (240, 160, 0),\n                  (240, 240, 0),\n                  (0, 240, 0),\n                  (160, 0, 240),\n                  (240, 0, 0)]\n\n        done = False\n\n        clock = pygame.time.Clock()\n\n        while not done:\n            for event in pygame.event.get():\n                if event.type == pygame.QUIT:\n                    done = True\n                elif event.type == pygame.KEYDOWN:\n                    if event.key == pygame.K_ESCAPE:\n                        done = True\n                    elif event.key == pygame.K_a:\n                        tetrion.enqueue_event(Event(key=Key.LEFT, type=EventType.PRESSED, frame=frame))\n                    elif event.key == pygame.K_d:\n                        tetrion.enqueue_event(Event(key=Key.RIGHT, type=EventType.PRESSED, frame=frame))\n                    elif event.key == pygame.K_SPACE:\n                        tetrion.enqueue_event(Event(key=Key.DROP, type=EventType.PRESSED, frame=frame))\n\n            tetrion.simulate_up_until(frame)\n            screen.fill((100, 100, 100))\n\n            matrix = tetrion.matrix()\n            for y, row in enumerate(matrix.rows):\n                for x, mino in enumerate(row):\n                    pygame.draw.rect(screen, COLORS[mino.value],\n                                     pygame.Rect(x * RECT_SIZE, y * RECT_SIZE, RECT_SIZE, RECT_SIZE))\n\n            active_tetromino = tetrion.try_get_active_tetromino()\n            if active_tetromino is not None:\n                for position in active_tetromino.mino_positions:",
    "prefix": "import pygame\nfrom tetrion import Event\nfrom tetrion import EventType\nfrom tetrion import Key\nfrom tetrion import Tetrion\n\n\n\ndef main() -> None:\n    frame = 0\n\n    with Tetrion() as tetrion:\n        pygame.init()\n\n        RECT_SIZE = 30\n        size = (RECT_SIZE * tetrion.width, (RECT_SIZE + 2) * tetrion.height)\n        screen = pygame.display.set_mode(size)\n\n        COLORS = [(0, 0, 0),\n                  (0, 240, 240),\n                  (0, 0, 240),\n                  (240, 160, 0),\n                  (240, 240, 0),\n                  (0, 240, 0),\n                  (160, 0, 240),\n                  (240, 0, 0)]\n\n        done = False\n\n        clock = pygame.time.Clock()\n\n        while not done:\n            for event in pygame.event.get():\n                if event.type == pygame.QUIT:\n                    done = True\n                elif event.type == pygame.KEYDOWN:\n                    if event.key == pygame.K_ESCAPE:\n                        done = True\n                    elif event.key == pygame.K_a:\n                        tetrion.enqueue_event(Event(key=Key.LEFT, type=EventType.PRESSED, frame=frame))\n                    elif event.key == pygame.K_d:\n                        tetrion.enqueue_event(Event(key=Key.RIGHT, type=EventType.PRESSED, frame=frame))\n                    elif event.key == pygame.K_SPACE:\n                        tetrion.enqueue_event(Event(key=Key.DROP, type=EventType.PRESSED, frame=frame))\n\n            tetrion.simulate_up_until(frame)\n            screen.fill((100, 100, 100))\n\n            matrix = tetrion.matrix()\n            for y, row in enumerate(matrix.rows):\n                for x, mino in enumerate(row):\n                    pygame.draw.rect(screen, COLORS[mino.value],\n                                     pygame.Rect(x * RECT_SIZE, y * RECT_SIZE, RECT_SIZE, RECT_SIZE))\n\n            active_tetromino = tetrion.try_get_active_tetromino()\n            if active_tetromino is not None:\n                for position in active_tetromino.mino_positions:",
    "suffix": ""
  },
  {
    "name": "Birch-san/natten-fwd-ad:script/demo.py@843",
    "canonical_solution": "canvas_len=32",
    "prompt": "import torch\nimport torch.autograd.forward_ad as fwAD\nfrom torch import inference_mode, enable_grad\nfrom torch.backends.cuda import sdp_kernel\nfrom src.natten_block import NattenBlock\nfrom src.hood_attn_block import NeighbourhoodAttnBlock\n\ndevice=torch.device('cuda')\ndtype=torch.bfloat16\nseed=42\nd_model=128\nd_head=64\nkernel_size=13\ntorch.manual_seed(seed)\nnatten_block = NattenBlock(d_model, d_head=d_head, kernel_size=kernel_size).to(device=device, dtype=dtype)\ntorch.manual_seed(seed)\nhood_block = NeighbourhoodAttnBlock(d_model, d_head=d_head, kernel_size=kernel_size).to(device=device, dtype=dtype)\n\nbatch=2",
    "prefix": "import torch\nimport torch.autograd.forward_ad as fwAD\nfrom torch import inference_mode, enable_grad\nfrom torch.backends.cuda import sdp_kernel\nfrom src.natten_block import NattenBlock\nfrom src.hood_attn_block import NeighbourhoodAttnBlock\n\ndevice=torch.device('cuda')\ndtype=torch.bfloat16\nseed=42\nd_model=128\nd_head=64\nkernel_size=13\ntorch.manual_seed(seed)\nnatten_block = NattenBlock(d_model, d_head=d_head, kernel_size=kernel_size).to(device=device, dtype=dtype)\ntorch.manual_seed(seed)\nhood_block = NeighbourhoodAttnBlock(d_model, d_head=d_head, kernel_size=kernel_size).to(device=device, dtype=dtype)\n\nbatch=2",
    "suffix": ""
  },
  {
    "name": "camenduru/ELYZA-japanese-Llama-2-13b-instruct-demo-hf:app.py@961",
    "canonical_solution": "- Llama 2 is licensed under the LLAMA 2 Community License, Copyright (c) Meta Platforms, Inc. All Rights Reserved.",
    "prompt": "from datetime import datetime, timezone, timedelta\nfrom typing import AsyncGenerator\nfrom botocore.config import Config\nfrom model_vllm import get_input_token_length, run\nimport os\nimport time\nimport uuid\nimport asyncio\nimport logging\nimport textwrap\nimport boto3\nimport gradio as gr\nimport pandas as pd\nimport torch\n\n\n\nlogging.basicConfig(encoding='utf-8', level=logging.ERROR)\nlogger = logging.getLogger(__name__)\n\nJST = timezone(timedelta(hours=+9), 'JST')\n\nDEFAULT_SYSTEM_PROMPT = '\u3042\u306a\u305f\u306f\u8aa0\u5b9f\u3067\u512a\u79c0\u306a\u65e5\u672c\u4eba\u306e\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002'\nMAX_MAX_NEW_TOKENS = 2048\nDEFAULT_MAX_NEW_TOKENS = 512\nMAX_INPUT_TOKEN_LENGTH = 4000\n\nTITLE = '# ELYZA-japanese-Llama-2-13b-instruct'\nDESCRIPTION = \"\"\"\n## \u6982\u8981\n- [ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b)\u306f\u3001[\u682a\u5f0f\u4f1a\u793eELYZA](https://elyza.ai/) (\u4ee5\u964d\u300c\u5f53\u793e\u300d\u3068\u547c\u79f0) \u304c[Llama2](https://ai.meta.com/llama/)\u3092\u30d9\u30fc\u30b9\u3068\u3057\u3066\u65e5\u672c\u8a9e\u80fd\u529b\u3092\u62e1\u5f35\u3059\u308b\u305f\u3081\u306b\u4e8b\u524d\u5b66\u7fd2\u3092\u884c\u3063\u305f\u30e2\u30c7\u30eb\u3067\u3059\u3002\n- [ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct)\u306f ELYZA-japanese-Llama-2-13b \u3092\u5f0a\u793e\u72ec\u81ea\u306einstruction tuning\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u4e8b\u5f8c\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u3059\u3002\n    - \u672c\u30c7\u30e2\u3067\u306f\u3053\u306e\u30e2\u30c7\u30eb\u304c\u4f7f\u308f\u308c\u3066\u3044\u307e\u3059\u3002\n- \u8a73\u7d30\u306f[Blog\u8a18\u4e8b](https://note.com/elyza/n/n5d42686b60b7)\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n- \u672c\u30c7\u30e2\u3067\u306f\u3053\u3061\u3089\u306e[Llama-2 7B Chat](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat)\u306e\u30c7\u30e2\u3092\u30d9\u30fc\u30b9\u306b\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002\n\n## License",
    "prefix": "from datetime import datetime, timezone, timedelta\nfrom typing import AsyncGenerator\nfrom botocore.config import Config\nfrom model_vllm import get_input_token_length, run\nimport os\nimport time\nimport uuid\nimport asyncio\nimport logging\nimport textwrap\nimport boto3\nimport gradio as gr\nimport pandas as pd\nimport torch\n\n\n\nlogging.basicConfig(encoding='utf-8', level=logging.ERROR)\nlogger = logging.getLogger(__name__)\n\nJST = timezone(timedelta(hours=+9), 'JST')\n\nDEFAULT_SYSTEM_PROMPT = '\u3042\u306a\u305f\u306f\u8aa0\u5b9f\u3067\u512a\u79c0\u306a\u65e5\u672c\u4eba\u306e\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002'\nMAX_MAX_NEW_TOKENS = 2048\nDEFAULT_MAX_NEW_TOKENS = 512\nMAX_INPUT_TOKEN_LENGTH = 4000\n\nTITLE = '# ELYZA-japanese-Llama-2-13b-instruct'\nDESCRIPTION = \"\"\"\n## \u6982\u8981\n- [ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b)\u306f\u3001[\u682a\u5f0f\u4f1a\u793eELYZA](https://elyza.ai/) (\u4ee5\u964d\u300c\u5f53\u793e\u300d\u3068\u547c\u79f0) \u304c[Llama2](https://ai.meta.com/llama/)\u3092\u30d9\u30fc\u30b9\u3068\u3057\u3066\u65e5\u672c\u8a9e\u80fd\u529b\u3092\u62e1\u5f35\u3059\u308b\u305f\u3081\u306b\u4e8b\u524d\u5b66\u7fd2\u3092\u884c\u3063\u305f\u30e2\u30c7\u30eb\u3067\u3059\u3002\n- [ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct)\u306f ELYZA-japanese-Llama-2-13b \u3092\u5f0a\u793e\u72ec\u81ea\u306einstruction tuning\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u4e8b\u5f8c\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067\u3059\u3002\n    - \u672c\u30c7\u30e2\u3067\u306f\u3053\u306e\u30e2\u30c7\u30eb\u304c\u4f7f\u308f\u308c\u3066\u3044\u307e\u3059\u3002\n- \u8a73\u7d30\u306f[Blog\u8a18\u4e8b](https://note.com/elyza/n/n5d42686b60b7)\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n- \u672c\u30c7\u30e2\u3067\u306f\u3053\u3061\u3089\u306e[Llama-2 7B Chat](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat)\u306e\u30c7\u30e2\u3092\u30d9\u30fc\u30b9\u306b\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002\n\n## License",
    "suffix": ""
  },
  {
    "name": "camenduru/MotionCtrl-hf:lvdm/modules/attention_temporal.py@797",
    "canonical_solution": "class FeedForward(nn.Module):",
    "prompt": "import math\nimport torch\nimport torch as th\nimport torch.nn.functional as F\n    import xformers\n    import xformers.ops\nfrom inspect import isfunction\nfrom torch import nn, einsum\nfrom einops import rearrange, repeat\nfrom lvdm.common import (\n    checkpoint,\n    exists,\n    uniq,\n    default,\n    max_neg_value,\n    init_\n)\nfrom lvdm.basics import (\n    conv_nd,\n    zero_module,\n    normalization\n)\n\ntry:\n    XFORMERS_IS_AVAILBLE = True\nexcept:\n    XFORMERS_IS_AVAILBLE = False\n    \n\n\nclass GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * F.gelu(gate)\n\n",
    "prefix": "import math\nimport torch\nimport torch as th\nimport torch.nn.functional as F\n    import xformers\n    import xformers.ops\nfrom inspect import isfunction\nfrom torch import nn, einsum\nfrom einops import rearrange, repeat\nfrom lvdm.common import (\n    checkpoint,\n    exists,\n    uniq,\n    default,\n    max_neg_value,\n    init_\n)\nfrom lvdm.basics import (\n    conv_nd,\n    zero_module,\n    normalization\n)\n\ntry:\n    XFORMERS_IS_AVAILBLE = True\nexcept:\n    XFORMERS_IS_AVAILBLE = False\n    \n\n\nclass GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * F.gelu(gate)\n\n",
    "suffix": ""
  },
  {
    "name": "vita-epfl/social-transmotion:evaluate_jrdb.py@1388",
    "canonical_solution": "    batch_id = 0",
    "prompt": "import argparse\nimport torch\nimport random\nimport numpy as np\nfrom progress.bar import Bar\nfrom torch.utils.data import DataLoader\nfrom dataset_jrdb import batch_process_coords, create_dataset, collate_batch\nfrom model_jrdb import create_model\nfrom utils.utils import create_logger\n\n\ndef inference(model, config, input_joints, padding_mask, out_len=14):\n    model.eval()\n    \n    with torch.no_grad():\n        pred_joints = model(input_joints, padding_mask)\n\n    output_joints = pred_joints[:,-out_len:]\n\n    return output_joints\n\n\ndef evaluate_ade_fde(model, modality_selection, dataloader, bs, config, logger, return_all=False, bar_prefix=\"\", per_joint=False, show_avg=False):\n    in_F, out_F = config['TRAIN']['input_track_size'], config['TRAIN']['output_track_size']\n    bar = Bar(f\"EVAL ADE_FDE\", fill=\"#\", max=len(dataloader))\n\n    batch_size = bs",
    "prefix": "import argparse\nimport torch\nimport random\nimport numpy as np\nfrom progress.bar import Bar\nfrom torch.utils.data import DataLoader\nfrom dataset_jrdb import batch_process_coords, create_dataset, collate_batch\nfrom model_jrdb import create_model\nfrom utils.utils import create_logger\n\n\ndef inference(model, config, input_joints, padding_mask, out_len=14):\n    model.eval()\n    \n    with torch.no_grad():\n        pred_joints = model(input_joints, padding_mask)\n\n    output_joints = pred_joints[:,-out_len:]\n\n    return output_joints\n\n\ndef evaluate_ade_fde(model, modality_selection, dataloader, bs, config, logger, return_all=False, bar_prefix=\"\", per_joint=False, show_avg=False):\n    in_F, out_F = config['TRAIN']['input_track_size'], config['TRAIN']['output_track_size']\n    bar = Bar(f\"EVAL ADE_FDE\", fill=\"#\", max=len(dataloader))\n\n    batch_size = bs",
    "suffix": ""
  },
  {
    "name": "facebookresearch/ca_body:ca_body/utils/geom_body.py@1430",
    "canonical_solution": "    n /= norm",
    "prompt": "import logging\nimport igl\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom logging import Logger\nfrom typing import Any, Dict, Optional, Tuple, Union\nfrom ca_body.utils.geom import (\n    index_image_impaint,\n    make_uv_barys,\n    make_uv_vert_index,\n)\nfrom trimesh import Trimesh\nfrom trimesh.triangles import points_to_barycentric\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# \n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n\n\n\n\n\n\nlogger: Logger = logging.getLogger(__name__)\n\n\ndef face_normals_v2(v: th.Tensor, vi: th.Tensor, eps: float = 1e-5) -> th.Tensor:\n    pts = v[:, vi]\n    v0 = pts[:, :, 1] - pts[:, :, 0]\n    v1 = pts[:, :, 2] - pts[:, :, 0]\n    n = th.cross(v0, v1, dim=-1)\n    norm = th.norm(n, dim=-1, keepdim=True)\n    norm[norm < eps] = 1",
    "prefix": "import logging\nimport igl\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom logging import Logger\nfrom typing import Any, Dict, Optional, Tuple, Union\nfrom ca_body.utils.geom import (\n    index_image_impaint,\n    make_uv_barys,\n    make_uv_vert_index,\n)\nfrom trimesh import Trimesh\nfrom trimesh.triangles import points_to_barycentric\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# \n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n\n\n\n\n\n\nlogger: Logger = logging.getLogger(__name__)\n\n\ndef face_normals_v2(v: th.Tensor, vi: th.Tensor, eps: float = 1e-5) -> th.Tensor:\n    pts = v[:, vi]\n    v0 = pts[:, :, 1] - pts[:, :, 0]\n    v1 = pts[:, :, 2] - pts[:, :, 0]\n    n = th.cross(v0, v1, dim=-1)\n    norm = th.norm(n, dim=-1, keepdim=True)\n    norm[norm < eps] = 1",
    "suffix": ""
  },
  {
    "name": "ccurme/chesster:chesster/app/app.py@1299",
    "canonical_solution": "    return templates.TemplateResponse(request, \"index.html\")",
    "prompt": "import time\nimport chess\nimport chess.svg\nfrom typing import Any, AsyncIterator\nfrom fastapi import FastAPI, Request, WebSocket\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.templating import Jinja2Templates\nfrom chesster.app.board_manager import BoardManager\nfrom chesster.app.utils import (\n    get_engine_move,\n    parse_chess_move,\n    parse_pgn_into_move_list,\n    serialize_board_state,\n)\n\n\n\n\napp = FastAPI()\napp.mount(\"/static\", StaticFiles(directory=\"chesster/app/static\"), name=\"static\")\ntemplates = Jinja2Templates(directory=\"chesster/app/templates\")\n\nboard_manager = BoardManager()\n\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def root(request: Request):",
    "prefix": "import time\nimport chess\nimport chess.svg\nfrom typing import Any, AsyncIterator\nfrom fastapi import FastAPI, Request, WebSocket\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.templating import Jinja2Templates\nfrom chesster.app.board_manager import BoardManager\nfrom chesster.app.utils import (\n    get_engine_move,\n    parse_chess_move,\n    parse_pgn_into_move_list,\n    serialize_board_state,\n)\n\n\n\n\napp = FastAPI()\napp.mount(\"/static\", StaticFiles(directory=\"chesster/app/static\"), name=\"static\")\ntemplates = Jinja2Templates(directory=\"chesster/app/templates\")\n\nboard_manager = BoardManager()\n\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def root(request: Request):",
    "suffix": ""
  },
  {
    "name": "zkarpinski/codeinsight-sdk-python:codeinsight_sdk/handlers.py@673",
    "canonical_solution": "        handler = handlers.get(k)",
    "prompt": "import abc\nfrom typing import List\nfrom codeinsight_sdk.models import Project, ProjectInventory, ProjectInventoryItem, Report\nfrom codeinsight_sdk.exceptions import CodeInsightError\n\n\nclass Handler(abc.ABC):\n    def __init__(self, client):\n        self.client = client\n        self.cls = None\n    \n    @staticmethod\n    def create(client, cls):\n        k = cls.__name__\n        handlers = {\"Project\": ProjectHandler,\n                    \"Report\": ReportHandler\n            }",
    "prefix": "import abc\nfrom typing import List\nfrom codeinsight_sdk.models import Project, ProjectInventory, ProjectInventoryItem, Report\nfrom codeinsight_sdk.exceptions import CodeInsightError\n\n\nclass Handler(abc.ABC):\n    def __init__(self, client):\n        self.client = client\n        self.cls = None\n    \n    @staticmethod\n    def create(client, cls):\n        k = cls.__name__\n        handlers = {\"Project\": ProjectHandler,\n                    \"Report\": ReportHandler\n            }",
    "suffix": ""
  },
  {
    "name": "chebupelka8/Engine:scripts/loop.py@1053",
    "canonical_solution": "    def fps(self) -> float:\r",
    "prompt": "import pygame, sys\r\nfrom pygame.locals import *\r\nfrom .math import Vec2\r\nfrom .image import Image\r\n\r\n\r\nclass WindowLoop:\r\n\r\n    def __init__(self, __size: Vec2, fps: int = 144) -> None:\r\n        pygame.init()\r\n\r\n        self.__display = pygame.display.set_mode((__size.x, __size.y))\r\n        pygame.display.set_caption(\"Engine: v0.1\")\r\n        pygame.display.set_icon(Image(\"Engine/assets/icon.png\").image)\r\n        \r\n        self.__clock = pygame.time.Clock()\r\n        self.__fps = fps\r\n    \r\n    def set_window_title(self, __title: str) -> None:\r\n        pygame.display.set_caption(__title)\r\n    \r\n    def set_window_icon(self, __icon: Image) -> None:\r\n        pygame.display.set_icon(__icon.image)\r\n    \r\n    @property\r\n    def display(self) -> pygame.Surface:\r\n        return self.__display\r\n    \r\n    @property\r",
    "prefix": "import pygame, sys\r\nfrom pygame.locals import *\r\nfrom .math import Vec2\r\nfrom .image import Image\r\n\r\n\r\nclass WindowLoop:\r\n\r\n    def __init__(self, __size: Vec2, fps: int = 144) -> None:\r\n        pygame.init()\r\n\r\n        self.__display = pygame.display.set_mode((__size.x, __size.y))\r\n        pygame.display.set_caption(\"Engine: v0.1\")\r\n        pygame.display.set_icon(Image(\"Engine/assets/icon.png\").image)\r\n        \r\n        self.__clock = pygame.time.Clock()\r\n        self.__fps = fps\r\n    \r\n    def set_window_title(self, __title: str) -> None:\r\n        pygame.display.set_caption(__title)\r\n    \r\n    def set_window_icon(self, __icon: Image) -> None:\r\n        pygame.display.set_icon(__icon.image)\r\n    \r\n    @property\r\n    def display(self) -> pygame.Surface:\r\n        return self.__display\r\n    \r\n    @property\r",
    "suffix": ""
  },
  {
    "name": "lxbme/TSPLifesaver:test/test_optimizer.py@689",
    "canonical_solution": "                       [[565, 575], [25, 185], [345, 750], [945, 685], [845, 655], [880, 660],",
    "prompt": "import unittest\nfrom TSPLifesaver.optimizer import *\nfrom TSPLifesaver.structure import BasicRoute, PointWithEuclideanDistance\n\n\n\nclass TestStructure(unittest.TestCase):\n\n    def test_SimulatedAnnealing(self):\n        self.points = [PointWithEuclideanDistance(i) for i in\n                       [[565, 575], [25, 185], [345, 750], [945, 685], [845, 655], [880, 660],\n                        [25, 230], [525, 1000], [580, 1175], [650, 1130], [1605, 620], [1220, 580], [1465, 200],\n                        [1530, 5],\n                        [845, 680], [725, 370]]]\n        self.route = BasicRoute(self.points, name=\"test route\")\n        optimizer = SimulatedAnnealing(self.route, 10000, 0.003, 1)\n        self.route = optimizer.optimize()\n        print(\"\\n\")\n        print(self.route.distance())\n\n    def test_ExhaustiveIndexing(self):\n        self.points = [PointWithEuclideanDistance(i) for i in",
    "prefix": "import unittest\nfrom TSPLifesaver.optimizer import *\nfrom TSPLifesaver.structure import BasicRoute, PointWithEuclideanDistance\n\n\n\nclass TestStructure(unittest.TestCase):\n\n    def test_SimulatedAnnealing(self):\n        self.points = [PointWithEuclideanDistance(i) for i in\n                       [[565, 575], [25, 185], [345, 750], [945, 685], [845, 655], [880, 660],\n                        [25, 230], [525, 1000], [580, 1175], [650, 1130], [1605, 620], [1220, 580], [1465, 200],\n                        [1530, 5],\n                        [845, 680], [725, 370]]]\n        self.route = BasicRoute(self.points, name=\"test route\")\n        optimizer = SimulatedAnnealing(self.route, 10000, 0.003, 1)\n        self.route = optimizer.optimize()\n        print(\"\\n\")\n        print(self.route.distance())\n\n    def test_ExhaustiveIndexing(self):\n        self.points = [PointWithEuclideanDistance(i) for i in",
    "suffix": ""
  },
  {
    "name": "daswer123/rvc-python:rvc_python/__main__.py@1376",
    "canonical_solution": "        index_path=args.index,",
    "prompt": "import argparse\nimport sys\nimport os\nfrom argparse import ArgumentParser\nfrom rvc_python.infer import infer_file,infer_files\n\n\nparser = ArgumentParser(description=\"RVC inference\")\n# Create a mutually exclusive group for input - only one of them can be provided\ninput_group = parser.add_mutually_exclusive_group(required=True)\ninput_group.add_argument(\"-i\", \"--input\", type=str, help=\"Path to input file\")\ninput_group.add_argument(\"-d\", \"--dir\", type=str, help=\"Directory path containing audio files\")\n\nparser.add_argument(\"-pi\",\"--pitch\", default=0, type=int, help=\"Transpose (integer, number of semitones)\")\nparser.add_argument(\"-ip\",\"--index\", type=str, nargs='?', default=\"\", help=\"Path to index file (optional)\")\nparser.add_argument(\"-me\",\"--method\", type=str, default=\"harvest\", choices=['harvest', \"crepe\", \"rmvpe\", 'pm'], help=\"Pitch extraction algorithm\")\nparser.add_argument(\"-v\",\"--version\", type=str, default=\"v2\", choices=['v1', \"v2\"], help=\"Model version\")\nparser.add_argument(\"-o\",\"--output\", type=str, nargs='?', default=\"out.wav\", help=\"Output path for single file, or output directory for multiple files\")\nparser.add_argument(\"-mp\",\"--model\", type=str, required=True, help=\"Path to model file\")\nparser.add_argument(\"-ir\",\"--index_rate\", type=float, default=0.5, help=\"Search feature ratio\")\nparser.add_argument(\"-de\",\"--device\", type=str, default=\"cuda:0\", help=\"Device to use (e.g., cpu:0, cuda:0)\")\nparser.add_argument(\"-fr\",\"--filter_radius\", type=int, default=3, help=\"Apply median filtering to the pitch results\")\nparser.add_argument(\"-rsr\",\"--resample_sr\", type=int, default=0, help=\"Resample rate for the output audio\")\nparser.add_argument(\"-rmr\",\"--rms_mix_rate\", type=float,default=0.25 ,help=\"Volume envelope mix rate\")\nparser.add_argument(\"-pr\",'--protect' ,type=float,default=0.33 ,help='Protect voiceless consonants and breath sounds')\n\nargs = parser.parse_args()\n\nif args.input:\n    # Single file processing\n    inferred_path = infer_file(\n        input_path=args.input,\n        model_path=args.model,\n        index_path=args.index,\n        device=args.device,\n        f0method=args.method,\n        f0up_key=args.pitch,\n        opt_path=args.output,\n        index_rate=args.index_rate,\n        filter_radius=args.filter_radius,\n        resample_sr=args.resample_sr,\n        rms_mix_rate=args.rms_mix_rate,\n        protect=args.protect,\n        version=args.version\n    )\nelif args.dir:\n    # Directory processing\n    processed_files = infer_files(\n        dir_path=args.dir,\n        model_path=args.model,",
    "prefix": "import argparse\nimport sys\nimport os\nfrom argparse import ArgumentParser\nfrom rvc_python.infer import infer_file,infer_files\n\n\nparser = ArgumentParser(description=\"RVC inference\")\n# Create a mutually exclusive group for input - only one of them can be provided\ninput_group = parser.add_mutually_exclusive_group(required=True)\ninput_group.add_argument(\"-i\", \"--input\", type=str, help=\"Path to input file\")\ninput_group.add_argument(\"-d\", \"--dir\", type=str, help=\"Directory path containing audio files\")\n\nparser.add_argument(\"-pi\",\"--pitch\", default=0, type=int, help=\"Transpose (integer, number of semitones)\")\nparser.add_argument(\"-ip\",\"--index\", type=str, nargs='?', default=\"\", help=\"Path to index file (optional)\")\nparser.add_argument(\"-me\",\"--method\", type=str, default=\"harvest\", choices=['harvest', \"crepe\", \"rmvpe\", 'pm'], help=\"Pitch extraction algorithm\")\nparser.add_argument(\"-v\",\"--version\", type=str, default=\"v2\", choices=['v1', \"v2\"], help=\"Model version\")\nparser.add_argument(\"-o\",\"--output\", type=str, nargs='?', default=\"out.wav\", help=\"Output path for single file, or output directory for multiple files\")\nparser.add_argument(\"-mp\",\"--model\", type=str, required=True, help=\"Path to model file\")\nparser.add_argument(\"-ir\",\"--index_rate\", type=float, default=0.5, help=\"Search feature ratio\")\nparser.add_argument(\"-de\",\"--device\", type=str, default=\"cuda:0\", help=\"Device to use (e.g., cpu:0, cuda:0)\")\nparser.add_argument(\"-fr\",\"--filter_radius\", type=int, default=3, help=\"Apply median filtering to the pitch results\")\nparser.add_argument(\"-rsr\",\"--resample_sr\", type=int, default=0, help=\"Resample rate for the output audio\")\nparser.add_argument(\"-rmr\",\"--rms_mix_rate\", type=float,default=0.25 ,help=\"Volume envelope mix rate\")\nparser.add_argument(\"-pr\",'--protect' ,type=float,default=0.33 ,help='Protect voiceless consonants and breath sounds')\n\nargs = parser.parse_args()\n\nif args.input:\n    # Single file processing\n    inferred_path = infer_file(\n        input_path=args.input,\n        model_path=args.model,\n        index_path=args.index,\n        device=args.device,\n        f0method=args.method,\n        f0up_key=args.pitch,\n        opt_path=args.output,\n        index_rate=args.index_rate,\n        filter_radius=args.filter_radius,\n        resample_sr=args.resample_sr,\n        rms_mix_rate=args.rms_mix_rate,\n        protect=args.protect,\n        version=args.version\n    )\nelif args.dir:\n    # Directory processing\n    processed_files = infer_files(\n        dir_path=args.dir,\n        model_path=args.model,",
    "suffix": ""
  },
  {
    "name": "CodeBrugs/ToolSculpt:tests/test_tools/test_tool2.py@657",
    "canonical_solution": "        self.assertEqual(result, \"Tool2 performed an additional task successfully\")",
    "prompt": "import unittest\nfrom src.tools.Tool2.tool2_functions import process_data, analyze_data, perform_additional_task\n# tests/test_tools/test_tool2.py\n\nclass TestTool2Functions(unittest.TestCase):\n    def test_process_data(self):\n        # Prueba para la funci\u00f3n process_data\n        input_data = \"example_data\"\n        result = process_data(input_data)\n        self.assertEqual(result, \"Tool2 processed the data: A_SPECIAL_TRANSFORMATION\")\n\n    def test_analyze_data(self):\n        # Prueba para la funci\u00f3n analyze_data\n        input_data = \"example_data\"\n        result = analyze_data(input_data)\n        self.assertEqual(result, \"Tool2 analyzed the data: example_data. Alphabetic characters: 11\")\n\n    def test_perform_additional_task(self):\n        # Prueba para la funci\u00f3n perform_additional_task\n        result = perform_additional_task()",
    "prefix": "import unittest\nfrom src.tools.Tool2.tool2_functions import process_data, analyze_data, perform_additional_task\n# tests/test_tools/test_tool2.py\n\nclass TestTool2Functions(unittest.TestCase):\n    def test_process_data(self):\n        # Prueba para la funci\u00f3n process_data\n        input_data = \"example_data\"\n        result = process_data(input_data)\n        self.assertEqual(result, \"Tool2 processed the data: A_SPECIAL_TRANSFORMATION\")\n\n    def test_analyze_data(self):\n        # Prueba para la funci\u00f3n analyze_data\n        input_data = \"example_data\"\n        result = analyze_data(input_data)\n        self.assertEqual(result, \"Tool2 analyzed the data: example_data. Alphabetic characters: 11\")\n\n    def test_perform_additional_task(self):\n        # Prueba para la funci\u00f3n perform_additional_task\n        result = perform_additional_task()",
    "suffix": ""
  },
  {
    "name": "run-llama/rags:1_\ud83c\udfe0_Home.py@1561",
    "canonical_solution": "        with st.chat_message(\"user\"):",
    "prompt": "import streamlit as st\nfrom streamlit_pills import pills\nfrom st_utils import (\n    add_builder_config,\n    add_sidebar,\n    get_current_state,\n)\n\n\ncurrent_state = get_current_state()\n\n####################\n#### STREAMLIT #####\n####################\n\n\nst.set_page_config(\n    page_title=\"Build a RAGs bot, powered by LlamaIndex\",\n    page_icon=\"\ud83e\udd99\",\n    layout=\"centered\",\n    initial_sidebar_state=\"auto\",\n    menu_items=None,\n)\nst.title(\"Build a RAGs bot, powered by LlamaIndex \ud83d\udcac\ud83e\udd99\")\nst.info(\n    \"Use this page to build your RAG bot over your data! \"\n    \"Once the agent is finished creating, check out the `RAG Config` and \"\n    \"`Generated RAG Agent` pages.\\n\"\n    \"To build a new agent, please make sure that 'Create a new agent' is selected.\",\n    icon=\"\u2139\ufe0f\",\n)\nif \"metaphor_key\" in st.secrets:\n    st.info(\"**NOTE**: The ability to add web search is enabled.\")\n\n\nadd_builder_config()\nadd_sidebar()\n\n\nst.info(f\"Currently building/editing agent: {current_state.cache.agent_id}\", icon=\"\u2139\ufe0f\")\n\n# add pills\nselected = pills(\n    \"Outline your task!\",\n    [\n        \"I want to analyze this PDF file (data/invoices.pdf)\",\n        \"I want to search over my CSV documents.\",\n    ],\n    clearable=True,\n    index=None,\n)\n\nif \"messages\" not in st.session_state.keys():  # Initialize the chat messages history\n    st.session_state.messages = [\n        {\"role\": \"assistant\", \"content\": \"What RAG bot do you want to build?\"}\n    ]\n\n\ndef add_to_message_history(role: str, content: str) -> None:\n    message = {\"role\": role, \"content\": str(content)}\n    st.session_state.messages.append(message)  # Add response to message history\n\n\nfor message in st.session_state.messages:  # Display the prior chat messages\n    with st.chat_message(message[\"role\"]):\n        st.write(message[\"content\"])\n\n# TODO: this is really hacky, only because st.rerun is jank\nif prompt := st.chat_input(\n    \"Your question\",\n):  # Prompt for user input and save to chat history\n    # TODO: hacky\n    if \"has_rerun\" in st.session_state.keys() and st.session_state.has_rerun:\n        # if this is true, skip the user input\n        st.session_state.has_rerun = False\n    else:\n        add_to_message_history(\"user\", prompt)",
    "prefix": "import streamlit as st\nfrom streamlit_pills import pills\nfrom st_utils import (\n    add_builder_config,\n    add_sidebar,\n    get_current_state,\n)\n\n\ncurrent_state = get_current_state()\n\n####################\n#### STREAMLIT #####\n####################\n\n\nst.set_page_config(\n    page_title=\"Build a RAGs bot, powered by LlamaIndex\",\n    page_icon=\"\ud83e\udd99\",\n    layout=\"centered\",\n    initial_sidebar_state=\"auto\",\n    menu_items=None,\n)\nst.title(\"Build a RAGs bot, powered by LlamaIndex \ud83d\udcac\ud83e\udd99\")\nst.info(\n    \"Use this page to build your RAG bot over your data! \"\n    \"Once the agent is finished creating, check out the `RAG Config` and \"\n    \"`Generated RAG Agent` pages.\\n\"\n    \"To build a new agent, please make sure that 'Create a new agent' is selected.\",\n    icon=\"\u2139\ufe0f\",\n)\nif \"metaphor_key\" in st.secrets:\n    st.info(\"**NOTE**: The ability to add web search is enabled.\")\n\n\nadd_builder_config()\nadd_sidebar()\n\n\nst.info(f\"Currently building/editing agent: {current_state.cache.agent_id}\", icon=\"\u2139\ufe0f\")\n\n# add pills\nselected = pills(\n    \"Outline your task!\",\n    [\n        \"I want to analyze this PDF file (data/invoices.pdf)\",\n        \"I want to search over my CSV documents.\",\n    ],\n    clearable=True,\n    index=None,\n)\n\nif \"messages\" not in st.session_state.keys():  # Initialize the chat messages history\n    st.session_state.messages = [\n        {\"role\": \"assistant\", \"content\": \"What RAG bot do you want to build?\"}\n    ]\n\n\ndef add_to_message_history(role: str, content: str) -> None:\n    message = {\"role\": role, \"content\": str(content)}\n    st.session_state.messages.append(message)  # Add response to message history\n\n\nfor message in st.session_state.messages:  # Display the prior chat messages\n    with st.chat_message(message[\"role\"]):\n        st.write(message[\"content\"])\n\n# TODO: this is really hacky, only because st.rerun is jank\nif prompt := st.chat_input(\n    \"Your question\",\n):  # Prompt for user input and save to chat history\n    # TODO: hacky\n    if \"has_rerun\" in st.session_state.keys() and st.session_state.has_rerun:\n        # if this is true, skip the user input\n        st.session_state.has_rerun = False\n    else:\n        add_to_message_history(\"user\", prompt)",
    "suffix": ""
  },
  {
    "name": "open-mmlab/Amphion:preprocessors/pjs.py@964",
    "canonical_solution": "                    \"Uid\": \"{}_{}\".format(song_id, i),",
    "prompt": "import os\nimport glob\nimport json\nimport torchaudio\nfrom tqdm import tqdm\nfrom utils.util import has_existed\nfrom utils.io import save_audio\n# Copyright (c) 2023 Amphion.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n\n\ndef get_splitted_utterances(\n    raw_wav_dir, trimed_wav_dir, n_utterance_splits, overlapping\n):\n    res = []\n    raw_song_files = glob.glob(\n        os.path.join(raw_wav_dir, \"**/pjs*_song.wav\"), recursive=True\n    )\n    trimed_song_files = glob.glob(\n        os.path.join(trimed_wav_dir, \"**/*.wav\"), recursive=True\n    )\n\n    if len(raw_song_files) * n_utterance_splits == len(trimed_song_files):\n        print(\"Splitted done...\")\n        for wav_file in tqdm(trimed_song_files):\n            uid = wav_file.split(\"/\")[-1].split(\".\")[0]\n            utt = {\"Dataset\": \"pjs\", \"Singer\": \"male1\", \"Uid\": uid, \"Path\": wav_file}\n\n            waveform, sample_rate = torchaudio.load(wav_file)\n            duration = waveform.size(-1) / sample_rate\n            utt[\"Duration\"] = duration\n\n            res.append(utt)\n\n    else:\n        for wav_file in tqdm(raw_song_files):\n            song_id = wav_file.split(\"/\")[-1].split(\".\")[0]\n\n            waveform, sample_rate = torchaudio.load(wav_file)\n            trimed_waveform = torchaudio.functional.vad(waveform, sample_rate)\n            trimed_waveform = torchaudio.functional.vad(\n                trimed_waveform.flip(dims=[1]), sample_rate\n            ).flip(dims=[1])\n\n            audio_len = trimed_waveform.size(-1)\n            lapping_len = overlapping * sample_rate\n\n            for i in range(n_utterance_splits):\n                start = i * audio_len // 3\n                end = start + audio_len // 3 + lapping_len\n                splitted_waveform = trimed_waveform[:, start:end]\n\n                utt = {\n                    \"Dataset\": \"pjs\",\n                    \"Singer\": \"male1\",",
    "prefix": "import os\nimport glob\nimport json\nimport torchaudio\nfrom tqdm import tqdm\nfrom utils.util import has_existed\nfrom utils.io import save_audio\n# Copyright (c) 2023 Amphion.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n\n\ndef get_splitted_utterances(\n    raw_wav_dir, trimed_wav_dir, n_utterance_splits, overlapping\n):\n    res = []\n    raw_song_files = glob.glob(\n        os.path.join(raw_wav_dir, \"**/pjs*_song.wav\"), recursive=True\n    )\n    trimed_song_files = glob.glob(\n        os.path.join(trimed_wav_dir, \"**/*.wav\"), recursive=True\n    )\n\n    if len(raw_song_files) * n_utterance_splits == len(trimed_song_files):\n        print(\"Splitted done...\")\n        for wav_file in tqdm(trimed_song_files):\n            uid = wav_file.split(\"/\")[-1].split(\".\")[0]\n            utt = {\"Dataset\": \"pjs\", \"Singer\": \"male1\", \"Uid\": uid, \"Path\": wav_file}\n\n            waveform, sample_rate = torchaudio.load(wav_file)\n            duration = waveform.size(-1) / sample_rate\n            utt[\"Duration\"] = duration\n\n            res.append(utt)\n\n    else:\n        for wav_file in tqdm(raw_song_files):\n            song_id = wav_file.split(\"/\")[-1].split(\".\")[0]\n\n            waveform, sample_rate = torchaudio.load(wav_file)\n            trimed_waveform = torchaudio.functional.vad(waveform, sample_rate)\n            trimed_waveform = torchaudio.functional.vad(\n                trimed_waveform.flip(dims=[1]), sample_rate\n            ).flip(dims=[1])\n\n            audio_len = trimed_waveform.size(-1)\n            lapping_len = overlapping * sample_rate\n\n            for i in range(n_utterance_splits):\n                start = i * audio_len // 3\n                end = start + audio_len // 3 + lapping_len\n                splitted_waveform = trimed_waveform[:, start:end]\n\n                utt = {\n                    \"Dataset\": \"pjs\",\n                    \"Singer\": \"male1\",",
    "suffix": ""
  },
  {
    "name": "KwaiKEG/KwaiAgents:kwaiagents/tools/search.py@911",
    "canonical_solution": "    @property",
    "prompt": "from itertools import islice\nfrom bs4 import BeautifulSoup as soup\nfrom duckduckgo_search import DDGS\nfrom kwaiagents.tools.base import BaseResult, BaseTool\nfrom kwaiagents.utils.selenium_utils import get_pagesource_with_selenium\nfrom kwaiagents.config import Config\nimport json\nimport os\nimport random\nimport traceback\n\n\n\nclass SearchResult(BaseResult):\n    @property\n    def answer(self):\n        if not self.json_data:\n            return \"\"\n        else:\n            rst = \"\"\n            for item in self.json_data:\n                rst += f'title: {item[\"title\"]}\\nbody: {item[\"body\"]}\\nurl: {item[\"href\"]}\\n'\n            return rst.strip()\n\n    @property\n    def answer_md(self):\n        if not self.json_data:\n            return \"\"\n        else:\n            return \"\\n\" + \"\\n\".join([f'{idx + 1}. <a href=\"{item[\"href\"]}\" target=\"_blank\"><b>{item[\"title\"]}</b></a>' + \" | \" + item[\"body\"] \n                for idx, item in enumerate(self.json_data)])\n",
    "prefix": "from itertools import islice\nfrom bs4 import BeautifulSoup as soup\nfrom duckduckgo_search import DDGS\nfrom kwaiagents.tools.base import BaseResult, BaseTool\nfrom kwaiagents.utils.selenium_utils import get_pagesource_with_selenium\nfrom kwaiagents.config import Config\nimport json\nimport os\nimport random\nimport traceback\n\n\n\nclass SearchResult(BaseResult):\n    @property\n    def answer(self):\n        if not self.json_data:\n            return \"\"\n        else:\n            rst = \"\"\n            for item in self.json_data:\n                rst += f'title: {item[\"title\"]}\\nbody: {item[\"body\"]}\\nurl: {item[\"href\"]}\\n'\n            return rst.strip()\n\n    @property\n    def answer_md(self):\n        if not self.json_data:\n            return \"\"\n        else:\n            return \"\\n\" + \"\\n\".join([f'{idx + 1}. <a href=\"{item[\"href\"]}\" target=\"_blank\"><b>{item[\"title\"]}</b></a>' + \" | \" + item[\"body\"] \n                for idx, item in enumerate(self.json_data)])\n",
    "suffix": ""
  },
  {
    "name": "EnVision-Research/LucidDreamer:utils/camera_utils.py@1501",
    "canonical_solution": "        else:",
    "prompt": "from scene.cameras import Camera, RCamera\nfrom utils.general_utils import PILtoTorch\nfrom utils.graphics_utils import fov2focal\nimport numpy as np\n#\n# Copyright (C) 2023, Inria\n# GRAPHDECO research group, https://team.inria.fr/graphdeco\n# All rights reserved.\n#\n# This software is free for non-commercial, research and evaluation use \n# under the terms of the LICENSE.md file.\n#\n# For inquiries contact  george.drettakis@inria.fr\n#\n\n\nWARNED = False\n\ndef loadCam(args, id, cam_info, resolution_scale):\n    orig_w, orig_h = cam_info.image.size\n\n    if args.resolution in [1, 2, 4, 8]:\n        resolution = round(orig_w/(resolution_scale * args.resolution)), round(orig_h/(resolution_scale * args.resolution))\n    else:  # should be a type that converts to float\n        if args.resolution == -1:\n            if orig_w > 1600:\n                global WARNED\n                if not WARNED:\n                    print(\"[ INFO ] Encountered quite large input images (>1.6K pixels width), rescaling to 1.6K.\\n \"\n                        \"If this is not desired, please explicitly specify '--resolution/-r' as 1\")\n                    WARNED = True\n                global_down = orig_w / 1600\n            else:\n                global_down = 1",
    "prefix": "from scene.cameras import Camera, RCamera\nfrom utils.general_utils import PILtoTorch\nfrom utils.graphics_utils import fov2focal\nimport numpy as np\n#\n# Copyright (C) 2023, Inria\n# GRAPHDECO research group, https://team.inria.fr/graphdeco\n# All rights reserved.\n#\n# This software is free for non-commercial, research and evaluation use \n# under the terms of the LICENSE.md file.\n#\n# For inquiries contact  george.drettakis@inria.fr\n#\n\n\nWARNED = False\n\ndef loadCam(args, id, cam_info, resolution_scale):\n    orig_w, orig_h = cam_info.image.size\n\n    if args.resolution in [1, 2, 4, 8]:\n        resolution = round(orig_w/(resolution_scale * args.resolution)), round(orig_h/(resolution_scale * args.resolution))\n    else:  # should be a type that converts to float\n        if args.resolution == -1:\n            if orig_w > 1600:\n                global WARNED\n                if not WARNED:\n                    print(\"[ INFO ] Encountered quite large input images (>1.6K pixels width), rescaling to 1.6K.\\n \"\n                        \"If this is not desired, please explicitly specify '--resolution/-r' as 1\")\n                    WARNED = True\n                global_down = orig_w / 1600\n            else:\n                global_down = 1",
    "suffix": ""
  },
  {
    "name": "VRSEN/agency-swarm:agency_swarm/tools/browsing/ReadURL.py@1152",
    "canonical_solution": "        var popUpSelectors = ['modal', 'popup', 'overlay', 'dialog']; // Add more selectors that are commonly used for pop-ups",
    "prompt": "import json\nimport os\nimport time\nfrom urllib.parse import urlparse\nfrom pydantic import Field\nfrom selenium.common import WebDriverException\nfrom agency_swarm.tools import BaseTool\nfrom agency_swarm.tools.browsing.util.selenium import get_web_driver, set_web_driver\n\n\n\n\nclass ReadURL(BaseTool):\n    \"\"\"\nThis tool reads a single URL and opens it in your current browser window. For each new source, go to a direct URL\nthat you think might contain the answer to the user's question or perform a google search like\n'https://google.com/search?q=search' if applicable. Otherwise, don't try to guess the direct url, use ClickElement tool\nto click on the link that you think might contain the desired information on the current web page.\nRemember, this tool only supports opening 1 URL at a time. Previous URL will be closed when you open a new one.\n    \"\"\"\n    url: str = Field(\n        ..., description=\"URL of the webpage.\", examples=[\"https://google.com/search?q=search\"]\n    )\n\n    def run(self):\n        wd = get_web_driver()\n\n        wd.get(self.url)\n\n        time.sleep(2)\n\n        # remove all popups\n        js_script = \"\"\"",
    "prefix": "import json\nimport os\nimport time\nfrom urllib.parse import urlparse\nfrom pydantic import Field\nfrom selenium.common import WebDriverException\nfrom agency_swarm.tools import BaseTool\nfrom agency_swarm.tools.browsing.util.selenium import get_web_driver, set_web_driver\n\n\n\n\nclass ReadURL(BaseTool):\n    \"\"\"\nThis tool reads a single URL and opens it in your current browser window. For each new source, go to a direct URL\nthat you think might contain the answer to the user's question or perform a google search like\n'https://google.com/search?q=search' if applicable. Otherwise, don't try to guess the direct url, use ClickElement tool\nto click on the link that you think might contain the desired information on the current web page.\nRemember, this tool only supports opening 1 URL at a time. Previous URL will be closed when you open a new one.\n    \"\"\"\n    url: str = Field(\n        ..., description=\"URL of the webpage.\", examples=[\"https://google.com/search?q=search\"]\n    )\n\n    def run(self):\n        wd = get_web_driver()\n\n        wd.get(self.url)\n\n        time.sleep(2)\n\n        # remove all popups\n        js_script = \"\"\"",
    "suffix": ""
  },
  {
    "name": "resemble-ai/resemble-enhance:resemble_enhance/enhancer/univnet/alias_free_torch/resample.py@1140",
    "canonical_solution": "        return x",
    "prompt": "import torch.nn as nn\nfrom torch.nn import functional as F\nfrom .filter import LowPassFilter1d\nfrom .filter import kaiser_sinc_filter1d\n# Adapted from https://github.com/junjun3518/alias-free-torch under the Apache License 2.0\n#   LICENSE is in incl_licenses directory.\n\n\n\nclass UpSample1d(nn.Module):\n    def __init__(self, ratio=2, kernel_size=None):\n        super().__init__()\n        self.ratio = ratio\n        self.kernel_size = int(6 * ratio // 2) * 2 if kernel_size is None else kernel_size\n        self.stride = ratio\n        self.pad = self.kernel_size // ratio - 1\n        self.pad_left = self.pad * self.stride + (self.kernel_size - self.stride) // 2\n        self.pad_right = self.pad * self.stride + (self.kernel_size - self.stride + 1) // 2\n        filter = kaiser_sinc_filter1d(cutoff=0.5 / ratio,\n                                      half_width=0.6 / ratio,\n                                      kernel_size=self.kernel_size)\n        self.register_buffer(\"filter\", filter)\n\n    # x: [B, C, T]\n    def forward(self, x):\n        _, C, _ = x.shape\n\n        x = F.pad(x, (self.pad, self.pad), mode='replicate')\n        x = self.ratio * F.conv_transpose1d(\n            x, self.filter.expand(C, -1, -1), stride=self.stride, groups=C)\n        x = x[..., self.pad_left:-self.pad_right]\n",
    "prefix": "import torch.nn as nn\nfrom torch.nn import functional as F\nfrom .filter import LowPassFilter1d\nfrom .filter import kaiser_sinc_filter1d\n# Adapted from https://github.com/junjun3518/alias-free-torch under the Apache License 2.0\n#   LICENSE is in incl_licenses directory.\n\n\n\nclass UpSample1d(nn.Module):\n    def __init__(self, ratio=2, kernel_size=None):\n        super().__init__()\n        self.ratio = ratio\n        self.kernel_size = int(6 * ratio // 2) * 2 if kernel_size is None else kernel_size\n        self.stride = ratio\n        self.pad = self.kernel_size // ratio - 1\n        self.pad_left = self.pad * self.stride + (self.kernel_size - self.stride) // 2\n        self.pad_right = self.pad * self.stride + (self.kernel_size - self.stride + 1) // 2\n        filter = kaiser_sinc_filter1d(cutoff=0.5 / ratio,\n                                      half_width=0.6 / ratio,\n                                      kernel_size=self.kernel_size)\n        self.register_buffer(\"filter\", filter)\n\n    # x: [B, C, T]\n    def forward(self, x):\n        _, C, _ = x.shape\n\n        x = F.pad(x, (self.pad, self.pad), mode='replicate')\n        x = self.ratio * F.conv_transpose1d(\n            x, self.filter.expand(C, -1, -1), stride=self.stride, groups=C)\n        x = x[..., self.pad_left:-self.pad_right]\n",
    "suffix": ""
  },
  {
    "name": "PKU-YuanGroup/Chat-UniVi:main_demo_7B.py@1559",
    "canonical_solution": "    if not textbox_in:",
    "prompt": "import torch\nimport gradio as gr\nimport os\nimport tempfile\nimport imageio\nimport shutil\nfrom fastapi import FastAPI\nfrom ChatUniVi.conversation import conv_templates, Conversation\nfrom ChatUniVi.demo import Chat\nfrom ChatUniVi.constants import *\nfrom PIL import Image\nfrom decord import VideoReader, cpu\n\n\napp = FastAPI()\nmodel_path = \"Chat-UniVi/Chat-UniVi\"  # model_path = [model path]\n\ndef save_image_to_local(image):\n    filename = os.path.join('temp', next(tempfile._get_candidate_names()) + '.jpg')\n    image = Image.open(image)\n    image.save(filename)\n    return filename\n\n\ndef save_video_to_local(video_path):\n    filename = os.path.join('temp', next(tempfile._get_candidate_names()) + '.mp4')\n    shutil.copyfile(video_path, filename)\n    return filename\n\n\ndef generate(image1, image2, video, textbox_in, first_run, state, state_, images_tensor):\n\n    flag = 1",
    "prefix": "import torch\nimport gradio as gr\nimport os\nimport tempfile\nimport imageio\nimport shutil\nfrom fastapi import FastAPI\nfrom ChatUniVi.conversation import conv_templates, Conversation\nfrom ChatUniVi.demo import Chat\nfrom ChatUniVi.constants import *\nfrom PIL import Image\nfrom decord import VideoReader, cpu\n\n\napp = FastAPI()\nmodel_path = \"Chat-UniVi/Chat-UniVi\"  # model_path = [model path]\n\ndef save_image_to_local(image):\n    filename = os.path.join('temp', next(tempfile._get_candidate_names()) + '.jpg')\n    image = Image.open(image)\n    image.save(filename)\n    return filename\n\n\ndef save_video_to_local(video_path):\n    filename = os.path.join('temp', next(tempfile._get_candidate_names()) + '.mp4')\n    shutil.copyfile(video_path, filename)\n    return filename\n\n\ndef generate(image1, image2, video, textbox_in, first_run, state, state_, images_tensor):\n\n    flag = 1",
    "suffix": ""
  },
  {
    "name": "tatsu-lab/gpt_paper_assistant:filter_papers.py@998",
    "canonical_solution": "                if config[\"OUTPUT\"].getboolean(\"debug_messages\"):",
    "prompt": "import configparser\nimport dataclasses\nimport json\nimport os\nimport re\nimport retry\nfrom collections import defaultdict\nfrom typing import List\nfrom openai import OpenAI\nfrom tqdm import tqdm\nfrom arxiv_scraper import Paper\nfrom arxiv_scraper import EnhancedJSONEncoder\n\n\n\n\ndef filter_by_author(all_authors, papers, author_targets, config):\n    # filter and parse the papers\n    selected_papers = {}  # pass to output\n    all_papers = {}  # dict for later filtering\n    sort_dict = {}  # dict storing key and score\n\n    # author based selection\n    for paper in papers:\n        all_papers[paper.arxiv_id] = paper\n        if config[\"FILTERING\"].getboolean(\"author_match\"):\n            for author in paper.authors:\n                if author in all_authors:\n                    for alias in all_authors[author]:\n                        if alias[\"authorId\"] in author_targets:\n                            selected_papers[paper.arxiv_id] = {\n                                **dataclasses.asdict(paper),\n                                **{\"COMMENT\": \"Author match\"},\n                            }\n                            sort_dict[paper.arxiv_id] = float(\n                                config[\"SELECTION\"][\"author_match_score\"]\n                            )\n                            break\n    return selected_papers, all_papers, sort_dict\n\n\ndef filter_papers_by_hindex(all_authors, papers, config):\n    # filters papers by checking to see if there's at least one author with > hcutoff hindex\n    paper_list = []\n    for paper in papers:\n        max_h = 0\n        for author in paper.authors:\n            if author in all_authors:\n                max_h = max(\n                    max_h, max([alias[\"hIndex\"] for alias in all_authors[author]])\n                )\n        if max_h >= float(config[\"FILTERING\"][\"hcutoff\"]):\n            paper_list.append(paper)\n    return paper_list\n\n\ndef calc_price(model, usage):\n    if model == \"gpt-4-1106-preview\":\n        return (0.01 * usage.prompt_tokens + 0.03 * usage.completion_tokens) / 1000.0\n    if model == \"gpt-4\":\n        return (0.03 * usage.prompt_tokens + 0.06 * usage.completion_tokens) / 1000.0\n    if (model == \"gpt-3.5-turbo\") or (model == \"gpt-3.5-turbo-1106\"):\n        return (0.0015 * usage.prompt_tokens + 0.002 * usage.completion_tokens) / 1000.0\n\n\n@retry.retry(tries=3, delay=2)\ndef call_chatgpt(full_prompt, openai_client, model, num_samples):\n    return openai_client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": full_prompt}],\n        temperature=0.0,\n        n=int(num_samples),\n        seed=0,\n    )\n\n\ndef run_and_parse_chatgpt(full_prompt, openai_client, config):\n    # just runs the chatgpt prompt, tries to parse the resulting JSON\n    completion = call_chatgpt(\n        full_prompt,\n        openai_client,\n        config[\"SELECTION\"][\"model\"],\n        config[\"FILTERING\"][\"num_samples\"],\n    )\n    json_dicts = defaultdict(list)\n    for choice in completion.choices:\n        out_text = choice.message.content\n        out_text = re.sub(\"```jsonl\\n\", \"\", out_text)\n        out_text = re.sub(\"```\", \"\", out_text)\n        out_text = re.sub(r\"\\n+\", \"\\n\", out_text)\n        out_text = re.sub(\"},\", \"}\", out_text).strip()\n        # split out_text line by line and parse each as a json.\n        for line in out_text.split(\"\\n\"):\n            # try catch block to attempt to parse json\n            try:\n                loaded_output = json.loads(line)\n                json_dicts[loaded_output[\"ARXIVID\"]].append(loaded_output)\n            except Exception as ex:",
    "prefix": "import configparser\nimport dataclasses\nimport json\nimport os\nimport re\nimport retry\nfrom collections import defaultdict\nfrom typing import List\nfrom openai import OpenAI\nfrom tqdm import tqdm\nfrom arxiv_scraper import Paper\nfrom arxiv_scraper import EnhancedJSONEncoder\n\n\n\n\ndef filter_by_author(all_authors, papers, author_targets, config):\n    # filter and parse the papers\n    selected_papers = {}  # pass to output\n    all_papers = {}  # dict for later filtering\n    sort_dict = {}  # dict storing key and score\n\n    # author based selection\n    for paper in papers:\n        all_papers[paper.arxiv_id] = paper\n        if config[\"FILTERING\"].getboolean(\"author_match\"):\n            for author in paper.authors:\n                if author in all_authors:\n                    for alias in all_authors[author]:\n                        if alias[\"authorId\"] in author_targets:\n                            selected_papers[paper.arxiv_id] = {\n                                **dataclasses.asdict(paper),\n                                **{\"COMMENT\": \"Author match\"},\n                            }\n                            sort_dict[paper.arxiv_id] = float(\n                                config[\"SELECTION\"][\"author_match_score\"]\n                            )\n                            break\n    return selected_papers, all_papers, sort_dict\n\n\ndef filter_papers_by_hindex(all_authors, papers, config):\n    # filters papers by checking to see if there's at least one author with > hcutoff hindex\n    paper_list = []\n    for paper in papers:\n        max_h = 0\n        for author in paper.authors:\n            if author in all_authors:\n                max_h = max(\n                    max_h, max([alias[\"hIndex\"] for alias in all_authors[author]])\n                )\n        if max_h >= float(config[\"FILTERING\"][\"hcutoff\"]):\n            paper_list.append(paper)\n    return paper_list\n\n\ndef calc_price(model, usage):\n    if model == \"gpt-4-1106-preview\":\n        return (0.01 * usage.prompt_tokens + 0.03 * usage.completion_tokens) / 1000.0\n    if model == \"gpt-4\":\n        return (0.03 * usage.prompt_tokens + 0.06 * usage.completion_tokens) / 1000.0\n    if (model == \"gpt-3.5-turbo\") or (model == \"gpt-3.5-turbo-1106\"):\n        return (0.0015 * usage.prompt_tokens + 0.002 * usage.completion_tokens) / 1000.0\n\n\n@retry.retry(tries=3, delay=2)\ndef call_chatgpt(full_prompt, openai_client, model, num_samples):\n    return openai_client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": full_prompt}],\n        temperature=0.0,\n        n=int(num_samples),\n        seed=0,\n    )\n\n\ndef run_and_parse_chatgpt(full_prompt, openai_client, config):\n    # just runs the chatgpt prompt, tries to parse the resulting JSON\n    completion = call_chatgpt(\n        full_prompt,\n        openai_client,\n        config[\"SELECTION\"][\"model\"],\n        config[\"FILTERING\"][\"num_samples\"],\n    )\n    json_dicts = defaultdict(list)\n    for choice in completion.choices:\n        out_text = choice.message.content\n        out_text = re.sub(\"```jsonl\\n\", \"\", out_text)\n        out_text = re.sub(\"```\", \"\", out_text)\n        out_text = re.sub(r\"\\n+\", \"\\n\", out_text)\n        out_text = re.sub(\"},\", \"}\", out_text).strip()\n        # split out_text line by line and parse each as a json.\n        for line in out_text.split(\"\\n\"):\n            # try catch block to attempt to parse json\n            try:\n                loaded_output = json.loads(line)\n                json_dicts[loaded_output[\"ARXIVID\"]].append(loaded_output)\n            except Exception as ex:",
    "suffix": ""
  },
  {
    "name": "banodoco/Steerable-Motion:imports/AdvancedControlNet/latent_keyframe_nodes.py@1362",
    "canonical_solution": "        try:",
    "prompt": "from typing import Union\nfrom collections.abc import Iterable\nfrom .control import LatentKeyframeImport, LatentKeyframeGroupImport\nfrom .control import StrengthInterpolationImport as SI\nfrom .logger import logger\nimport numpy as np\n\n\n\nclass LatentKeyframeNodeImport:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"batch_index\": (\"INT\", {\"default\": 0, \"min\": -1000, \"max\": 1000, \"step\": 1}),\n                \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.001}, ),\n            },\n            \"optional\": {\n                \"prev_latent_kf\": (\"LATENT_KEYFRAME\", ),\n            }\n        }\n\n    RETURN_NAMES = (\"LATENT_KF\", )\n    RETURN_TYPES = (\"LATENT_KEYFRAME\", )\n    FUNCTION = \"load_keyframe\"\n\n    CATEGORY = \"Adv-ControlNet \ud83d\udec2\ud83c\udd50\ud83c\udd52\ud83c\udd5d/keyframes\"\n\n    def load_keyframe(self,\n                      batch_index: int,\n                      strength: float,\n                      prev_latent_kf: LatentKeyframeGroupImport=None,\n                      prev_latent_keyframe: LatentKeyframeGroupImport=None, # old name\n                      ):\n        prev_latent_keyframe = prev_latent_keyframe if prev_latent_keyframe else prev_latent_kf\n        if not prev_latent_keyframe:\n            prev_latent_keyframe = LatentKeyframeGroupImport()\n        else:\n            prev_latent_keyframe = prev_latent_keyframe.clone()\n        keyframe = LatentKeyframeImport(batch_index, strength)\n        prev_latent_keyframe.add(keyframe)\n        return (prev_latent_keyframe,)\n\n\nclass LatentKeyframeGroupNodeImport:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"index_strengths\": (\"STRING\", {\"multiline\": True, \"default\": \"\"}),\n            },\n            \"optional\": {\n                \"prev_latent_kf\": (\"LATENT_KEYFRAME\", ),\n                \"latent_optional\": (\"LATENT\", ),\n                \"print_keyframes\": (\"BOOLEAN\", {\"default\": False})\n            }\n        }\n    \n    RETURN_NAMES = (\"LATENT_KF\", )\n    RETURN_TYPES = (\"LATENT_KEYFRAME\", )\n    FUNCTION = \"load_keyframes\"\n\n    CATEGORY = \"Adv-ControlNet \ud83d\udec2\ud83c\udd50\ud83c\udd52\ud83c\udd5d/keyframes\"\n\n    def validate_index(self, index: int, latent_count: int = 0, is_range: bool = False, allow_negative = False) -> int:\n        # if part of range, do nothing\n        if is_range:\n            return index\n        # otherwise, validate index\n        # validate not out of range - only when latent_count is passed in\n        if latent_count > 0 and index > latent_count-1:\n            raise IndexError(f\"Index '{index}' out of range for the total {latent_count} latents.\")\n        # if negative, validate not out of range\n        if index < 0:\n            if not allow_negative:\n                raise IndexError(f\"Negative indeces not allowed, but was {index}.\")\n            conv_index = latent_count+index\n            if conv_index < 0:\n                raise IndexError(f\"Index '{index}', converted to '{conv_index}' out of range for the total {latent_count} latents.\")\n            index = conv_index\n        return index\n\n    def convert_to_index_int(self, raw_index: str, latent_count: int = 0, is_range: bool = False, allow_negative = False) -> int:",
    "prefix": "from typing import Union\nfrom collections.abc import Iterable\nfrom .control import LatentKeyframeImport, LatentKeyframeGroupImport\nfrom .control import StrengthInterpolationImport as SI\nfrom .logger import logger\nimport numpy as np\n\n\n\nclass LatentKeyframeNodeImport:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"batch_index\": (\"INT\", {\"default\": 0, \"min\": -1000, \"max\": 1000, \"step\": 1}),\n                \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.001}, ),\n            },\n            \"optional\": {\n                \"prev_latent_kf\": (\"LATENT_KEYFRAME\", ),\n            }\n        }\n\n    RETURN_NAMES = (\"LATENT_KF\", )\n    RETURN_TYPES = (\"LATENT_KEYFRAME\", )\n    FUNCTION = \"load_keyframe\"\n\n    CATEGORY = \"Adv-ControlNet \ud83d\udec2\ud83c\udd50\ud83c\udd52\ud83c\udd5d/keyframes\"\n\n    def load_keyframe(self,\n                      batch_index: int,\n                      strength: float,\n                      prev_latent_kf: LatentKeyframeGroupImport=None,\n                      prev_latent_keyframe: LatentKeyframeGroupImport=None, # old name\n                      ):\n        prev_latent_keyframe = prev_latent_keyframe if prev_latent_keyframe else prev_latent_kf\n        if not prev_latent_keyframe:\n            prev_latent_keyframe = LatentKeyframeGroupImport()\n        else:\n            prev_latent_keyframe = prev_latent_keyframe.clone()\n        keyframe = LatentKeyframeImport(batch_index, strength)\n        prev_latent_keyframe.add(keyframe)\n        return (prev_latent_keyframe,)\n\n\nclass LatentKeyframeGroupNodeImport:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"index_strengths\": (\"STRING\", {\"multiline\": True, \"default\": \"\"}),\n            },\n            \"optional\": {\n                \"prev_latent_kf\": (\"LATENT_KEYFRAME\", ),\n                \"latent_optional\": (\"LATENT\", ),\n                \"print_keyframes\": (\"BOOLEAN\", {\"default\": False})\n            }\n        }\n    \n    RETURN_NAMES = (\"LATENT_KF\", )\n    RETURN_TYPES = (\"LATENT_KEYFRAME\", )\n    FUNCTION = \"load_keyframes\"\n\n    CATEGORY = \"Adv-ControlNet \ud83d\udec2\ud83c\udd50\ud83c\udd52\ud83c\udd5d/keyframes\"\n\n    def validate_index(self, index: int, latent_count: int = 0, is_range: bool = False, allow_negative = False) -> int:\n        # if part of range, do nothing\n        if is_range:\n            return index\n        # otherwise, validate index\n        # validate not out of range - only when latent_count is passed in\n        if latent_count > 0 and index > latent_count-1:\n            raise IndexError(f\"Index '{index}' out of range for the total {latent_count} latents.\")\n        # if negative, validate not out of range\n        if index < 0:\n            if not allow_negative:\n                raise IndexError(f\"Negative indeces not allowed, but was {index}.\")\n            conv_index = latent_count+index\n            if conv_index < 0:\n                raise IndexError(f\"Index '{index}', converted to '{conv_index}' out of range for the total {latent_count} latents.\")\n            index = conv_index\n        return index\n\n    def convert_to_index_int(self, raw_index: str, latent_count: int = 0, is_range: bool = False, allow_negative = False) -> int:",
    "suffix": ""
  },
  {
    "name": "Zaloog/kanban-python:src/kanban_python/config.py@1100",
    "canonical_solution": "    @property",
    "prompt": "import configparser\nfrom pathlib import Path\nfrom .constants import (\n    CONFIG_FILE_NAME,\n    CONFIG_FILE_PATH,\n    CONFIG_PATH,\n    DATA_PATH,\n    KANBAN_BOARDS_FOLDER_NAME,\n    KANBAN_BOARDS_PATH,\n    TASK_FILE_NAME,\n)\nfrom .utils import console\n\n\n\nclass KanbanConfig:\n    def __init__(self, path=CONFIG_FILE_PATH) -> None:\n        self.configpath = path\n        self._config = configparser.ConfigParser(default_section=None)\n        self._config.optionxform = str\n        self._config.read(path)\n\n    def __repr__(self) -> str:\n        output = \"\"\n        for sec in self.config:\n            if sec:\n                output += 15 * \"-\"\n                output += f\"Section: {sec}\"\n                output += 15 * \"-\" + \"\\n\"\n            for key, val in self.config[sec].items():\n                output += f\"{key}: {val}\\n\"\n        return output\n\n    def save(self):\n        with open(self.configpath, \"w\") as configfile:\n            self.config.write(configfile)\n\n    @property\n    def config(self) -> configparser.ConfigParser:\n        return self._config\n\n    @property\n    def active_board(self) -> str:\n        return self._config[\"settings.general\"][\"Active_Board\"]\n\n    @active_board.setter\n    def active_board(self, new_board):\n        self.config[\"settings.general\"][\"Active_Board\"] = new_board\n        self.save()\n\n    @property\n    def kanban_boards(self) -> list:\n        return [board for board in self.config[\"kanban_boards\"]]\n\n    @property\n    def kanban_boards_dict(self) -> dict:\n        return self.config[\"kanban_boards\"]\n\n    @kanban_boards_dict.setter\n    def kanban_boards_dict(self, board_name: str) -> dict:\n        self.config[\"kanban_boards\"][board_name] = get_json_path(board_name)\n        self.save()\n\n    @property\n    def active_board_path(self) -> str:\n        return self.config[\"kanban_boards\"][self.active_board]\n\n    @property\n    def show_footer(self):\n        return self.config[\"settings.general\"][\"Show_Footer\"]\n\n    @show_footer.setter\n    def show_footer(self, visible):\n        self.config[\"settings.general\"][\"Show_Footer\"] = visible\n        self.save()\n",
    "prefix": "import configparser\nfrom pathlib import Path\nfrom .constants import (\n    CONFIG_FILE_NAME,\n    CONFIG_FILE_PATH,\n    CONFIG_PATH,\n    DATA_PATH,\n    KANBAN_BOARDS_FOLDER_NAME,\n    KANBAN_BOARDS_PATH,\n    TASK_FILE_NAME,\n)\nfrom .utils import console\n\n\n\nclass KanbanConfig:\n    def __init__(self, path=CONFIG_FILE_PATH) -> None:\n        self.configpath = path\n        self._config = configparser.ConfigParser(default_section=None)\n        self._config.optionxform = str\n        self._config.read(path)\n\n    def __repr__(self) -> str:\n        output = \"\"\n        for sec in self.config:\n            if sec:\n                output += 15 * \"-\"\n                output += f\"Section: {sec}\"\n                output += 15 * \"-\" + \"\\n\"\n            for key, val in self.config[sec].items():\n                output += f\"{key}: {val}\\n\"\n        return output\n\n    def save(self):\n        with open(self.configpath, \"w\") as configfile:\n            self.config.write(configfile)\n\n    @property\n    def config(self) -> configparser.ConfigParser:\n        return self._config\n\n    @property\n    def active_board(self) -> str:\n        return self._config[\"settings.general\"][\"Active_Board\"]\n\n    @active_board.setter\n    def active_board(self, new_board):\n        self.config[\"settings.general\"][\"Active_Board\"] = new_board\n        self.save()\n\n    @property\n    def kanban_boards(self) -> list:\n        return [board for board in self.config[\"kanban_boards\"]]\n\n    @property\n    def kanban_boards_dict(self) -> dict:\n        return self.config[\"kanban_boards\"]\n\n    @kanban_boards_dict.setter\n    def kanban_boards_dict(self, board_name: str) -> dict:\n        self.config[\"kanban_boards\"][board_name] = get_json_path(board_name)\n        self.save()\n\n    @property\n    def active_board_path(self) -> str:\n        return self.config[\"kanban_boards\"][self.active_board]\n\n    @property\n    def show_footer(self):\n        return self.config[\"settings.general\"][\"Show_Footer\"]\n\n    @show_footer.setter\n    def show_footer(self, visible):\n        self.config[\"settings.general\"][\"Show_Footer\"] = visible\n        self.save()\n",
    "suffix": ""
  },
  {
    "name": "AMAAI-Lab/mustango:audioldm/latent_diffusion/openaimodel.py@1481",
    "canonical_solution": "    pass",
    "prompt": "from abc import abstractmethod\nfrom audioldm.latent_diffusion.util import (\n    checkpoint,\n    conv_nd,\n    linear,\n    avg_pool_nd,\n    zero_module,\n    normalization,\n    timestep_embedding,\n)\nfrom audioldm.latent_diffusion.attention import SpatialTransformer\n            from omegaconf.listconfig import ListConfig\nimport math\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n\n\n# dummy replace\ndef convert_module_to_f16(x):\n    pass\n\n\ndef convert_module_to_f32(x):",
    "prefix": "from abc import abstractmethod\nfrom audioldm.latent_diffusion.util import (\n    checkpoint,\n    conv_nd,\n    linear,\n    avg_pool_nd,\n    zero_module,\n    normalization,\n    timestep_embedding,\n)\nfrom audioldm.latent_diffusion.attention import SpatialTransformer\n            from omegaconf.listconfig import ListConfig\nimport math\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n\n\n# dummy replace\ndef convert_module_to_f16(x):\n    pass\n\n\ndef convert_module_to_f32(x):",
    "suffix": ""
  },
  {
    "name": "lxmusics/lx-music-api-server-python:modules/tx/player.py@1083",
    "canonical_solution": "                'loginflag': 1,",
    "prompt": "from common.exceptions import FailedException\nfrom common import config, utils, variable\nfrom .musicInfo import getMusicInfo\nfrom .utils import tools\nfrom .utils import signRequest\nimport random\n# ----------------------------------------\n# - mode: python - \n# - author: helloplhm-qwq - \n# - name: player.py - \n# - project: lx-music-api-server - \n# - license: MIT - \n# ----------------------------------------\n# This file is part of the \"lx-music-api-server\" project.\n\n\ncreateObject = utils.CreateObject\n\nasync def url(songId, quality):\n    infoBody = await getMusicInfo(songId)\n    strMediaMid = infoBody['track_info']['file']['media_mid']\n    user_info = config.read_config('module.tx.user') if (not variable.use_cookie_pool) else random.choice(config.read_config('module.cookiepool.tx'))\n    requestBody = {\n        'req_0': {\n            'module': 'vkey.GetVkeyServer',\n            'method': 'CgiGetVkey',\n            'param': {\n                'filename': [f\"{tools.fileInfo[quality]['h']}{strMediaMid}{tools.fileInfo[quality]['e']}\"],\n                'guid': config.read_config('module.tx.vkeyserver.guid'),\n                'songmid': [songId],\n                'songtype': [0],\n                'uin': str(user_info['uin']),",
    "prefix": "from common.exceptions import FailedException\nfrom common import config, utils, variable\nfrom .musicInfo import getMusicInfo\nfrom .utils import tools\nfrom .utils import signRequest\nimport random\n# ----------------------------------------\n# - mode: python - \n# - author: helloplhm-qwq - \n# - name: player.py - \n# - project: lx-music-api-server - \n# - license: MIT - \n# ----------------------------------------\n# This file is part of the \"lx-music-api-server\" project.\n\n\ncreateObject = utils.CreateObject\n\nasync def url(songId, quality):\n    infoBody = await getMusicInfo(songId)\n    strMediaMid = infoBody['track_info']['file']['media_mid']\n    user_info = config.read_config('module.tx.user') if (not variable.use_cookie_pool) else random.choice(config.read_config('module.cookiepool.tx'))\n    requestBody = {\n        'req_0': {\n            'module': 'vkey.GetVkeyServer',\n            'method': 'CgiGetVkey',\n            'param': {\n                'filename': [f\"{tools.fileInfo[quality]['h']}{strMediaMid}{tools.fileInfo[quality]['e']}\"],\n                'guid': config.read_config('module.tx.vkeyserver.guid'),\n                'songmid': [songId],\n                'songtype': [0],\n                'uin': str(user_info['uin']),",
    "suffix": ""
  },
  {
    "name": "spfrommer/torchexplorer:torchexplorer/api/backend.py@1035",
    "canonical_solution": "        delete_version = min(lagged_version, self.delete_counter)",
    "prompt": "import os\nimport shutil\nimport json\nimport threading\nimport wandb\nimport sys\n        import app # type: ignore\nfrom torchexplorer.render import serialize\nfrom torchexplorer.render.structs import NodeLayout\nfrom __future__ import annotations\n\n\n\n\nclass Backend():\n    def update(self, renderable: NodeLayout):\n        raise NotImplementedError()\n\n\nclass WandbBackend(Backend):\n    def __init__(self, watch_counter: int):\n        self.watch_counter = watch_counter\n        self.update_counter = 0\n        self.delete_counter = 0\n        self.delete_old_artifact_lag = 10\n\n    def update(self, renderable: NodeLayout):\n        if wandb.run is None:\n            raise ValueError('Must call `wandb.init` before `torchexplorer.watch`')\n\n        explorer_table, fields = self._wandb_table(renderable)\n\n        chart = wandb.plot_table(\n            vega_spec_name='spfrom_team/torchexplorer_v4a',\n            data_table=explorer_table,\n            fields=fields,\n            string_fields={}\n        )\n\n        wandb.log({f'explorer_chart_{self.watch_counter}': chart}, commit=False)\n\n        self._delete_old_artifacts()\n        \n        self.update_counter += 1\n\n    def _wandb_table(\n            self, renderable: NodeLayout\n        ) -> tuple[wandb.Table, dict[str, str]]:\n\n        rows = serialize.serialize_rows(renderable)\n        fields = {key:key for key in rows[0]}\n        keys = fields.keys() \n        data = [[row[key] for key in keys] for row in rows]\n        table = wandb.Table(data=data, columns=list(keys))\n        return table, fields\n    \n    def _delete_old_artifacts(self):\n        lagged_version = self.update_counter - self.delete_old_artifact_lag",
    "prefix": "import os\nimport shutil\nimport json\nimport threading\nimport wandb\nimport sys\n        import app # type: ignore\nfrom torchexplorer.render import serialize\nfrom torchexplorer.render.structs import NodeLayout\nfrom __future__ import annotations\n\n\n\n\nclass Backend():\n    def update(self, renderable: NodeLayout):\n        raise NotImplementedError()\n\n\nclass WandbBackend(Backend):\n    def __init__(self, watch_counter: int):\n        self.watch_counter = watch_counter\n        self.update_counter = 0\n        self.delete_counter = 0\n        self.delete_old_artifact_lag = 10\n\n    def update(self, renderable: NodeLayout):\n        if wandb.run is None:\n            raise ValueError('Must call `wandb.init` before `torchexplorer.watch`')\n\n        explorer_table, fields = self._wandb_table(renderable)\n\n        chart = wandb.plot_table(\n            vega_spec_name='spfrom_team/torchexplorer_v4a',\n            data_table=explorer_table,\n            fields=fields,\n            string_fields={}\n        )\n\n        wandb.log({f'explorer_chart_{self.watch_counter}': chart}, commit=False)\n\n        self._delete_old_artifacts()\n        \n        self.update_counter += 1\n\n    def _wandb_table(\n            self, renderable: NodeLayout\n        ) -> tuple[wandb.Table, dict[str, str]]:\n\n        rows = serialize.serialize_rows(renderable)\n        fields = {key:key for key in rows[0]}\n        keys = fields.keys() \n        data = [[row[key] for key in keys] for row in rows]\n        table = wandb.Table(data=data, columns=list(keys))\n        return table, fields\n    \n    def _delete_old_artifacts(self):\n        lagged_version = self.update_counter - self.delete_old_artifact_lag",
    "suffix": ""
  },
  {
    "name": "namin/llm-verified-with-monte-carlo-tree-search:run_whole.py@695",
    "canonical_solution": "    solution_stats[solution_key] += 1",
    "prompt": "from cmdline import args\nfrom lang import can_be_solution\nfrom lang import score_func as uncached_score_func\nfrom common_cache import create_cached_func\nfrom prompts import prompt, min_lines, check_func\nimport llm\n\nGREEDY = args.greedy\nN_SAMPLES = args.n_samples\n\n# GREEDY = True\n\n\nscore_func, cache_stats = create_cached_func(uncached_score_func)\n\n\n\nscore_stats = {'positive': 0, 'negative': 0, 'unknown': 0}\nsolution_stats = {'yes': 0, 'no': 0}\nsolutions = []\n\ndef attempt():\n    if GREEDY:\n        text = llm.generate_full(prompt)\n    else:\n        text = llm.generate_full(prompt, do_sample=True, top_p=0.9, top_k=7, temperature=0.8)\n    score = score_func(text)\n    score_key = \"unknown\" if score is None else \"positive\" if score > 0 else \"negative\"\n    score_stats[score_key] += 1\n    solution_key = 'yes' if score is not None and score > 0 and can_be_solution(text, min_lines, check_func) else 'no'",
    "prefix": "from cmdline import args\nfrom lang import can_be_solution\nfrom lang import score_func as uncached_score_func\nfrom common_cache import create_cached_func\nfrom prompts import prompt, min_lines, check_func\nimport llm\n\nGREEDY = args.greedy\nN_SAMPLES = args.n_samples\n\n# GREEDY = True\n\n\nscore_func, cache_stats = create_cached_func(uncached_score_func)\n\n\n\nscore_stats = {'positive': 0, 'negative': 0, 'unknown': 0}\nsolution_stats = {'yes': 0, 'no': 0}\nsolutions = []\n\ndef attempt():\n    if GREEDY:\n        text = llm.generate_full(prompt)\n    else:\n        text = llm.generate_full(prompt, do_sample=True, top_p=0.9, top_k=7, temperature=0.8)\n    score = score_func(text)\n    score_key = \"unknown\" if score is None else \"positive\" if score > 0 else \"negative\"\n    score_stats[score_key] += 1\n    solution_key = 'yes' if score is not None and score > 0 and can_be_solution(text, min_lines, check_func) else 'no'",
    "suffix": ""
  },
  {
    "name": "BraveGroup/Drive-WM:src/diffusers/utils/testing_utils.py@1141",
    "canonical_solution": ") > version.parse(\"4.33\")",
    "prompt": "import functools\nimport importlib\nimport inspect\nimport io\nimport logging\nimport multiprocessing\nimport os\nimport random\nimport re\nimport struct\nimport sys\nimport tempfile\nimport time\nimport unittest\nimport urllib.parse\nimport numpy as np\nimport PIL.Image\nimport PIL.ImageOps\nimport requests\n    import torch\n        import cv2\nfrom contextlib import contextmanager\nfrom distutils.util import strtobool\nfrom io import BytesIO, StringIO\nfrom pathlib import Path\nfrom typing import List, Optional, Union\nfrom numpy.linalg import norm\nfrom packaging import version\nfrom .import_utils import (\n    BACKENDS_MAPPING,\n    is_compel_available,\n    is_flax_available,\n    is_note_seq_available,\n    is_onnx_available,\n    is_opencv_available,\n    is_peft_available,\n    is_torch_available,\n    is_torch_version,\n    is_torchsde_available,\n    is_transformers_available,\n)\nfrom .logging import get_logger\n    from _pytest.config import create_terminal_writer\n\n\n\n\nglobal_rng = random.Random()\n\nlogger = get_logger(__name__)\n\n_required_peft_version = is_peft_available() and version.parse(\n    version.parse(importlib.metadata.version(\"peft\")).base_version\n) > version.parse(\"0.5\")\n_required_transformers_version = is_transformers_available() and version.parse(\n    version.parse(importlib.metadata.version(\"transformers\")).base_version",
    "prefix": "import functools\nimport importlib\nimport inspect\nimport io\nimport logging\nimport multiprocessing\nimport os\nimport random\nimport re\nimport struct\nimport sys\nimport tempfile\nimport time\nimport unittest\nimport urllib.parse\nimport numpy as np\nimport PIL.Image\nimport PIL.ImageOps\nimport requests\n    import torch\n        import cv2\nfrom contextlib import contextmanager\nfrom distutils.util import strtobool\nfrom io import BytesIO, StringIO\nfrom pathlib import Path\nfrom typing import List, Optional, Union\nfrom numpy.linalg import norm\nfrom packaging import version\nfrom .import_utils import (\n    BACKENDS_MAPPING,\n    is_compel_available,\n    is_flax_available,\n    is_note_seq_available,\n    is_onnx_available,\n    is_opencv_available,\n    is_peft_available,\n    is_torch_available,\n    is_torch_version,\n    is_torchsde_available,\n    is_transformers_available,\n)\nfrom .logging import get_logger\n    from _pytest.config import create_terminal_writer\n\n\n\n\nglobal_rng = random.Random()\n\nlogger = get_logger(__name__)\n\n_required_peft_version = is_peft_available() and version.parse(\n    version.parse(importlib.metadata.version(\"peft\")).base_version\n) > version.parse(\"0.5\")\n_required_transformers_version = is_transformers_available() and version.parse(\n    version.parse(importlib.metadata.version(\"transformers\")).base_version",
    "suffix": ""
  },
  {
    "name": "xk-huang/segment-caption-anything:scripts/tools/utils/git_utils/tsv_io.py@933",
    "canonical_solution": "    File.prepare(ins)",
    "prompt": "import numpy as np\nimport shutil\nimport mmap\nimport time\nimport logging\n    import types\nimport os\nimport os.path as op\nimport subprocess\nimport tempfile\nimport hashlib\nimport logging\n                        import struct\nfrom .common import qd_tqdm as tqdm\nfrom .common import (\n    dict_update_path_value,\n    dict_get_path_value,\n    get_all_path,\n    load_from_yaml_str,\n)\n    from azfuse import File\nfrom contextlib import contextmanager\nfrom datasets.utils.filelock import FileLock\nfrom urllib.parse import urlparse, urlunparse\n        from pathos.multiprocessing import ProcessingPool as Pool\n\n# NOTE(xiaoke): Modified. Try to use azfuse.File if possible.\ntry:\nexcept ImportError:\n\n    File = types.SimpleNamespace()\n    File.open = open\n    File.get_file_size = lambda x: os.stat(x).st_size\n\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef concat_files(ins, out):",
    "prefix": "import numpy as np\nimport shutil\nimport mmap\nimport time\nimport logging\n    import types\nimport os\nimport os.path as op\nimport subprocess\nimport tempfile\nimport hashlib\nimport logging\n                        import struct\nfrom .common import qd_tqdm as tqdm\nfrom .common import (\n    dict_update_path_value,\n    dict_get_path_value,\n    get_all_path,\n    load_from_yaml_str,\n)\n    from azfuse import File\nfrom contextlib import contextmanager\nfrom datasets.utils.filelock import FileLock\nfrom urllib.parse import urlparse, urlunparse\n        from pathos.multiprocessing import ProcessingPool as Pool\n\n# NOTE(xiaoke): Modified. Try to use azfuse.File if possible.\ntry:\nexcept ImportError:\n\n    File = types.SimpleNamespace()\n    File.open = open\n    File.get_file_size = lambda x: os.stat(x).st_size\n\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef concat_files(ins, out):",
    "suffix": ""
  },
  {
    "name": "fjzzq2002/is-my-problem-new:src/ui.py@960",
    "canonical_solution": "        template_id = int(template_choice.split(\" \")[1]) - 1",
    "prompt": "from .embedder import VectorDB, get_embeddings\nfrom .utils import read_problems, problems_filenames\nfrom tqdm.auto import tqdm\nfrom openai import OpenAI\nimport gradio as gr\nimport json\n\ndb = VectorDB().load()\nemb_keys = set([x[0] for x in db.metadata])\nprint(\"read\", len(emb_keys), \"embeddings from db\")\nproblems = {}\nfor f in problems_filenames():\n    for p in read_problems(\"problems/\" + f):\n        problems[p[\"uid\"]] = p\nprint(\"read\", len(problems), \"problems from db\")\n\nwith open(\"settings.json\") as f:\n    settings = json.load(f)\n\nclient = OpenAI(\n    api_key=settings[\"OPENAI_API_KEY\"],\n)\n\n\ndef querier(statement, template_choice, topk):\n    # print(statement, template_choice)\n    paraphrased = statement\n    if \"None\" not in template_choice:",
    "prefix": "from .embedder import VectorDB, get_embeddings\nfrom .utils import read_problems, problems_filenames\nfrom tqdm.auto import tqdm\nfrom openai import OpenAI\nimport gradio as gr\nimport json\n\ndb = VectorDB().load()\nemb_keys = set([x[0] for x in db.metadata])\nprint(\"read\", len(emb_keys), \"embeddings from db\")\nproblems = {}\nfor f in problems_filenames():\n    for p in read_problems(\"problems/\" + f):\n        problems[p[\"uid\"]] = p\nprint(\"read\", len(problems), \"problems from db\")\n\nwith open(\"settings.json\") as f:\n    settings = json.load(f)\n\nclient = OpenAI(\n    api_key=settings[\"OPENAI_API_KEY\"],\n)\n\n\ndef querier(statement, template_choice, topk):\n    # print(statement, template_choice)\n    paraphrased = statement\n    if \"None\" not in template_choice:",
    "suffix": ""
  },
  {
    "name": "p0p4k/pflowtts_pytorch:pflow/utils/generate_data_statistics.py@967",
    "canonical_solution": "Parameters from hparam.py will be used",
    "prompt": "import os\nimport sys\nimport argparse\nimport json\nimport sys\nimport rootutils\nimport torch\nfrom pathlib import Path\nfrom hydra import compose, initialize\nfrom omegaconf import open_dict\nfrom tqdm.auto import tqdm\nfrom pflow.data.text_mel_datamodule import TextMelDataModule\nfrom pflow.utils.logging_utils import pylogger\nr\"\"\"\nThe file creates a pickle file where the values needed for loading of dataset is stored and the model can load it\nwhen needed.\n",
    "prefix": "import os\nimport sys\nimport argparse\nimport json\nimport sys\nimport rootutils\nimport torch\nfrom pathlib import Path\nfrom hydra import compose, initialize\nfrom omegaconf import open_dict\nfrom tqdm.auto import tqdm\nfrom pflow.data.text_mel_datamodule import TextMelDataModule\nfrom pflow.utils.logging_utils import pylogger\nr\"\"\"\nThe file creates a pickle file where the values needed for loading of dataset is stored and the model can load it\nwhen needed.\n",
    "suffix": ""
  },
  {
    "name": "theroyallab/tabbyAPI:OAI/types/chat_completion.py@1284",
    "canonical_solution": "class ChatCompletionStreamChoice(BaseModel):",
    "prompt": "from uuid import uuid4\nfrom time import time\nfrom pydantic import BaseModel, Field\nfrom typing import Union, List, Optional, Dict\nfrom OAI.types.common import UsageStats, CommonCompletionRequest\n\n\nclass ChatCompletionMessage(BaseModel):\n    role: Optional[str] = None\n    content: Optional[str] = None\n\n\nclass ChatCompletionRespChoice(BaseModel):\n    # Index is 0 since we aren't using multiple choices\n    index: int = 0\n    finish_reason: str\n    message: ChatCompletionMessage\n\n",
    "prefix": "from uuid import uuid4\nfrom time import time\nfrom pydantic import BaseModel, Field\nfrom typing import Union, List, Optional, Dict\nfrom OAI.types.common import UsageStats, CommonCompletionRequest\n\n\nclass ChatCompletionMessage(BaseModel):\n    role: Optional[str] = None\n    content: Optional[str] = None\n\n\nclass ChatCompletionRespChoice(BaseModel):\n    # Index is 0 since we aren't using multiple choices\n    index: int = 0\n    finish_reason: str\n    message: ChatCompletionMessage\n\n",
    "suffix": ""
  },
  {
    "name": "ShipBit/wingman-ai:gui/views/context_view.py@1549",
    "canonical_solution": "        self.core = master.core",
    "prompt": "import customtkinter as ctk\nfrom gui.sections.context_switcher import ContextSwitcher\nfrom gui.sections.context_runner import ContextRunner\n\nclass ContextView(ctk.CTkFrame):\n    def __init__(self, master, **kwargs):\n        super().__init__(master, **kwargs)\n",
    "prefix": "import customtkinter as ctk\nfrom gui.sections.context_switcher import ContextSwitcher\nfrom gui.sections.context_runner import ContextRunner\n\nclass ContextView(ctk.CTkFrame):\n    def __init__(self, master, **kwargs):\n        super().__init__(master, **kwargs)\n",
    "suffix": ""
  },
  {
    "name": "OliverMao/FlaskAutoApiBuilder:demo/app.py@1165",
    "canonical_solution": "        'test': 'mysql+pymysql://%s:%s@%s:3306/%s' % (user, password, host, 'test')",
    "prompt": "from Faab import Faab\nfrom Faab.FaabJWT import jwt_authentication\nfrom blueprints.test import test_bp\nfrom blueprints.test.model import Users\nimport factory as fac\n# Faab Project Demo\n\n\n\nclass DBConfig(object):\n    # \u57fa\u7840\u914d\u7f6e\n    user = 'faab'\n    host = 'localhost'\n    password = 'faab'\n    SQLALCHEMY_DATABASE_URI = 'mysql+pymysql://%s:%s@%s:3306/%s' % (user, password, host, 'faab')\n    SQLALCHEMY_BINDS = {",
    "prefix": "from Faab import Faab\nfrom Faab.FaabJWT import jwt_authentication\nfrom blueprints.test import test_bp\nfrom blueprints.test.model import Users\nimport factory as fac\n# Faab Project Demo\n\n\n\nclass DBConfig(object):\n    # \u57fa\u7840\u914d\u7f6e\n    user = 'faab'\n    host = 'localhost'\n    password = 'faab'\n    SQLALCHEMY_DATABASE_URI = 'mysql+pymysql://%s:%s@%s:3306/%s' % (user, password, host, 'faab')\n    SQLALCHEMY_BINDS = {",
    "suffix": ""
  }
]